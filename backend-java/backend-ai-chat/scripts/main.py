"""
ç™½å©çºªé£Ÿå“æº¯æºç³»ç»Ÿ - AIé£Ÿå“åŠ å·¥æ•°æ®åˆ†ææœåŠ¡
åŸºäºé˜¿é‡Œäº‘é€šä¹‰åƒé—® (DashScope) çš„æ™ºèƒ½åˆ†æAPI
æ”¯æŒæ€è€ƒæ¨¡å¼ (Thinking Mode) - æ·±åº¦æ¨ç†åˆ†æ
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, Dict, List, Any
import os
import json
from dotenv import load_dotenv

# å¯¼å…¥ OpenAI SDK (é˜¿é‡Œäº‘ DashScope å…¼å®¹ OpenAI æ ¼å¼)
from openai import OpenAI

# å¯¼å…¥ç”µå­ç§¤è§†è§‰è§£æå™¨
from scale_vision_parser import parse_scale_image, is_vision_enabled

# å¯¼å…¥ Sentence-BERT Embedding æœåŠ¡
try:
    from embedding_service import router as embedding_router, warmup_model as warmup_embedding
    EMBEDDING_ENABLED = True
except ImportError:
    EMBEDDING_ENABLED = False
    embedding_router = None
    print("[WARN] embedding_service not available - semantic cache disabled")

load_dotenv()

# ==================== é…ç½® ====================
# é˜¿é‡Œäº‘ DashScope é…ç½®
DASHSCOPE_API_KEY = os.environ.get('DASHSCOPE_API_KEY', '')
DASHSCOPE_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
# å¯é€‰æ¨¡å‹: qwen-turbo (æœ€å¿«æœ€ä¾¿å®œ), qwen-plus (å¹³è¡¡), qwen-max (æœ€å¼º)
DASHSCOPE_MODEL = os.environ.get('DASHSCOPE_MODEL', 'qwen-plus')

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ (å…¼å®¹ DashScope)
client = None
if DASHSCOPE_API_KEY:
    client = OpenAI(
        api_key=DASHSCOPE_API_KEY,
        base_url=DASHSCOPE_BASE_URL,
    )

# ==================== FastAPI åº”ç”¨ ====================
app = FastAPI(title="é£Ÿå“åŠ å·¥æ•°æ®åˆ†æ API", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œ Embedding è·¯ç”±
if EMBEDDING_ENABLED and embedding_router:
    app.include_router(embedding_router)

# ==================== æ•°æ®æ¨¡å‹ ====================
class FoodProcessingRequest(BaseModel):
    section_data: Dict[str, str]  # æ‰€æœ‰å‚æ•°ï¼ˆå®é™…å€¼å’Œå¹³å‡å€¼ï¼‰

class FoodProcessingResponse(BaseModel):
    success: bool
    analysis: str
    message: Optional[str] = None

# æˆæœ¬åˆ†æä¸“ç”¨è¯·æ±‚æ¨¡å‹
class CostAnalysisRequest(BaseModel):
    message: str  # æˆæœ¬æ•°æ®çš„æ–‡æœ¬æè¿°
    user_id: str  # å·¥å‚ID_batch_æ‰¹æ¬¡ID
    session_id: Optional[str] = None
    enable_thinking: Optional[bool] = True  # é»˜è®¤å¼€å¯æ€è€ƒæ¨¡å¼
    thinking_budget: Optional[int] = 50  # æ€è€ƒé¢„ç®— (10-100)

# ==================== æ ¸å¿ƒåŠŸèƒ½ ====================
def query_qwen(messages: list, enable_thinking: bool = False, thinking_budget: int = 50) -> dict:
    """
    è°ƒç”¨é˜¿é‡Œäº‘é€šä¹‰åƒé—®æ¨¡å‹

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
        thinking_budget: æ€è€ƒé¢„ç®— (10-100)

    Returns:
        dict: {
            "content": str,  # æœ€ç»ˆå›ç­”
            "reasoning_content": str,  # æ€è€ƒè¿‡ç¨‹ (ä»…æ€è€ƒæ¨¡å¼)
            "thinking_enabled": bool
        }
    """
    if not client:
        raise HTTPException(status_code=500, detail="DASHSCOPE_API_KEYæœªé…ç½®")

    try:
        if enable_thinking:
            # æ€è€ƒæ¨¡å¼ï¼šä½¿ç”¨æµå¼å“åº”æ”¶é›†æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ
            return query_qwen_with_thinking(messages, thinking_budget)
        else:
            # æ™®é€šæ¨¡å¼
            completion = client.chat.completions.create(
                model=DASHSCOPE_MODEL,
                messages=messages,
                max_tokens=1500,
                temperature=0.7,
            )
            return {
                "content": completion.choices[0].message.content,
                "reasoning_content": "",
                "thinking_enabled": False
            }
    except Exception as e:
        # å‚è€ƒæ–‡æ¡£: https://help.aliyun.com/zh/model-studio/developer-reference/error-code
        raise HTTPException(status_code=500, detail=f"é€šä¹‰åƒé—®è°ƒç”¨å¤±è´¥: {str(e)}")


def query_qwen_with_thinking(messages: list, thinking_budget: int = 50) -> dict:
    """
    æ€è€ƒæ¨¡å¼è°ƒç”¨ - ä½¿ç”¨æµå¼å“åº”æ”¶é›†æ€è€ƒè¿‡ç¨‹

    æ€è€ƒæ¨¡å¼ä¼šè¿”å›ä¸¤éƒ¨åˆ†å†…å®¹:
    1. reasoning_content: AIçš„æ€è€ƒè¿‡ç¨‹
    2. content: æœ€ç»ˆå›ç­”
    """
    reasoning_content = ""
    answer_content = ""

    try:
        completion = client.chat.completions.create(
            model=DASHSCOPE_MODEL,
            messages=messages,
            extra_body={
                "enable_thinking": True,
                "thinking_budget": thinking_budget
            },
            stream=True,
            stream_options={
                "include_usage": True
            },
        )

        for chunk in completion:
            if chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                delta = choice.delta

                # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹
                if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                    reasoning_content += delta.reasoning_content
                elif hasattr(delta, 'content') and delta.content:
                    answer_content += delta.content

        return {
            "content": answer_content,
            "reasoning_content": reasoning_content,
            "thinking_enabled": True
        }

    except Exception as e:
        # å¦‚æœæ€è€ƒæ¨¡å¼å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼
        print(f"[WARN] æ€è€ƒæ¨¡å¼å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼: {e}")
        completion = client.chat.completions.create(
            model=DASHSCOPE_MODEL,
            messages=messages,
            max_tokens=1500,
            temperature=0.7,
        )
        return {
            "content": completion.choices[0].message.content,
            "reasoning_content": "",
            "thinking_enabled": False
        }

def build_prompt(section_data: Dict[str, str]) -> str:
    """
    æ„å»ºåˆ†ææç¤ºè¯ - å°†ç”¨æˆ·è¾“å…¥çš„å‚æ•°è½¬æ¢ä¸ºPromptæ–‡æœ¬

    ç¤ºä¾‹ï¼š
    è¾“å…¥: {"thawing_time": "4.5", "avg_thawing_time": "4.0", ...}
    è¾“å‡º:
    '''
    è¯·åˆ†æä»¥ä¸‹é£Ÿå“åŠ å·¥æ•°æ®ï¼ˆå®é™…æ•°æ® vs å¹³å‡æ•°æ®ï¼‰ï¼š

    ã€æ¥æ”¶&åŠè§£å†»ã€‘
      è§£å†»æ—¶é—´: å®é™…=4.5 | å¹³å‡=4.0
      ...
    '''
    """
    sections = {
        'æ¥æ”¶&åŠè§£å†»': ['thawing_time', 'drip_loss', 'temperature'],
        'å»å°¾': ['tail_rate', 'trim_rate', 'rework_rate'],
        'æœºæ¢°åˆ‡ç‰‡': ['thickness_sd', 'jam_rate', 'oee'],
        'æ¸…æ´—(å€æ¸©)': ['water_usage', 'outlet_temp', 'micro_pass_rate'],
        'æ²¥å¹²': ['surface_loss', 'dwell_time'],
        'æ·±è¾Šä¸Šæµ†(åŠæˆå“)': ['marinade_absorption', 'ph_salinity', 'marinade_variance'],
        'åŒ…è£…&IQFé€Ÿå†»': ['sec', 'pack_pass_rate', 'cooling_time'],
        'å“æ§&é£Ÿå“å®‰å…¨': ['ccp_pass_rate', 'audit_issues'],
        'æ¸…æ´—&æ¢çº¿': ['clean_duration', 'atp_pass_rate'],
    }

    param_labels = {
        'thawing_time': 'è§£å†»æ—¶é—´', 'drip_loss': 'æ»´æ°´æŸå¤±ç‡(%)', 'temperature': 'æ¸©åº¦(Â°C)',
        'tail_rate': 'å°¾æ®µç‡(%)', 'trim_rate': 'ä¿®æ•´ç‡(%)', 'rework_rate': 'è¿”å·¥ç‡(%)',
        'thickness_sd': 'åšåº¦åå·®SD(mm)', 'jam_rate': 'å¡æœºç‡(%)', 'oee': 'OEE(%)',
        'water_usage': 'å•ä½ç”¨æ°´(L/kg)', 'outlet_temp': 'å‡ºå£æ¸©åº¦(Â°C)',
        'micro_pass_rate': 'å¾®ç”Ÿç‰©æ£€æµ‹åˆæ ¼ç‡(%)',
        'surface_loss': 'è¡¨é¢å¤±æ°´ç‡(%)', 'dwell_time': 'åœç•™æ—¶é—´(min)',
        'marinade_absorption': 'è…Œæ–™å¸æ”¶ç‡(%)', 'ph_salinity': 'pH/ç›åº¦',
        'marinade_variance': 'è…Œæ–™æ¶ˆè€—å·®å¼‚(%)',
        'sec': 'sEC(kWh/kg)', 'pack_pass_rate': 'åŒ…è£…åˆæ ¼ç‡(%)',
        'cooling_time': 'æ ¸å¿ƒé™æ¸©æ—¶é—´(min)',
        'ccp_pass_rate': 'CCPåˆæ ¼ç‡(%)', 'audit_issues': 'å®¡è®¡é—®é¢˜æ•°(ä¸ª)',
        'clean_duration': 'æ¸…æ´æ—¶é•¿(min)', 'atp_pass_rate': 'ATPæ£€æµ‹åˆæ ¼ç‡(%)',
    }

    prompt_parts = ["è¯·åˆ†æä»¥ä¸‹é£Ÿå“åŠ å·¥æ•°æ®ï¼ˆå®é™…æ•°æ® vs å¹³å‡æ•°æ®ï¼‰ï¼š\n"]

    # éå†æ¯ä¸ªç¯èŠ‚
    for section_name, param_keys in sections.items():
        section_text = f"\nã€{section_name}ã€‘\n"
        section_has_data = False

        # éå†æ¯ä¸ªå‚æ•°
        for param_key in param_keys:
            actual_val = section_data.get(param_key, "").strip()
            avg_val = section_data.get(f"avg_{param_key}", "").strip()

            if actual_val or avg_val:
                section_has_data = True
                label = param_labels.get(param_key, param_key)
                # ç»„è£…æˆ: "è§£å†»æ—¶é—´: å®é™…=4.5 | å¹³å‡=4.0"
                section_text += f"  {label}: å®é™…={actual_val or 'æœªå¡«'} | å¹³å‡={avg_val or 'æœªå¡«'}\n"

        if section_has_data:
            prompt_parts.append(section_text)

    if len(prompt_parts) == 1:
        prompt_parts.append("\nâš ï¸ æœªæä¾›ä»»ä½•æ•°æ®")
    else:
        prompt_parts.append("\nè¯·åŸºäºä»¥ä¸Šæ•°æ®è¿›è¡Œæ·±åº¦åˆ†æï¼Œç»™å‡ºä¸“ä¸šå»ºè®®ã€‚")

    return "".join(prompt_parts)

# ==================== APIç«¯ç‚¹ ====================
@app.get("/")
async def root():
    return {
        "service": "é£Ÿå“åŠ å·¥æ•°æ®åˆ†æ API",
        "status": "running",
        "model": f"é˜¿é‡Œäº‘é€šä¹‰åƒé—® ({DASHSCOPE_MODEL})",
        "api_configured": bool(DASHSCOPE_API_KEY)
    }

@app.post("/api/ai/food-processing-analysis", response_model=FoodProcessingResponse)
async def analyze(request: FoodProcessingRequest):
    """
    é£Ÿå“åŠ å·¥æ•°æ®åˆ†æ - æ ¸å¿ƒåŠŸèƒ½

    æµç¨‹ï¼š
    1. æ¥æ”¶section_data (æ‰€æœ‰å‚æ•°)
    2. æ„å»ºPromptæ–‡æœ¬
    3. å‘é€ç»™Llama 3.1
    4. è¿”å›AIåˆ†æç»“æœ
    """
    try:
        # æ­¥éª¤1: æ„å»ºPrompt
        prompt = build_prompt(request.section_data)

        # æ­¥éª¤2: è°ƒç”¨AIæ¨¡å‹
        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯é£Ÿå“åŠ å·¥ä¸“å®¶ï¼Œä¸“é—¨åˆ†æåŠ å·¥æ•°æ®ã€‚

ä»»åŠ¡ï¼š
1. å¯¹æ¯”å®é™…æ•°æ®ä¸å¹³å‡æ•°æ®ï¼Œè¯†åˆ«å·®å¼‚
2. è¯Šæ–­é—®é¢˜å’Œé£é™©ç‚¹
3. æä¾›å…·ä½“ä¼˜åŒ–å»ºè®®
4. åˆ†ææˆæœ¬ä¼˜åŒ–ç©ºé—´

è¦æ±‚ï¼š
- ç®€æ´ä¸“ä¸šçš„è¯­è¨€
- å…·ä½“æ•°å­—å’Œç™¾åˆ†æ¯”å¯¹æ¯”
- å¯é‡åŒ–çš„æ”¹è¿›ç›®æ ‡
- ä¸­æ–‡å›å¤

è¾“å‡ºæ ¼å¼ï¼š
ğŸ“Š **æ€»ä½“è¯„ä¼°**
[æ•´ä½“è¯„ä»·]

ğŸ” **ç¯èŠ‚åˆ†æ**
[é€ç¯èŠ‚åˆ†æå®é™…vså¹³å‡å·®å¼‚]

âš ï¸ **ä¸»è¦é—®é¢˜**
1. [é—®é¢˜åŠå½±å“]

ğŸ’¡ **ä¼˜åŒ–å»ºè®®**
1. [å…·ä½“å»ºè®®]

ğŸ“ˆ **é¢„æœŸæ”¶ç›Š**
[é¢„æœŸæ”¹å–„]"""
            },
            {
                "role": "user",
                "content": prompt  # è¿™é‡Œæ˜¯ç”¨æˆ·æ•°æ®è½¬æ¢æˆçš„Promptæ–‡æœ¬
            }
        ]

        # æ­¥éª¤3: è·å–AIåˆ†æ (æ™®é€šæ¨¡å¼ï¼Œä¸ä½¿ç”¨æ€è€ƒ)
        result = query_qwen(messages, enable_thinking=False)

        # æ­¥éª¤4: è¿”å›ç»“æœ
        return FoodProcessingResponse(
            success=True,
            analysis=result["content"],
            message="åˆ†æå®Œæˆ"
        )

    except Exception as e:
        return FoodProcessingResponse(
            success=False,
            analysis="",
            message=f"åˆ†æå¤±è´¥: {str(e)}"
        )

@app.post("/api/ai/chat")
async def cost_analysis(request: CostAnalysisRequest):
    """
    æˆæœ¬åˆ†æä¸“ç”¨æ¥å£ - ä¸Javaåç«¯é›†æˆ

    æ¥æ”¶æ ¼å¼åŒ–çš„æˆæœ¬æ•°æ®æ–‡æœ¬ï¼Œè¿”å›AIåˆ†æå»ºè®®
    """
    try:
        import uuid
        import time

        # æ„å»ºä¸“é—¨çš„æˆæœ¬åˆ†ææ¶ˆæ¯
        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯é£Ÿå“åŠ å·¥ä¼ä¸šçš„æˆæœ¬åˆ†æä¸“å®¶ã€‚

ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”Ÿäº§æ‰¹æ¬¡çš„æˆæœ¬æ•°æ®ï¼Œæä¾›ä¸“ä¸šçš„æˆæœ¬ä¼˜åŒ–å»ºè®®ã€‚

åˆ†æè¦ç‚¹ï¼š
1. æˆæœ¬ç»“æ„åˆç†æ€§ï¼šè¯„ä¼°åŸææ–™ã€äººå·¥ã€è®¾å¤‡æˆæœ¬çš„å æ¯”æ˜¯å¦åˆç†
2. å¼‚å¸¸è¯†åˆ«ï¼šæ‰¾å‡ºæˆæœ¬æ•°æ®ä¸­çš„å¼‚å¸¸ç‚¹å’Œé£é™©
3. å¯¹æ¯”åˆ†æï¼šå°†å½“å‰æˆæœ¬ä¸è¡Œä¸šæ ‡å‡†æˆ–å†å²æ•°æ®å¯¹æ¯”
4. ä¼˜åŒ–å»ºè®®ï¼šæä¾›å…·ä½“å¯è¡Œçš„æˆæœ¬é™ä½æªæ–½
5. æ•ˆç‡è¯„ä¼°ï¼šåˆ†æç”Ÿäº§æ•ˆç‡ã€è‰¯å“ç‡ã€äººå‡äº§èƒ½ç­‰æŒ‡æ ‡

è¾“å‡ºè¦æ±‚ï¼š
- ä½¿ç”¨ä¸­æ–‡
- ç®€æ´ä¸“ä¸šï¼Œæ¡ç†æ¸…æ™°
- æä¾›å…·ä½“æ•°å­—å’Œç™¾åˆ†æ¯”
- ç»™å‡ºå¯é‡åŒ–çš„æ”¹è¿›ç›®æ ‡
- åˆ†æè¦æ·±å…¥ï¼Œå»ºè®®è¦å…·ä½“

è¾“å‡ºæ ¼å¼ï¼š
ğŸ“Š **æˆæœ¬ç»“æ„åˆ†æ**
[åˆ†æå„é¡¹æˆæœ¬å æ¯”çš„åˆç†æ€§]

âš ï¸ **å‘ç°çš„é—®é¢˜**
1. [é—®é¢˜ç‚¹åŠå½±å“]

ğŸ’¡ **ä¼˜åŒ–å»ºè®®**
1. [å…·ä½“çš„æ”¹è¿›æªæ–½]

ğŸ“ˆ **é¢„æœŸæ•ˆæœ**
[å®æ–½å»ºè®®åçš„é¢„æœŸæˆæœ¬èŠ‚çœ]"""
            },
            {
                "role": "user",
                "content": request.message
            }
        ]

        # è·å–æ€è€ƒæ¨¡å¼é…ç½® (é»˜è®¤å¼€å¯)
        enable_thinking = request.enable_thinking if request.enable_thinking is not None else True
        thinking_budget = request.thinking_budget if request.thinking_budget else 50

        # å°è¯•è°ƒç”¨AIæ¨¡å‹ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›æ¨¡æ‹Ÿåˆ†æï¼ˆç”¨äºæ¼”ç¤ºï¼‰
        try:
            result = query_qwen(messages, enable_thinking=enable_thinking, thinking_budget=thinking_budget)
            ai_analysis = result["content"]
            reasoning_content = result["reasoning_content"]
            thinking_enabled = result["thinking_enabled"]
        except Exception as ai_error:
            # å¦‚æœAIè°ƒç”¨å¤±è´¥ï¼Œè¿”å›åŸºäºè§„åˆ™çš„æ¨¡æ‹Ÿåˆ†æï¼ˆä»…ç”¨äºæ¼”ç¤ºå’Œæµ‹è¯•ï¼‰
            print(f"[WARN] AIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿåˆ†æ: {ai_error}")
            ai_analysis = generate_mock_analysis(request.message)
            reasoning_content = ""
            thinking_enabled = False

        # ç”Ÿæˆä¼šè¯IDï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
        session_id = request.session_id if request.session_id else f"session_{uuid.uuid4().hex[:16]}"

        # è¿”å›ç»“æœï¼ˆåŒ¹é…JavaæœŸæœ›çš„æ ¼å¼ï¼Œå¢åŠ æ€è€ƒå†…å®¹ï¼‰
        return {
            "success": True,
            "aiAnalysis": ai_analysis,
            "reasoningContent": reasoning_content,  # æ€è€ƒè¿‡ç¨‹
            "thinkingEnabled": thinking_enabled,    # æ˜¯å¦ä½¿ç”¨äº†æ€è€ƒæ¨¡å¼
            "sessionId": session_id,
            "messageCount": 1,
            "timestamp": int(time.time() * 1000)
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"AIåˆ†æå¤±è´¥: {str(e)}")


@app.post("/api/ai/chat/stream")
async def cost_analysis_stream(request: CostAnalysisRequest):
    """
    æˆæœ¬åˆ†æä¸“ç”¨æ¥å£ - æµå¼å“åº”ç‰ˆæœ¬ (SSE)

    å®æ—¶è¿”å›AIåˆ†æè¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ
    """
    import uuid
    import time

    async def event_generator():
        try:
            # å‘é€å¼€å§‹äº‹ä»¶
            yield f"data: {json.dumps({'type': 'start', 'timestamp': int(time.time() * 1000)})}\n\n"

            # æ„å»ºä¸“é—¨çš„æˆæœ¬åˆ†ææ¶ˆæ¯
            messages = [
                {
                    "role": "system",
                    "content": """ä½ æ˜¯é£Ÿå“åŠ å·¥ä¼ä¸šçš„æˆæœ¬åˆ†æä¸“å®¶ã€‚

ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”Ÿäº§æ‰¹æ¬¡çš„æˆæœ¬æ•°æ®ï¼Œæä¾›ä¸“ä¸šçš„æˆæœ¬ä¼˜åŒ–å»ºè®®ã€‚

åˆ†æè¦ç‚¹ï¼š
1. æˆæœ¬ç»“æ„åˆç†æ€§ï¼šè¯„ä¼°åŸææ–™ã€äººå·¥ã€è®¾å¤‡æˆæœ¬çš„å æ¯”æ˜¯å¦åˆç†
2. å¼‚å¸¸è¯†åˆ«ï¼šæ‰¾å‡ºæˆæœ¬æ•°æ®ä¸­çš„å¼‚å¸¸ç‚¹å’Œé£é™©
3. å¯¹æ¯”åˆ†æï¼šå°†å½“å‰æˆæœ¬ä¸è¡Œä¸šæ ‡å‡†æˆ–å†å²æ•°æ®å¯¹æ¯”
4. ä¼˜åŒ–å»ºè®®ï¼šæä¾›å…·ä½“å¯è¡Œçš„æˆæœ¬é™ä½æªæ–½
5. æ•ˆç‡è¯„ä¼°ï¼šåˆ†æç”Ÿäº§æ•ˆç‡ã€è‰¯å“ç‡ã€äººå‡äº§èƒ½ç­‰æŒ‡æ ‡

è¾“å‡ºè¦æ±‚ï¼š
- ä½¿ç”¨ä¸­æ–‡
- ç®€æ´ä¸“ä¸šï¼Œæ¡ç†æ¸…æ™°
- æä¾›å…·ä½“æ•°å­—å’Œç™¾åˆ†æ¯”
- ç»™å‡ºå¯é‡åŒ–çš„æ”¹è¿›ç›®æ ‡
- åˆ†æè¦æ·±å…¥ï¼Œå»ºè®®è¦å…·ä½“

è¾“å‡ºæ ¼å¼ï¼š
ğŸ“Š **æˆæœ¬ç»“æ„åˆ†æ**
[åˆ†æå„é¡¹æˆæœ¬å æ¯”çš„åˆç†æ€§]

âš ï¸ **å‘ç°çš„é—®é¢˜**
1. [é—®é¢˜ç‚¹åŠå½±å“]

ğŸ’¡ **ä¼˜åŒ–å»ºè®®**
1. [å…·ä½“çš„æ”¹è¿›æªæ–½]

ğŸ“ˆ **é¢„æœŸæ•ˆæœ**
[å®æ–½å»ºè®®åçš„é¢„æœŸæˆæœ¬èŠ‚çœ]"""
                },
                {
                    "role": "user",
                    "content": request.message
                }
            ]

            # è·å–æ€è€ƒæ¨¡å¼é…ç½®
            enable_thinking = request.enable_thinking if request.enable_thinking is not None else True
            thinking_budget = request.thinking_budget if request.thinking_budget else 50

            if not client:
                yield f"data: {json.dumps({'type': 'error', 'message': 'DASHSCOPE_API_KEYæœªé…ç½®'})}\n\n"
                return

            reasoning_content = ""
            answer_content = ""

            try:
                # å¼€å¯æµå¼æ¨¡å¼
                completion = client.chat.completions.create(
                    model=DASHSCOPE_MODEL,
                    messages=messages,
                    extra_body={
                        "enable_thinking": enable_thinking,
                        "thinking_budget": thinking_budget
                    } if enable_thinking else {},
                    stream=True,
                    stream_options={"include_usage": True} if enable_thinking else None,
                )

                # é€å—å‘é€å“åº”
                for chunk in completion:
                    if chunk.choices and len(chunk.choices) > 0:
                        choice = chunk.choices[0]
                        delta = choice.delta

                        # å‘é€æ€è€ƒå†…å®¹
                        if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                            reasoning_content += delta.reasoning_content
                            yield f"data: {json.dumps({'type': 'thinking', 'content': delta.reasoning_content})}\n\n"

                        # å‘é€å›ç­”å†…å®¹
                        elif hasattr(delta, 'content') and delta.content:
                            answer_content += delta.content
                            yield f"data: {json.dumps({'type': 'answer', 'content': delta.content})}\n\n"

            except Exception as ai_error:
                print(f"[WARN] æµå¼AIè°ƒç”¨å¤±è´¥: {ai_error}")
                # å›é€€åˆ°æ¨¡æ‹Ÿåˆ†æ
                answer_content = generate_mock_analysis(request.message)
                yield f"data: {json.dumps({'type': 'answer', 'content': answer_content})}\n\n"

            # ç”Ÿæˆä¼šè¯ID
            session_id = request.session_id if request.session_id else f"session_{uuid.uuid4().hex[:16]}"

            # å‘é€å®Œæˆäº‹ä»¶
            yield f"data: {json.dumps({'type': 'complete', 'sessionId': session_id, 'reasoningContent': reasoning_content, 'answerContent': answer_content, 'timestamp': int(time.time() * 1000)})}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # ç¦ç”¨nginxç¼“å†²
        }
    )


def generate_mock_analysis(cost_data: str) -> str:
    """
    ç”Ÿæˆæ¨¡æ‹Ÿçš„æˆæœ¬åˆ†æï¼ˆç”¨äºæ¼”ç¤ºï¼Œå½“AI APIä¸å¯ç”¨æ—¶ï¼‰
    """
    # ä»æˆæœ¬æ•°æ®ä¸­æå–å…³é”®ä¿¡æ¯
    lines = cost_data.split('\n')
    batch_number = ""
    product_name = ""
    total_cost = 0
    material_cost_ratio = 0
    labor_cost_ratio = 0
    equipment_cost_ratio = 0
    yield_rate = 0

    for line in lines:
        if "æ‰¹æ¬¡ç¼–å·:" in line:
            batch_number = line.split(':')[1].strip()
        elif "äº§å“åç§°:" in line:
            product_name = line.split(':')[1].strip()
        elif "æ€»æˆæœ¬:" in line:
            # æå–æ•°å­—
            import re
            match = re.search(r'Â¥([\d,]+)', line)
            if match:
                total_cost = int(match.group(1).replace(',', ''))
        elif "åŸææ–™æˆæœ¬:" in line and "å æ¯”" in line:
            match = re.search(r'å æ¯”([\d.]+)%', line)
            if match:
                material_cost_ratio = float(match.group(1))
        elif "äººå·¥æˆæœ¬:" in line and "å æ¯”" in line:
            match = re.search(r'å æ¯”([\d.]+)%', line)
            if match:
                labor_cost_ratio = float(match.group(1))
        elif "è®¾å¤‡æˆæœ¬:" in line and "å æ¯”" in line:
            match = re.search(r'å æ¯”([\d.]+)%', line)
            if match:
                equipment_cost_ratio = float(match.group(1))
        elif "è‰¯å“ç‡:" in line:
            match = re.search(r'([\d.]+)%', line)
            if match:
                yield_rate = float(match.group(1))

    # åŸºäºæ•°æ®ç”Ÿæˆåˆ†æ
    analysis = f"""ğŸ“Š **æˆæœ¬ç»“æ„åˆ†æ**

æ ¹æ®æ‰¹æ¬¡ {batch_number} ({product_name}) çš„æˆæœ¬æ•°æ®ï¼Œæ€»æˆæœ¬ä¸º Â¥{total_cost:,}ï¼Œæˆæœ¬ç»“æ„å¦‚ä¸‹ï¼š

- åŸææ–™æˆæœ¬å æ¯” {material_cost_ratio:.1f}%
- äººå·¥æˆæœ¬å æ¯” {labor_cost_ratio:.1f}%
- è®¾å¤‡æˆæœ¬å æ¯” {equipment_cost_ratio:.1f}%

**ç»“æ„è¯„ä¼°ï¼š**
"""

    # åŸææ–™æˆæœ¬åˆ†æ
    if material_cost_ratio > 60:
        analysis += f"â€¢ åŸææ–™æˆæœ¬å æ¯” {material_cost_ratio:.1f}% åé«˜ï¼Œå»ºè®®ä¼˜åŒ–é‡‡è´­ç­–ç•¥\n"
    elif material_cost_ratio < 45:
        analysis += f"â€¢ åŸææ–™æˆæœ¬å æ¯” {material_cost_ratio:.1f}% åˆç†ï¼Œé‡‡è´­æ§åˆ¶è‰¯å¥½\n"
    else:
        analysis += f"â€¢ åŸææ–™æˆæœ¬å æ¯” {material_cost_ratio:.1f}% å¤„äºæ­£å¸¸èŒƒå›´\n"

    # äººå·¥æˆæœ¬åˆ†æ
    if labor_cost_ratio > 35:
        analysis += f"â€¢ äººå·¥æˆæœ¬å æ¯” {labor_cost_ratio:.1f}% è¾ƒé«˜ï¼Œå­˜åœ¨äººå‘˜æ•ˆç‡ä¼˜åŒ–ç©ºé—´\n"
    else:
        analysis += f"â€¢ äººå·¥æˆæœ¬å æ¯” {labor_cost_ratio:.1f}% åˆç†\n"

    # è®¾å¤‡æˆæœ¬åˆ†æ
    if equipment_cost_ratio < 15:
        analysis += f"â€¢ è®¾å¤‡æˆæœ¬å æ¯” {equipment_cost_ratio:.1f}% åˆç†ï¼Œè®¾å¤‡åˆ©ç”¨ç‡è‰¯å¥½\n"

    analysis += f"\nâš ï¸ **å‘ç°çš„é—®é¢˜**\n\n"

    problems = []
    if yield_rate < 98:
        problems.append(f"1. è‰¯å“ç‡ {yield_rate:.1f}% ä½äºè¡Œä¸šæ ‡å‡†98%ï¼Œé€ æˆåŸææ–™æµªè´¹å’Œæˆæœ¬å¢åŠ ")
    if labor_cost_ratio > 35:
        problems.append(f"2. äººå·¥æˆæœ¬å æ¯” {labor_cost_ratio:.1f}% åé«˜ï¼Œå¯èƒ½å­˜åœ¨äººå‘˜å†—ä½™æˆ–æ•ˆç‡ä¸è¶³")
    if material_cost_ratio > 60:
        problems.append(f"3. åŸææ–™æˆæœ¬å æ¯” {material_cost_ratio:.1f}% è¿‡é«˜ï¼Œéœ€è¦å®¡æŸ¥ä¾›åº”å•†æŠ¥ä»·å’Œé‡‡è´­æµç¨‹")

    if problems:
        analysis += "\n".join(problems)
    else:
        analysis += "1. æœªå‘ç°é‡å¤§æˆæœ¬å¼‚å¸¸ï¼Œæ•´ä½“æ§åˆ¶è‰¯å¥½\n"

    analysis += f"\n\nğŸ’¡ **ä¼˜åŒ–å»ºè®®**\n\n"

    suggestions = []
    if yield_rate < 98:
        target_saving = total_cost * (98 - yield_rate) / 100 * 0.5
        suggestions.append(f"1. **æå‡è‰¯å“ç‡**ï¼šåŠ å¼ºè´¨é‡æ§åˆ¶ï¼Œç›®æ ‡æå‡è‡³98%ä»¥ä¸Šï¼Œé¢„è®¡èŠ‚çœæˆæœ¬çº¦Â¥{target_saving:,.0f}")

    if labor_cost_ratio > 35:
        target_ratio = 30
        target_saving = total_cost * (labor_cost_ratio - target_ratio) / 100
        suggestions.append(f"2. **ä¼˜åŒ–äººå‘˜é…ç½®**ï¼šé€šè¿‡æµç¨‹ä¼˜åŒ–å’ŒåŸ¹è®­æå‡äººå‡äº§èƒ½ï¼Œç›®æ ‡é™ä½äººå·¥æˆæœ¬è‡³30%ï¼Œé¢„è®¡èŠ‚çœÂ¥{target_saving:,.0f}")

    if material_cost_ratio > 60:
        target_saving = total_cost * 0.05
        suggestions.append(f"3. **é‡‡è´­ä¼˜åŒ–**ï¼šå¯¹æ¯”å¤šå®¶ä¾›åº”å•†æŠ¥ä»·ï¼Œæ‰¹é‡é‡‡è´­è°ˆåˆ¤ï¼Œé¢„è®¡å¯é™ä½é‡‡è´­æˆæœ¬3-5%ï¼Œçº¦Â¥{target_saving:,.0f}")

    suggestions.append("4. **è®¾å¤‡åˆ©ç”¨ç‡**ï¼šä¿æŒç°æœ‰è®¾å¤‡åˆ©ç”¨æ°´å¹³ï¼Œå®šæœŸç»´æŠ¤ä¿å…»å»¶é•¿ä½¿ç”¨å¯¿å‘½")

    analysis += "\n".join(suggestions)

    # é¢„æœŸæ”¶ç›Š
    total_potential_saving = sum([
        total_cost * (98 - yield_rate) / 100 * 0.5 if yield_rate < 98 else 0,
        total_cost * (labor_cost_ratio - 30) / 100 if labor_cost_ratio > 35 else 0,
        total_cost * 0.05 if material_cost_ratio > 60 else 0
    ])

    if total_potential_saving > 0:
        new_unit_cost_estimate = (total_cost - total_potential_saving) / (total_cost / 7.20)  # å‡è®¾å•ä½æˆæœ¬7.20
        analysis += f"\n\nğŸ“ˆ **é¢„æœŸæ•ˆæœ**\n\n"
        analysis += f"å®æ–½ä»¥ä¸Šä¼˜åŒ–æªæ–½åï¼š\n"
        analysis += f"â€¢ é¢„è®¡æ€»æˆæœ¬å¯ä» Â¥{total_cost:,} é™ä½è‡³ Â¥{total_cost - total_potential_saving:,.0f}\n"
        analysis += f"â€¢ æˆæœ¬èŠ‚çœçº¦ Â¥{total_potential_saving:,.0f} ({total_potential_saving/total_cost*100:.1f}%)\n"
        analysis += f"â€¢ å•ä½æˆæœ¬é¢„è®¡ä» Â¥7.20/kg é™è‡³ Â¥{new_unit_cost_estimate:.2f}/kg\n"
        analysis += f"â€¢ æ•´ä½“åˆ©æ¶¦ç‡å¯æå‡ {total_potential_saving/total_cost*100:.1f} ä¸ªç™¾åˆ†ç‚¹"
    else:
        analysis += f"\n\nğŸ“ˆ **é¢„æœŸæ•ˆæœ**\n\n"
        analysis += f"å½“å‰æˆæœ¬æ§åˆ¶å·²ç»è¾ƒä¸ºä¼˜ç§€ï¼Œå»ºè®®ï¼š\n"
        analysis += f"â€¢ ä¿æŒç°æœ‰æˆæœ¬ç®¡ç†æ°´å¹³\n"
        analysis += f"â€¢ æŒç»­ç›‘æ§å„é¡¹æˆæœ¬æŒ‡æ ‡\n"
        analysis += f"â€¢ æ¢ç´¢è‡ªåŠ¨åŒ–å’ŒæŠ€æœ¯å‡çº§æœºä¼š"

    analysis += "\n\n---\nğŸ’¡ *æœ¬åˆ†æåŸºäºæä¾›çš„æˆæœ¬æ•°æ®ç”Ÿæˆï¼Œå…·ä½“å®æ–½è¯·ç»“åˆå·¥å‚å®é™…æƒ…å†µè°ƒæ•´*"

    return analysis


# ==================== AIè¡¨å•åŠ©æ‰‹æœåŠ¡ ====================

class FormParseRequest(BaseModel):
    """è¡¨å•è§£æè¯·æ±‚"""
    user_input: str  # ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ï¼ˆè¯­éŸ³è½¬æ–‡å­—åçš„å†…å®¹ï¼‰
    form_fields: List[Dict]  # è¡¨å•å­—æ®µå®šä¹‰ [{"name": "materialType", "title": "åŸæ–™ç±»å‹", "type": "string"}]
    entity_type: str  # å®ä½“ç±»å‹ï¼Œå¦‚ MATERIAL_BATCH, QUALITY_CHECK
    factory_id: Optional[str] = None
    context: Optional[Dict] = None  # å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    session_id: Optional[str] = None  # ä¼šè¯IDï¼Œç”¨äºå¤šè½®å¯¹è¯
    validation_errors: Optional[List[str]] = None  # æ ¡éªŒé”™è¯¯åˆ—è¡¨ï¼ˆç”¨äºåé¦ˆä¿®æ­£ï¼‰
    previous_values: Optional[Dict] = None  # ä¹‹å‰å¡«å†™çš„å€¼ï¼ˆç”¨äºåé¦ˆä¿®æ­£ï¼‰

class FormParseResponse(BaseModel):
    """è¡¨å•è§£æå“åº”"""
    success: bool
    field_values: Dict  # è§£æå‡ºçš„å­—æ®µå€¼ {"materialType": "å¸¦é±¼", "quantity": 500}
    confidence: float  # ç½®ä¿¡åº¦ 0-1
    unparsed_text: Optional[str] = None  # æœªèƒ½è§£æçš„éƒ¨åˆ†
    message: Optional[str] = None
    session_id: Optional[str] = None  # ä¼šè¯ID
    validation_errors: Optional[List[str]] = None  # æ ¡éªŒé”™è¯¯åˆ—è¡¨
    correction_hints: Optional[Dict[str, str]] = None  # å­—æ®µä¿®æ­£å»ºè®® {"quantity": "è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°é‡ï¼Œå¦‚ï¼š500å…¬æ–¤"}
    missing_required_fields: Optional[List[str]] = None  # ç¼ºå¤±çš„å¿…å¡«å­—æ®µ
    suggested_questions: Optional[List[str]] = None  # AIç”Ÿæˆçš„è¿½é—®


class ValidationFeedbackRequest(BaseModel):
    """æ ¡éªŒåé¦ˆè¯·æ±‚ - ç”¨äºè¡¨å•æ ¡éªŒå¤±è´¥æ—¶çš„AIä¿®æ­£"""
    session_id: Optional[str] = None  # ä¼šè¯ID
    entity_type: str  # å®ä½“ç±»å‹
    form_fields: List[Dict]  # è¡¨å•å­—æ®µå®šä¹‰
    submitted_values: Dict  # ç”¨æˆ·æäº¤çš„å€¼
    validation_errors: List[Dict]  # æ ¡éªŒé”™è¯¯ [{"field": "quantity", "message": "å¿…é¡»å¤§äº0"}]
    user_instruction: Optional[str] = None  # ç”¨æˆ·è¡¥å……è¯´æ˜
    factory_id: Optional[str] = None


class ValidationFeedbackResponse(BaseModel):
    """æ ¡éªŒåé¦ˆå“åº”"""
    success: bool
    correction_hints: Dict[str, str]  # å­—æ®µä¿®æ­£å»ºè®®
    corrected_values: Optional[Dict] = None  # AIå»ºè®®çš„ä¿®æ­£å€¼
    explanation: Optional[str] = None  # AIè§£é‡Š
    confidence: float = 0.0
    session_id: Optional[str] = None

class OCRParseRequest(BaseModel):
    """OCRè§£æè¯·æ±‚"""
    image_base64: str  # Base64ç¼–ç çš„å›¾ç‰‡
    form_fields: List[Dict]  # è¡¨å•å­—æ®µå®šä¹‰
    entity_type: str
    factory_id: Optional[str] = None

class OCRParseResponse(BaseModel):
    """OCRè§£æå“åº”"""
    success: bool
    extracted_text: str  # OCRè¯†åˆ«çš„åŸå§‹æ–‡æœ¬


# ==================== AI å·¥å‚æ‰¹é‡åˆå§‹åŒ–æœåŠ¡ ====================

class FactoryBatchInitRequest(BaseModel):
    """å·¥å‚æ‰¹é‡åˆå§‹åŒ–è¯·æ±‚"""
    factory_description: str  # ç”¨æˆ·å¯¹å·¥å‚çš„æè¿° (å¦‚: "è¿™æ˜¯ä¸€ä¸ªæ°´äº§åŠ å·¥å‚ï¼Œä¸»è¦ç”Ÿäº§å¸¦é±¼ç½å¤´ï¼Œéœ€è¦åŸæ–™å…¥åº“ã€ç”Ÿäº§ã€è´¨æ£€ã€å‡ºè´§å…¨æµç¨‹")
    industry_hint: Optional[str] = None  # è¡Œä¸šæç¤º (seafood_processing, prepared_food, etc.)
    factory_id: Optional[str] = None
    factory_name: Optional[str] = None
    include_business_data: Optional[bool] = True  # æ˜¯å¦åŒ…å«å»ºè®®çš„ä¸šåŠ¡æ•°æ®


class EntitySchemaDefinition(BaseModel):
    """å•ä¸ªå®ä½“ç±»å‹çš„å®Œæ•´ Schema"""
    entity_type: str  # MATERIAL_BATCH, QUALITY_CHECK, etc.
    entity_name: str  # åŸææ–™æ‰¹æ¬¡, è´¨æ£€è®°å½•, etc.
    fields: List[Dict]  # Formily æ ¼å¼çš„å­—æ®µåˆ—è¡¨
    description: Optional[str] = None


class SuggestedBusinessData(BaseModel):
    """å»ºè®®çš„ä¸šåŠ¡æ•°æ®"""
    product_types: List[Dict]  # [{"code": "PT001", "name": "å¸¦é±¼ç½å¤´", "description": "..."}]
    material_types: List[Dict]  # [{"code": "MT001", "name": "å¸¦é±¼", "unit": "kg", ...}]
    conversion_rates: Optional[List[Dict]] = None  # [{"materialType": "MT001", "productType": "PT001", "rate": 0.7}]


class FactoryBatchInitResponse(BaseModel):
    """å·¥å‚æ‰¹é‡åˆå§‹åŒ–å“åº”"""
    success: bool
    schemas: List[EntitySchemaDefinition]  # æ‰€æœ‰å®ä½“ç±»å‹çš„ Schema
    suggested_data: Optional[SuggestedBusinessData] = None  # å»ºè®®çš„ä¸šåŠ¡æ•°æ®
    industry_code: str  # è¯†åˆ«çš„è¡Œä¸šä»£ç 
    industry_name: str  # è¡Œä¸šåç§°
    ai_summary: Optional[str] = None  # AI æ€»ç»“
    message: Optional[str] = None


def build_factory_init_prompt() -> str:
    """
    æ„å»ºå·¥å‚åˆå§‹åŒ–çš„ç³»ç»Ÿæç¤ºè¯
    """
    return """ä½ æ˜¯ç™½å©çºªé£Ÿå“æº¯æºç³»ç»Ÿçš„å·¥å‚åˆå§‹åŒ–åŠ©æ‰‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·å¯¹å·¥å‚çš„æè¿°ï¼Œç”Ÿæˆå®Œæ•´çš„è¡¨å•é…ç½®å’Œä¸šåŠ¡æ•°æ®å»ºè®®ã€‚

æ”¯æŒçš„è¡¨å•ç±»å‹ (EntityType):
1. MATERIAL_BATCH - åŸææ–™æ‰¹æ¬¡å…¥åº“
2. PROCESSING_BATCH - ç”Ÿäº§åŠ å·¥æ‰¹æ¬¡
3. QUALITY_CHECK - è´¨æ£€è®°å½•
4. SHIPMENT - å‡ºè´§è®°å½•
5. EQUIPMENT - è®¾å¤‡ä¿¡æ¯
6. DISPOSAL_RECORD - æŠ¥åºŸ/å¤„ç½®è®°å½•

æ¯ä¸ªè¡¨å•ç±»å‹éœ€è¦ç”Ÿæˆçš„å­—æ®µåº”è¯¥åŒ…å«:
- åŸºæœ¬ä¿¡æ¯å­—æ®µ (ç¼–å·ã€åç§°ã€æ—¥æœŸç­‰)
- è¡Œä¸šç‰¹æœ‰å­—æ®µ (å¦‚æ°´äº§çš„æ¸©åº¦ã€å†»å“ç±»å‹ï¼›é¢„åˆ¶èœçš„è¾£åº¦ã€å£å‘³ç­‰)
- è´¨é‡æ§åˆ¶å­—æ®µ (æ£€æµ‹é¡¹ç›®ã€åˆæ ¼æ ‡å‡†ç­‰)

å¯ç”¨çš„ Formily ç»„ä»¶:
- Input: å•è¡Œæ–‡æœ¬
- Input.TextArea: å¤šè¡Œæ–‡æœ¬
- NumberPicker: æ•°å­— (æ”¯æŒ min, max)
- Select: ä¸‹æ‹‰é€‰æ‹© (éœ€è¦ enum)
- DatePicker: æ—¥æœŸé€‰æ‹©
- Switch: å¼€å…³
- Upload: æ–‡ä»¶ä¸Šä¼ 
- Rate: è¯„åˆ†

è¾“å‡ºæ ¼å¼ (ä¸¥æ ¼JSON):
{
  "industry_code": "seafood_processing",
  "industry_name": "æ°´äº§åŠ å·¥",
  "schemas": [
    {
      "entity_type": "MATERIAL_BATCH",
      "entity_name": "åŸææ–™æ‰¹æ¬¡",
      "description": "è®°å½•åŸææ–™å…¥åº“ä¿¡æ¯",
      "fields": [
        {
          "name": "materialType",
          "title": "åŸæ–™ç±»å‹",
          "type": "string",
          "x_component": "Select",
          "enum": [{"label": "å¸¦é±¼", "value": "daiyu"}, {"label": "é…¸èœ", "value": "suancai"}],
          "required": true
        }
      ]
    }
  ],
  "suggested_data": {
    "product_types": [
      {"code": "PT001", "name": "å¸¦é±¼ç½å¤´", "description": "ç»å…¸å¸¦é±¼ç½å¤´äº§å“"}
    ],
    "material_types": [
      {"code": "MT001", "name": "å¸¦é±¼", "unit": "kg", "description": "æ–°é²œæˆ–å†·å†»å¸¦é±¼"}
    ],
    "conversion_rates": [
      {"materialTypeCode": "MT001", "productTypeCode": "PT001", "rate": 0.7, "description": "1kgå¸¦é±¼äº§å‡º0.7kgç½å¤´"}
    ]
  },
  "ai_summary": "æ ¹æ®æ‚¨çš„æè¿°ï¼Œå·²ä¸ºæ°´äº§åŠ å·¥å‚ç”Ÿæˆ6ä¸ªè¡¨å•æ¨¡æ¿ï¼ŒåŒ…å«å¸¦é±¼ç½å¤´çš„å…¨æµç¨‹é…ç½®..."
}

æ³¨æ„:
- å­—æ®µåä½¿ç”¨ camelCase
- æ ¹æ®è¡Œä¸šç‰¹ç‚¹æ·»åŠ è¡Œä¸šç‰¹æœ‰å­—æ®µ
- è´¨æ£€è¡¨å•è¦åŒ…å«è¡Œä¸šå¸¸è§çš„æ£€æµ‹é¡¹ç›®
- å»ºè®®çš„ä¸šåŠ¡æ•°æ®è¦ç¬¦åˆç”¨æˆ·æè¿°çš„äº§å“
- è½¬æ¢ç‡æ ¹æ®è¡Œä¸šç»éªŒç»™å‡ºåˆç†ä¼°è®¡"""


@app.post("/api/ai/factory/batch-initialize", response_model=FactoryBatchInitResponse)
async def batch_initialize_factory(request: FactoryBatchInitRequest):
    """
    AI å·¥å‚æ‰¹é‡åˆå§‹åŒ– - æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰è¡¨å•é…ç½®

    ç”¨é€”:
    - æ–°å·¥å‚å¿«é€Ÿä¸Šçº¿ (5åˆ†é’Ÿ)
    - æ ¹æ® SOP æ–‡æ¡£æè¿°ç”Ÿæˆå®Œæ•´é…ç½®
    - åŒ…å«äº§å“ç±»å‹ã€åŸæ–™ç±»å‹ç­‰ä¸šåŠ¡æ•°æ®å»ºè®®

    ç¤ºä¾‹è¾“å…¥:
    "è¿™æ˜¯ä¸€ä¸ªæ°´äº§å“åŠ å·¥å‚ï¼Œä¸»è¦ç”Ÿäº§å¸¦é±¼ç½å¤´ï¼Œéœ€è¦åŸæ–™å…¥åº“ã€ç”Ÿäº§ã€è´¨æ£€ã€å‡ºè´§å…¨æµç¨‹"

    ç¤ºä¾‹è¾“å‡º:
    - 6ä¸ª EntityType çš„å®Œæ•´ Schema (MATERIAL_BATCH, PROCESSING_BATCH, QUALITY_CHECK, SHIPMENT, EQUIPMENT, DISPOSAL_RECORD)
    - å»ºè®®çš„äº§å“ç±»å‹: [å¸¦é±¼ç½å¤´]
    - å»ºè®®çš„åŸæ–™ç±»å‹: [å¸¦é±¼]
    - å»ºè®®çš„è½¬æ¢ç‡é…ç½®
    """
    try:
        if not request.factory_description or not request.factory_description.strip():
            return FactoryBatchInitResponse(
                success=False,
                schemas=[],
                industry_code="",
                industry_name="",
                message="å·¥å‚æè¿°ä¸èƒ½ä¸ºç©º"
            )

        # æ„å»ºæç¤ºè¯
        system_prompt = build_factory_init_prompt()

        # æ·»åŠ è¡Œä¸šæç¤º
        user_content = f"å·¥å‚æè¿°: {request.factory_description}"
        if request.industry_hint:
            user_content += f"\nè¡Œä¸šæç¤º: {request.industry_hint}"
        if request.factory_name:
            user_content += f"\nå·¥å‚åç§°: {request.factory_name}"

        user_content += "\n\nè¯·ç”Ÿæˆå®Œæ•´çš„è¡¨å•é…ç½®å’Œä¸šåŠ¡æ•°æ®å»ºè®®ã€‚"

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content}
        ]

        # è°ƒç”¨AI (ä½¿ç”¨è¾ƒé«˜ token é™åˆ¶ï¼Œå› ä¸ºè¾“å‡ºè¾ƒé•¿)
        try:
            if not client:
                raise Exception("DASHSCOPE_API_KEYæœªé…ç½®")

            completion = client.chat.completions.create(
                model=DASHSCOPE_MODEL,
                messages=messages,
                max_tokens=4000,  # è¾ƒé•¿è¾“å‡º
                temperature=0.7,
            )
            response_text = completion.choices[0].message.content.strip()

        except Exception as ai_error:
            # AI è°ƒç”¨å¤±è´¥ï¼Œè¿”å›é»˜è®¤æ¨¡æ¿
            print(f"[WARN] AIè°ƒç”¨å¤±è´¥: {ai_error}")
            return generate_default_factory_config(request)

        # è§£æJSONå“åº”
        try:
            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()

            parsed = json.loads(response_text)

            # æå– schemas
            schemas = []
            for s in parsed.get("schemas", []):
                schema = EntitySchemaDefinition(
                    entity_type=s.get("entity_type", ""),
                    entity_name=s.get("entity_name", ""),
                    fields=s.get("fields", []),
                    description=s.get("description")
                )
                schemas.append(schema)

            # æå–å»ºè®®çš„ä¸šåŠ¡æ•°æ®
            suggested_data = None
            if request.include_business_data and "suggested_data" in parsed:
                sd = parsed["suggested_data"]
                suggested_data = SuggestedBusinessData(
                    product_types=sd.get("product_types", []),
                    material_types=sd.get("material_types", []),
                    conversion_rates=sd.get("conversion_rates")
                )

            return FactoryBatchInitResponse(
                success=True,
                schemas=schemas,
                suggested_data=suggested_data,
                industry_code=parsed.get("industry_code", "general"),
                industry_name=parsed.get("industry_name", "é€šç”¨åŠ å·¥"),
                ai_summary=parsed.get("ai_summary"),
                message=f"æˆåŠŸç”Ÿæˆ {len(schemas)} ä¸ªè¡¨å•æ¨¡æ¿"
            )

        except json.JSONDecodeError as e:
            return FactoryBatchInitResponse(
                success=False,
                schemas=[],
                industry_code="",
                industry_name="",
                message=f"AIè¿”å›æ ¼å¼é”™è¯¯: {str(e)}"
            )

    except HTTPException as e:
        raise e
    except Exception as e:
        return FactoryBatchInitResponse(
            success=False,
            schemas=[],
            industry_code="",
            industry_name="",
            message=f"å·¥å‚åˆå§‹åŒ–å¤±è´¥: {str(e)}"
        )


def generate_default_factory_config(request: FactoryBatchInitRequest) -> FactoryBatchInitResponse:
    """
    ç”Ÿæˆé»˜è®¤çš„å·¥å‚é…ç½® (å½“AIä¸å¯ç”¨æ—¶çš„å›é€€)
    """
    # é»˜è®¤æ°´äº§åŠ å·¥æ¨¡æ¿
    default_schemas = [
        EntitySchemaDefinition(
            entity_type="MATERIAL_BATCH",
            entity_name="åŸææ–™æ‰¹æ¬¡",
            description="è®°å½•åŸææ–™å…¥åº“ä¿¡æ¯",
            fields=[
                {"name": "batchNumber", "title": "æ‰¹æ¬¡ç¼–å·", "type": "string", "x_component": "Input", "required": True},
                {"name": "materialType", "title": "åŸæ–™ç±»å‹", "type": "string", "x_component": "Select", "required": True},
                {"name": "quantity", "title": "æ•°é‡", "type": "number", "x_component": "NumberPicker", "required": True},
                {"name": "unit", "title": "å•ä½", "type": "string", "x_component": "Select", "enum": [{"label": "kg", "value": "kg"}, {"label": "ä¸ª", "value": "pcs"}]},
                {"name": "temperature", "title": "æ¸©åº¦(Â°C)", "type": "number", "x_component": "NumberPicker"},
                {"name": "supplierId", "title": "ä¾›åº”å•†", "type": "string", "x_component": "Select"},
                {"name": "receivedDate", "title": "å…¥åº“æ—¥æœŸ", "type": "string", "x_component": "DatePicker", "required": True},
            ]
        ),
        EntitySchemaDefinition(
            entity_type="PROCESSING_BATCH",
            entity_name="ç”Ÿäº§æ‰¹æ¬¡",
            description="è®°å½•ç”Ÿäº§åŠ å·¥ä¿¡æ¯",
            fields=[
                {"name": "batchNumber", "title": "æ‰¹æ¬¡ç¼–å·", "type": "string", "x_component": "Input", "required": True},
                {"name": "productType", "title": "äº§å“ç±»å‹", "type": "string", "x_component": "Select", "required": True},
                {"name": "plannedQuantity", "title": "è®¡åˆ’æ•°é‡", "type": "number", "x_component": "NumberPicker", "required": True},
                {"name": "actualQuantity", "title": "å®é™…äº§å‡º", "type": "number", "x_component": "NumberPicker"},
                {"name": "startTime", "title": "å¼€å§‹æ—¶é—´", "type": "string", "x_component": "DatePicker"},
                {"name": "endTime", "title": "ç»“æŸæ—¶é—´", "type": "string", "x_component": "DatePicker"},
            ]
        ),
        EntitySchemaDefinition(
            entity_type="QUALITY_CHECK",
            entity_name="è´¨æ£€è®°å½•",
            description="è®°å½•è´¨é‡æ£€éªŒä¿¡æ¯",
            fields=[
                {"name": "checkNumber", "title": "æ£€éªŒç¼–å·", "type": "string", "x_component": "Input", "required": True},
                {"name": "batchId", "title": "å…³è”æ‰¹æ¬¡", "type": "string", "x_component": "Select", "required": True},
                {"name": "temperature", "title": "æ¸©åº¦æ£€æµ‹(Â°C)", "type": "number", "x_component": "NumberPicker"},
                {"name": "appearance", "title": "å¤–è§‚æ£€æŸ¥", "type": "string", "x_component": "Select", "enum": [{"label": "åˆæ ¼", "value": "pass"}, {"label": "ä¸åˆæ ¼", "value": "fail"}]},
                {"name": "result", "title": "æ£€éªŒç»“æœ", "type": "string", "x_component": "Select", "enum": [{"label": "åˆæ ¼", "value": "pass"}, {"label": "ä¸åˆæ ¼", "value": "fail"}], "required": True},
                {"name": "remarks", "title": "å¤‡æ³¨", "type": "string", "x_component": "Input.TextArea"},
            ]
        ),
        EntitySchemaDefinition(
            entity_type="SHIPMENT",
            entity_name="å‡ºè´§è®°å½•",
            description="è®°å½•äº§å“å‡ºè´§ä¿¡æ¯",
            fields=[
                {"name": "shipmentNumber", "title": "å‡ºè´§å•å·", "type": "string", "x_component": "Input", "required": True},
                {"name": "customerId", "title": "å®¢æˆ·", "type": "string", "x_component": "Select", "required": True},
                {"name": "productBatchId", "title": "äº§å“æ‰¹æ¬¡", "type": "string", "x_component": "Select", "required": True},
                {"name": "quantity", "title": "å‡ºè´§æ•°é‡", "type": "number", "x_component": "NumberPicker", "required": True},
                {"name": "shipmentDate", "title": "å‡ºè´§æ—¥æœŸ", "type": "string", "x_component": "DatePicker", "required": True},
            ]
        ),
    ]

    return FactoryBatchInitResponse(
        success=True,
        schemas=default_schemas,
        suggested_data=SuggestedBusinessData(
            product_types=[
                {"code": "PT001", "name": "é»˜è®¤äº§å“", "description": "é»˜è®¤äº§å“ç±»å‹"}
            ],
            material_types=[
                {"code": "MT001", "name": "é»˜è®¤åŸæ–™", "unit": "kg", "description": "é»˜è®¤åŸæ–™ç±»å‹"}
            ],
            conversion_rates=None
        ),
        industry_code="general",
        industry_name="é€šç”¨åŠ å·¥",
        ai_summary="ç”±äºAIæœåŠ¡ä¸å¯ç”¨ï¼Œå·²ç”Ÿæˆé»˜è®¤é€šç”¨é…ç½®æ¨¡æ¿ã€‚æ‚¨å¯ä»¥ç¨åæ‰‹åŠ¨è°ƒæ•´ã€‚",
        message="å·²ç”Ÿæˆé»˜è®¤é…ç½® (AIä¸å¯ç”¨)"
    )


# ==================== AI Schema ç”ŸæˆæœåŠ¡ ====================

class SchemaFieldDefinition(BaseModel):
    """ç”Ÿæˆçš„å•ä¸ªå­—æ®µå®šä¹‰"""
    name: str  # å­—æ®µè‹±æ–‡å (camelCase)
    title: str  # å­—æ®µä¸­æ–‡å
    type: str  # string, number, boolean, array
    description: Optional[str] = None  # å­—æ®µæè¿°
    x_component: str  # Formily ç»„ä»¶å
    x_component_props: Optional[Dict] = None  # ç»„ä»¶å±æ€§
    x_decorator: str = "FormItem"  # è£…é¥°å™¨
    x_decorator_props: Optional[Dict] = None  # è£…é¥°å™¨å±æ€§
    x_validator: Optional[List[Dict]] = None  # éªŒè¯è§„åˆ™
    x_reactions: Optional[Dict] = None  # è”åŠ¨è§„åˆ™
    enum: Optional[List[Dict]] = None  # æšä¸¾å€¼ (ä¸‹æ‹‰é€‰é¡¹)
    default: Optional[Any] = None  # é»˜è®¤å€¼

class SchemaGenerateRequest(BaseModel):
    """Schemaç”Ÿæˆè¯·æ±‚"""
    user_input: str  # ç”¨æˆ·è‡ªç„¶è¯­è¨€æè¿° (ä¾‹å¦‚: "åŠ ä¸€ä¸ªè¾£åº¦è¯„åˆ†å­—æ®µï¼Œ1-5åˆ†ï¼Œ3åˆ†ä»¥ä¸Šåˆæ ¼")
    entity_type: str  # è¡¨å•ç±»å‹: QUALITY_CHECK, MATERIAL_BATCH, etc.
    existing_fields: Optional[List[str]] = None  # ç°æœ‰å­—æ®µååˆ—è¡¨ï¼Œé¿å…é‡å¤
    factory_id: Optional[str] = None
    context: Optional[Dict] = None  # å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯

class SchemaGenerateResponse(BaseModel):
    """Schemaç”Ÿæˆå“åº”"""
    success: bool
    fields: List[SchemaFieldDefinition]  # ç”Ÿæˆçš„å­—æ®µåˆ—è¡¨
    validation_rules: Optional[List[Dict]] = None  # é¢å¤–çš„éªŒè¯è§„åˆ™
    suggestions: Optional[List[str]] = None  # AIå»ºè®® (å¦‚: "å»ºè®®æ·»åŠ ä¸åˆæ ¼åŸå› å­—æ®µ")
    message: Optional[str] = None


def build_form_parse_prompt(form_fields: List[Dict], entity_type: str) -> str:
    """
    æ„å»ºè¡¨å•è§£æçš„ç³»ç»Ÿæç¤ºè¯
    """
    field_descriptions = []
    for field in form_fields:
        name = field.get('name', '')
        title = field.get('title', name)
        field_type = field.get('type', 'string')
        required = field.get('required', False)
        enum_values = field.get('enum', [])

        desc = f"- {name} ({title}): ç±»å‹={field_type}"
        if required:
            desc += ", å¿…å¡«"
        if enum_values:
            enum_labels = [e.get('label', e) if isinstance(e, dict) else str(e) for e in enum_values]
            desc += f", å¯é€‰å€¼=[{', '.join(enum_labels)}]"
        field_descriptions.append(desc)

    fields_text = "\n".join(field_descriptions)

    entity_type_chinese = {
        'MATERIAL_BATCH': 'åŸææ–™æ‰¹æ¬¡',
        'QUALITY_CHECK': 'è´¨æ£€è®°å½•',
        'PROCESSING_BATCH': 'ç”Ÿäº§æ‰¹æ¬¡',
        'SHIPMENT': 'å‡ºè´§è®°å½•',
        'EQUIPMENT': 'è®¾å¤‡ä¿¡æ¯',
        'DISPOSAL_RECORD': 'å¤„ç½®è®°å½•'
    }.get(entity_type, entity_type)

    return f"""ä½ æ˜¯ç™½å©çºªé£Ÿå“æº¯æºç³»ç»Ÿçš„æ™ºèƒ½è¡¨å•åŠ©æ‰‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ä»ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è¾“å…¥ä¸­æå–è¡¨å•å­—æ®µå€¼ã€‚

å½“å‰æ­£åœ¨å¡«å†™: {entity_type_chinese}

è¡¨å•å­—æ®µå®šä¹‰:
{fields_text}

æå–è§„åˆ™:
1. ä»…æå–ç”¨æˆ·æ˜ç¡®æåˆ°çš„å­—æ®µå€¼
2. æ•°å€¼ç±»å‹éœ€è¦è½¬æ¢ä¸ºæ•°å­—
3. æ—¥æœŸæ—¶é—´ä½¿ç”¨ ISO 8601 æ ¼å¼ (YYYY-MM-DDTHH:mm:ss)
4. æšä¸¾ç±»å‹éœ€è¦åŒ¹é…å¯é€‰å€¼
5. å¦‚æœç”¨æˆ·æ²¡æœ‰æåˆ°æŸä¸ªå­—æ®µï¼Œä¸è¦çŒœæµ‹ï¼Œç›´æ¥ä¸å¡«

è¾“å‡ºæ ¼å¼ (ä¸¥æ ¼JSON):
{{
  "fieldName1": "value1",
  "fieldName2": 123,
  ...
}}

æ³¨æ„:
- åªè¾“å‡ºJSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—
- æ²¡æœ‰æåˆ°çš„å­—æ®µä¸è¦åŒ…å«
- æ¸©åº¦å•ä½é»˜è®¤æ‘„æ°åº¦ï¼Œé‡é‡å•ä½æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­ï¼ˆå…‹/å…¬æ–¤/å¨ï¼‰
- å¦‚æœç”¨æˆ·è¯´çš„æ˜¯ç®€ç§°æˆ–åˆ«åï¼Œéœ€è¦è¯†åˆ«å¹¶è½¬æ¢ä¸ºæ ‡å‡†å€¼"""


@app.post("/api/ai/form/parse", response_model=FormParseResponse)
async def parse_form_input(request: FormParseRequest):
    """
    AIè¡¨å•è§£æ - å°†ç”¨æˆ·è‡ªç„¶è¯­è¨€è¾“å…¥è§£æä¸ºè¡¨å•å­—æ®µå€¼

    ç”¨é€”:
    - è¯­éŸ³è½¬æ–‡å­—åçš„å†…å®¹è§£æ
    - ç”¨æˆ·æ–‡æœ¬è¾“å…¥çš„è§£æ

    ç¤ºä¾‹:
    è¾“å…¥: "å¸®æˆ‘å¡«ä¸€ä¸ªå¸¦é±¼æ‰¹æ¬¡ï¼Œ500å…¬æ–¤ï¼Œæ¸©åº¦é›¶ä¸‹18åº¦"
    è¾“å‡º: {"materialType": "å¸¦é±¼", "quantity": 500, "unit": "kg", "temperature": -18}
    """
    try:
        if not request.user_input or not request.user_input.strip():
            return FormParseResponse(
                success=False,
                field_values={},
                confidence=0,
                message="ç”¨æˆ·è¾“å…¥ä¸èƒ½ä¸ºç©º"
            )

        # æ„å»ºæç¤ºè¯
        system_prompt = build_form_parse_prompt(request.form_fields, request.entity_type)

        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_text = ""
        if request.context:
            context_items = []
            if request.context.get('factoryName'):
                context_items.append(f"å½“å‰å·¥å‚: {request.context['factoryName']}")
            if request.context.get('userName'):
                context_items.append(f"æ“ä½œäºº: {request.context['userName']}")
            if request.context.get('recentMaterials'):
                context_items.append(f"å¸¸ç”¨åŸæ–™: {', '.join(request.context['recentMaterials'][:5])}")
            if context_items:
                context_text = "\n\nèƒŒæ™¯ä¿¡æ¯:\n" + "\n".join(context_items)

        messages = [
            {"role": "system", "content": system_prompt + context_text},
            {"role": "user", "content": request.user_input}
        ]

        # è°ƒç”¨AI
        result = query_qwen(messages, enable_thinking=False)
        response_text = result["content"].strip()

        # è§£æJSONå“åº”
        try:
            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()

            field_values = json.loads(response_text)

            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºè§£æå‡ºçš„å­—æ®µæ•°é‡å’Œç”¨æˆ·è¾“å…¥é•¿åº¦çš„æ¯”ä¾‹ï¼‰
            parsed_count = len(field_values)
            total_fields = len(request.form_fields)
            input_length = len(request.user_input)

            # ç®€å•çš„ç½®ä¿¡åº¦è®¡ç®—
            if parsed_count == 0:
                confidence = 0.3
            elif parsed_count >= total_fields * 0.5:
                confidence = 0.9
            else:
                confidence = 0.6 + (parsed_count / max(total_fields, 1)) * 0.3

            # P1-1: æ£€æµ‹ç¼ºå¤±çš„å¿…å¡«å­—æ®µ
            missing_required_fields = []
            suggested_questions = []
            for field in request.form_fields:
                field_name = field.get('name', '')
                is_required = field.get('required', False)
                if is_required and field_name not in field_values:
                    missing_required_fields.append(field_name)
                    field_title = field.get('title', field_name)
                    field_type = field.get('type', 'string')
                    # æ ¹æ®å­—æ®µç±»å‹ç”Ÿæˆè¿½é—®
                    if field.get('enum'):
                        options = ', '.join(str(v) for v in field['enum'][:5])
                        suggested_questions.append(f"è¯·é€‰æ‹©{field_title}ï¼ˆå¯é€‰: {options}ï¼‰")
                    elif field_type == 'number':
                        suggested_questions.append(f"è¯·å‘Šè¯‰æˆ‘{field_title}çš„æ•°å€¼")
                    elif field_type == 'date':
                        suggested_questions.append(f"è¯·æä¾›{field_title}ï¼ˆæ—¥æœŸæ ¼å¼ï¼‰")
                    else:
                        suggested_questions.append(f"è¯·æä¾›{field_title}")

            # ç”Ÿæˆä¸»è¦è¿½é—®é—®é¢˜
            follow_up_question = None
            if missing_required_fields:
                if len(missing_required_fields) == 1:
                    follow_up_question = suggested_questions[0]
                else:
                    field_titles = []
                    for fn in missing_required_fields[:3]:  # æœ€å¤šå±•ç¤º3ä¸ª
                        for f in request.form_fields:
                            if f.get('name') == fn:
                                field_titles.append(f.get('title', fn))
                                break
                    follow_up_question = f"è¯·è¡¥å……ä»¥ä¸‹ä¿¡æ¯: {', '.join(field_titles)}"

            return FormParseResponse(
                success=True,
                field_values=field_values,
                confidence=confidence,
                message=f"æˆåŠŸè§£æ {parsed_count} ä¸ªå­—æ®µ",
                missing_required_fields=missing_required_fields if missing_required_fields else None,
                suggested_questions=suggested_questions if suggested_questions else None
            )

        except json.JSONDecodeError as e:
            return FormParseResponse(
                success=False,
                field_values={},
                confidence=0,
                unparsed_text=response_text,
                message=f"AIè¿”å›æ ¼å¼é”™è¯¯: {str(e)}"
            )

    except HTTPException as e:
        raise e
    except Exception as e:
        return FormParseResponse(
            success=False,
            field_values={},
            confidence=0,
            message=f"è§£æå¤±è´¥: {str(e)}"
        )


def build_validation_feedback_prompt(
    form_fields: List[Dict],
    entity_type: str,
    submitted_values: Dict,
    validation_errors: List[Dict]
) -> str:
    """æ„å»ºæ ¡éªŒåé¦ˆçš„AIæç¤ºè¯"""
    # æ ¼å¼åŒ–å­—æ®µä¿¡æ¯
    fields_info = []
    for field in form_fields:
        field_name = field.get('name', '')
        field_title = field.get('title', field_name)
        field_type = field.get('type', 'string')
        required = field.get('required', False)
        constraints = []
        if field.get('minimum') is not None:
            constraints.append(f"æœ€å°å€¼: {field['minimum']}")
        if field.get('maximum') is not None:
            constraints.append(f"æœ€å¤§å€¼: {field['maximum']}")
        if field.get('enum'):
            constraints.append(f"å¯é€‰å€¼: {', '.join(map(str, field['enum']))}")
        if field.get('pattern'):
            constraints.append(f"æ ¼å¼: {field['pattern']}")

        constraint_text = f" ({', '.join(constraints)})" if constraints else ""
        required_text = " [å¿…å¡«]" if required else ""
        fields_info.append(f"- {field_name} ({field_title}): {field_type}{constraint_text}{required_text}")

    fields_text = "\n".join(fields_info)

    # æ ¼å¼åŒ–ç”¨æˆ·æäº¤çš„å€¼
    submitted_text = json.dumps(submitted_values, ensure_ascii=False, indent=2)

    # æ ¼å¼åŒ–æ ¡éªŒé”™è¯¯
    errors_text = "\n".join([
        f"- å­—æ®µ '{e.get('field', 'æœªçŸ¥')}': {e.get('message', 'æ ¡éªŒå¤±è´¥')}"
        for e in validation_errors
    ])

    entity_type_chinese = {
        "MATERIAL_BATCH": "åŸææ–™æ‰¹æ¬¡",
        "QUALITY_CHECK": "è´¨æ£€è®°å½•",
        "PROCESSING_BATCH": "åŠ å·¥æ‰¹æ¬¡",
        "SHIPMENT": "å‡ºè´§è®°å½•",
    }.get(entity_type, entity_type)

    return f"""ä½ æ˜¯ç™½å©çºªé£Ÿå“æº¯æºç³»ç»Ÿçš„æ™ºèƒ½è¡¨å•æ ¡éªŒåŠ©æ‰‹ã€‚

ç”¨æˆ·æ­£åœ¨å¡«å†™: {entity_type_chinese}

è¡¨å•å­—æ®µå®šä¹‰:
{fields_text}

ç”¨æˆ·æäº¤çš„å€¼:
{submitted_text}

æ ¡éªŒå¤±è´¥çš„é”™è¯¯:
{errors_text}

ä½ çš„ä»»åŠ¡æ˜¯:
1. åˆ†ææ¯ä¸ªæ ¡éªŒé”™è¯¯çš„åŸå› 
2. ç»™å‡ºå…·ä½“çš„ä¿®æ­£å»ºè®®ï¼Œå¸®åŠ©ç”¨æˆ·æ­£ç¡®å¡«å†™
3. å¦‚æœå¯èƒ½ï¼Œæ¨æµ‹ç”¨æˆ·çš„æ„å›¾å¹¶ç»™å‡ºæ­£ç¡®çš„å€¼

è¾“å‡ºæ ¼å¼ (ä¸¥æ ¼JSON):
{{
  "correction_hints": {{
    "å­—æ®µå1": "ä¿®æ­£å»ºè®®1",
    "å­—æ®µå2": "ä¿®æ­£å»ºè®®2"
  }},
  "corrected_values": {{
    "å­—æ®µå1": ä¿®æ­£åçš„å€¼,
    "å­—æ®µå2": ä¿®æ­£åçš„å€¼
  }},
  "explanation": "æ•´ä½“è§£é‡Š"
}}

æ³¨æ„:
- correction_hints å¿…é¡»åŒ…å«æ‰€æœ‰å‡ºé”™å­—æ®µçš„å»ºè®®
- corrected_values åªåŒ…å«ä½ æœ‰ä¿¡å¿ƒä¿®æ­£çš„å€¼
- å¦‚æœæ— æ³•ç¡®å®šæ­£ç¡®å€¼ï¼Œä¸è¦çŒœæµ‹
- å»ºè®®è¦å…·ä½“ã€å‹å¥½ã€æ˜“äºç†è§£"""


@app.post("/api/ai/form/parse/feedback", response_model=ValidationFeedbackResponse)
async def parse_form_validation_feedback(request: ValidationFeedbackRequest):
    """
    æ ¡éªŒåé¦ˆç«¯ç‚¹ - è¡¨å•æ ¡éªŒå¤±è´¥æ—¶ï¼ŒAIç”Ÿæˆä¿®æ­£å»ºè®®

    ç”¨é€”:
    - è¡¨å•æäº¤æ ¡éªŒå¤±è´¥åï¼Œè°ƒç”¨æ­¤ç«¯ç‚¹è·å–AIä¿®æ­£å»ºè®®
    - AIåˆ†æé”™è¯¯åŸå› ï¼Œç»™å‡ºå…·ä½“çš„ä¿®æ­£æ–¹æ¡ˆ

    ç¤ºä¾‹:
    è¾“å…¥: {"quantity": -10} + é”™è¯¯ [{"field": "quantity", "message": "å¿…é¡»å¤§äº0"}]
    è¾“å‡º: {
        "correction_hints": {"quantity": "æ•°é‡å¿…é¡»æ˜¯æ­£æ•°ï¼Œè¯·è¾“å…¥æ­£ç¡®çš„æ•°é‡ï¼Œå¦‚ 500"},
        "corrected_values": {"quantity": 10},
        "explanation": "æ‚¨è¾“å…¥çš„æ•°é‡æ˜¯è´Ÿæ•°ï¼Œå·²è‡ªåŠ¨ä¿®æ­£ä¸ºæ­£æ•°"
    }
    """
    try:
        if not request.validation_errors:
            return ValidationFeedbackResponse(
                success=True,
                correction_hints={},
                explanation="æ²¡æœ‰æ ¡éªŒé”™è¯¯éœ€è¦å¤„ç†",
                confidence=1.0,
                session_id=request.session_id
            )

        # æ„å»ºæç¤ºè¯
        system_prompt = build_validation_feedback_prompt(
            request.form_fields,
            request.entity_type,
            request.submitted_values,
            request.validation_errors
        )

        # æ·»åŠ ç”¨æˆ·è¡¥å……è¯´æ˜
        user_message = "è¯·åˆ†æä»¥ä¸Šæ ¡éªŒé”™è¯¯å¹¶ç»™å‡ºä¿®æ­£å»ºè®®ã€‚"
        if request.user_instruction:
            user_message += f"\n\nç”¨æˆ·è¡¥å……è¯´æ˜: {request.user_instruction}"

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]

        # è°ƒç”¨AI
        result = query_qwen(messages, enable_thinking=True)
        response_text = result["content"].strip()

        # è§£æJSONå“åº”
        try:
            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()

            parsed_response = json.loads(response_text)

            correction_hints = parsed_response.get("correction_hints", {})
            corrected_values = parsed_response.get("corrected_values", {})
            explanation = parsed_response.get("explanation", "")

            # è®¡ç®—ç½®ä¿¡åº¦
            error_count = len(request.validation_errors)
            hint_count = len(correction_hints)
            corrected_count = len(corrected_values)

            if hint_count >= error_count:
                confidence = 0.8 + (corrected_count / max(error_count, 1)) * 0.2
            else:
                confidence = 0.5 + (hint_count / max(error_count, 1)) * 0.3

            return ValidationFeedbackResponse(
                success=True,
                correction_hints=correction_hints,
                corrected_values=corrected_values if corrected_values else None,
                explanation=explanation,
                confidence=min(confidence, 1.0),
                session_id=request.session_id or str(uuid.uuid4())
            )

        except json.JSONDecodeError as e:
            # AIè¿”å›æ ¼å¼é”™è¯¯ï¼Œå°è¯•ç”Ÿæˆé€šç”¨å»ºè®®
            generic_hints = {}
            for error in request.validation_errors:
                field = error.get("field", "unknown")
                message = error.get("message", "æ ¡éªŒå¤±è´¥")
                generic_hints[field] = f"è¯·æ£€æŸ¥æ­¤å­—æ®µ: {message}"

            return ValidationFeedbackResponse(
                success=True,
                correction_hints=generic_hints,
                explanation=f"AIæ ¼å¼è§£æå¤±è´¥ï¼Œå·²ç”Ÿæˆé€šç”¨å»ºè®®: {str(e)}",
                confidence=0.3,
                session_id=request.session_id
            )

    except HTTPException as e:
        raise e
    except Exception as e:
        return ValidationFeedbackResponse(
            success=False,
            correction_hints={},
            explanation=f"ç”Ÿæˆä¿®æ­£å»ºè®®å¤±è´¥: {str(e)}",
            confidence=0,
            session_id=request.session_id
        )


@app.post("/api/ai/form/ocr", response_model=OCRParseResponse)
async def parse_form_ocr(request: OCRParseRequest):
    """
    AIè¡¨å•OCRè§£æ - ä»å›¾ç‰‡ä¸­æå–è¡¨å•å­—æ®µå€¼

    ç”¨é€”:
    - æ‹ç…§è¯†åˆ«é€è´§å•ã€è´¨æ£€æŠ¥å‘Šç­‰
    - æ‰«ææ–‡æ¡£è‡ªåŠ¨å¡«å……è¡¨å•
    - ç”µå­ç§¤è®¾å¤‡é“­ç‰Œ/è§„æ ¼ä¹¦è¯†åˆ«

    æµç¨‹:
    1. æ£€æŸ¥ entity_typeï¼Œå¦‚æœæ˜¯ SCALE_CONFIGURATION åˆ™ä½¿ç”¨ä¸“ç”¨è§†è§‰è§£æå™¨
    2. å…¶ä»–ç±»å‹ä½¿ç”¨é˜¿é‡Œäº‘OCRè¯†åˆ«å›¾ç‰‡æ–‡å­—
    3. å°†è¯†åˆ«ç»“æœå‘é€ç»™LLMè¿›è¡Œç»“æ„åŒ–æå–
    4. è¿”å›è§£æå‡ºçš„å­—æ®µå€¼
    """
    try:
        if not request.image_base64:
            return OCRParseResponse(
                success=False,
                extracted_text="",
                field_values={},
                confidence=0,
                message="å›¾ç‰‡æ•°æ®ä¸èƒ½ä¸ºç©º"
            )

        # ==================== ç”µå­ç§¤è®¾å¤‡è¯†åˆ« (Qwen VL) ====================
        if request.entity_type == "SCALE_CONFIGURATION":
            if not is_vision_enabled():
                return OCRParseResponse(
                    success=False,
                    extracted_text="",
                    field_values={},
                    confidence=0,
                    message="è§†è§‰è¯†åˆ«åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·é…ç½® VISION_MODEL å’Œ VISION_ENABLED ç¯å¢ƒå˜é‡"
                )

            # ä½¿ç”¨ä¸“ç”¨è§†è§‰è§£æå™¨è¯†åˆ«è®¾å¤‡é“­ç‰Œ
            vision_result = parse_scale_image(request.image_base64, "é“­ç‰Œ")

            if vision_result.get("success"):
                # æ˜ å°„åˆ°è¡¨å•å­—æ®µ
                field_values = {
                    "equipmentName": f"{vision_result.get('brand', '')} {vision_result.get('model', '')}".strip(),
                    "brandModel": {
                        "brandName": vision_result.get('brand'),
                        "modelCode": vision_result.get('model'),
                    },
                    "serialNumber": vision_result.get('serial_number'),
                    "maxCapacity": vision_result.get('max_capacity'),
                    "precision": vision_result.get('precision'),
                    "connectionType": vision_result.get('connection_type'),
                    "notes": vision_result.get('notes'),
                }
                # æ¸…ç† None å€¼
                field_values = {k: v for k, v in field_values.items() if v is not None}

                return OCRParseResponse(
                    success=True,
                    extracted_text=vision_result.get('raw_text', ''),
                    field_values=field_values,
                    confidence=vision_result.get('confidence', 0.8),
                    message=vision_result.get('message', 'è®¾å¤‡è¯†åˆ«æˆåŠŸ')
                )
            else:
                return OCRParseResponse(
                    success=False,
                    extracted_text=vision_result.get('raw_text', ''),
                    field_values={},
                    confidence=0,
                    message=vision_result.get('message', 'è®¾å¤‡è¯†åˆ«å¤±è´¥')
                )

        # ==================== é€šç”¨è¡¨å•OCRè¯†åˆ« ====================
        # TODO: é›†æˆé˜¿é‡Œäº‘OCR API
        # å½“å‰ä½¿ç”¨æ¨¡æ‹ŸOCRç»“æœï¼ˆç”¨äºå¼€å‘æµ‹è¯•ï¼‰
        # å®é™…ç”Ÿäº§ç¯å¢ƒéœ€è¦æ›¿æ¢ä¸ºçœŸå®OCRè°ƒç”¨:
        # https://help.aliyun.com/document_detail/442323.html

        # æ¨¡æ‹ŸOCRç»“æœï¼ˆæ ¹æ®å›¾ç‰‡ç±»å‹è¿”å›ä¸åŒçš„æ¨¡æ‹Ÿæ–‡æœ¬ï¼‰
        mock_ocr_text = f"""é€è´§å•
æ—¥æœŸ: 2025-12-28
ä¾›åº”å•†: ä¸œæµ·æ¸”ä¸šæœ‰é™å…¬å¸
äº§å“: å¸¦é±¼ (ç²¾é€‰)
æ•°é‡: 500 kg
æ‰¹æ¬¡å·: MB-2025-12-28-001
æ¸©åº¦è®°å½•: -18Â°C
æ£€éªŒå‘˜: å¼ ä¸‰
å¤‡æ³¨: å†·é“¾è¿è¾“ï¼Œè´¨é‡åˆæ ¼"""

        extracted_text = mock_ocr_text

        # æ„å»ºæç¤ºè¯
        system_prompt = build_form_parse_prompt(request.form_fields, request.entity_type)

        messages = [
            {"role": "system", "content": system_prompt + "\n\nä»¥ä¸‹æ˜¯ä»å•æ®å›¾ç‰‡ä¸­OCRè¯†åˆ«çš„æ–‡å­—:"},
            {"role": "user", "content": extracted_text}
        ]

        # è°ƒç”¨AIè¿›è¡Œç»“æ„åŒ–æå–
        result = query_qwen(messages, enable_thinking=False)
        response_text = result["content"].strip()

        try:
            # æ¸…ç†markdownä»£ç å—
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()

            field_values = json.loads(response_text)

            return OCRParseResponse(
                success=True,
                extracted_text=extracted_text,
                field_values=field_values,
                confidence=0.85,  # OCRæœ‰é¢å¤–çš„ä¸ç¡®å®šæ€§
                message=f"æˆåŠŸä»å›¾ç‰‡è§£æ {len(field_values)} ä¸ªå­—æ®µ"
            )

        except json.JSONDecodeError as e:
            return OCRParseResponse(
                success=False,
                extracted_text=extracted_text,
                field_values={},
                confidence=0,
                message=f"ç»“æ„åŒ–æå–å¤±è´¥: {str(e)}"
            )

    except HTTPException as e:
        raise e
    except Exception as e:
        return OCRParseResponse(
            success=False,
            extracted_text="",
            field_values={},
            confidence=0,
            message=f"OCRè§£æå¤±è´¥: {str(e)}"
        )


@app.get("/api/ai/form/health")
async def form_assistant_health():
    """
    è¡¨å•åŠ©æ‰‹æœåŠ¡å¥åº·æ£€æŸ¥
    """
    return {
        "service": "form_assistant",
        "status": "running",
        "llm_available": bool(client),
        "vision_enabled": is_vision_enabled(),  # Qwen VL è§†è§‰è¯†åˆ«
        "ocr_enabled": False,  # é€šç”¨ OCR (å¾…é›†æˆé˜¿é‡Œäº‘ OCR)
        "schema_generation_enabled": True,  # AI Schema ç”ŸæˆåŠŸèƒ½
        "supported_entity_types": [
            "MATERIAL_BATCH",
            "QUALITY_CHECK",
            "PROCESSING_BATCH",
            "SHIPMENT",
            "EQUIPMENT",
            "DISPOSAL_RECORD",
            "SCALE_CONFIGURATION"  # ç”µå­ç§¤è®¾å¤‡é…ç½®
        ],
        "capabilities": [
            "form_parse",           # è¯­éŸ³/æ–‡æœ¬è§£æå¡«å……è¡¨å•
            "ocr_parse",            # OCRå›¾ç‰‡è§£æ (é€šç”¨è¡¨å•)
            "scale_vision_parse",   # ç”µå­ç§¤è®¾å¤‡é“­ç‰Œè¯†åˆ« (Qwen VL)
            "schema_generate"       # AIç”ŸæˆSchemaå­—æ®µ
        ]
    }


# ==================== AI Schema ç”Ÿæˆç«¯ç‚¹ ====================

def build_schema_generate_prompt(entity_type: str, existing_fields: List[str] = None) -> str:
    """
    æ„å»º Schema ç”Ÿæˆçš„ç³»ç»Ÿæç¤ºè¯
    """
    entity_type_chinese = {
        'MATERIAL_BATCH': 'åŸææ–™æ‰¹æ¬¡',
        'QUALITY_CHECK': 'è´¨æ£€è®°å½•',
        'PROCESSING_BATCH': 'ç”Ÿäº§æ‰¹æ¬¡',
        'SHIPMENT': 'å‡ºè´§è®°å½•',
        'EQUIPMENT': 'è®¾å¤‡ä¿¡æ¯',
        'DISPOSAL_RECORD': 'å¤„ç½®è®°å½•'
    }.get(entity_type, entity_type)

    existing_fields_text = ""
    if existing_fields:
        existing_fields_text = f"\n\nç°æœ‰å­—æ®µ (é¿å…é‡å¤): {', '.join(existing_fields)}"

    # å¯ç”¨çš„ Formily ç»„ä»¶æ˜ å°„
    component_guide = """
å¯ç”¨çš„ç»„ä»¶ç±»å‹:
- Input: å•è¡Œæ–‡æœ¬è¾“å…¥
- Input.TextArea: å¤šè¡Œæ–‡æœ¬è¾“å…¥
- NumberPicker: æ•°å­—è¾“å…¥ (æ”¯æŒ min, max, step)
- Select: ä¸‹æ‹‰é€‰æ‹© (éœ€è¦ enum)
- Radio.Group: å•é€‰æŒ‰é’®ç»„ (éœ€è¦ enum)
- Checkbox.Group: å¤šé€‰æ¡†ç»„ (éœ€è¦ enum)
- DatePicker: æ—¥æœŸé€‰æ‹©
- DatePicker.RangePicker: æ—¥æœŸèŒƒå›´é€‰æ‹©
- Switch: å¼€å…³ (å¸ƒå°”å€¼)
- Upload: æ–‡ä»¶/å›¾ç‰‡ä¸Šä¼ 
- Rate: è¯„åˆ† (1-5æ˜Ÿ)
"""

    return f"""ä½ æ˜¯ç™½å©çºªé£Ÿå“æº¯æºç³»ç»Ÿçš„è¡¨å•é…ç½®åŠ©æ‰‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œç”Ÿæˆ Formily JSON Schema æ ¼å¼çš„å­—æ®µå®šä¹‰ã€‚

å½“å‰æ­£åœ¨é…ç½®: {entity_type_chinese} è¡¨å•
{existing_fields_text}

{component_guide}

ç”Ÿæˆè§„åˆ™:
1. å­—æ®µå (name) ä½¿ç”¨ camelCase è‹±æ–‡å‘½åï¼Œç®€æ´æœ‰æ„ä¹‰
2. ä¸­æ–‡å (title) ç›´æ¥ä½¿ç”¨ç”¨æˆ·æè¿°çš„åç§°
3. æ ¹æ®ç”¨æˆ·æè¿°é€‰æ‹©åˆé€‚çš„ç»„ä»¶ç±»å‹
4. å¦‚æœç”¨æˆ·æåˆ°æ•°å€¼èŒƒå›´ï¼Œæ·»åŠ  x-validator éªŒè¯è§„åˆ™
5. å¦‚æœç”¨æˆ·æåˆ°æ¡ä»¶æ˜¾ç¤º/éšè—ï¼Œæ·»åŠ  x-reactions è”åŠ¨è§„åˆ™
6. å¦‚æœç”¨æˆ·æåˆ°"åˆæ ¼æ ‡å‡†"ï¼Œå¯ä»¥å»ºè®®æ·»åŠ å…³è”å­—æ®µ

è¾“å‡ºæ ¼å¼ (ä¸¥æ ¼JSON):
{{
  "fields": [
    {{
      "name": "å­—æ®µè‹±æ–‡å",
      "title": "å­—æ®µä¸­æ–‡å",
      "type": "string|number|boolean|array",
      "description": "å­—æ®µæè¿°(å¯é€‰)",
      "x_component": "ç»„ä»¶å",
      "x_component_props": {{}},
      "x_decorator": "FormItem",
      "x_decorator_props": {{"label": "å­—æ®µä¸­æ–‡å"}},
      "x_validator": [],
      "x_reactions": {{}},
      "enum": null,
      "default": null
    }}
  ],
  "validation_rules": [
    {{"field": "å­—æ®µå", "passCondition": "æ¡ä»¶æè¿°"}}
  ],
  "suggestions": ["å»ºè®®1", "å»ºè®®2"]
}}

æ³¨æ„:
- åªè¾“å‡ºJSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—
- å­—æ®µåä¸è¦ä¸ç°æœ‰å­—æ®µé‡å¤
- x_reactions ç”¨äºæ¡ä»¶æ˜¾ç¤ºé€»è¾‘ï¼Œæ ¼å¼ä¸º {{"when": "æ¡ä»¶", "fulfill": {{"state": {{"visible": true}}}}}}
- å¦‚æœç”¨æˆ·æè¿°å¤æ‚ï¼Œå¯ä»¥æ‹†åˆ†æˆå¤šä¸ªå­—æ®µ"""


@app.post("/api/ai/form/generate-schema", response_model=SchemaGenerateResponse)
async def generate_schema(request: SchemaGenerateRequest):
    """
    AIè¡¨å•Schemaç”Ÿæˆ - æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆ Formily JSON Schema å­—æ®µ

    ç”¨é€”:
    - åŠ¨æ€åˆ›å»ºæ–°çš„è¡¨å•å­—æ®µ
    - æ ¹æ®ä¸šåŠ¡éœ€æ±‚æ‰©å±•è¡¨å•ç»“æ„
    - å·¥å‚è‡ªå®šä¹‰é…ç½®

    ç¤ºä¾‹:
    è¾“å…¥: "åŠ ä¸€ä¸ªè¾£åº¦è¯„åˆ†å­—æ®µï¼Œ1-5åˆ†ï¼Œ3åˆ†ä»¥ä¸Šåˆæ ¼"
    è¾“å‡º: Formily æ ¼å¼çš„å­—æ®µå®šä¹‰ + éªŒè¯è§„åˆ™
    """
    try:
        if not request.user_input or not request.user_input.strip():
            return SchemaGenerateResponse(
                success=False,
                fields=[],
                message="ç”¨æˆ·è¾“å…¥ä¸èƒ½ä¸ºç©º"
            )

        # æ„å»ºæç¤ºè¯
        system_prompt = build_schema_generate_prompt(
            request.entity_type,
            request.existing_fields
        )

        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_text = ""
        if request.context:
            context_items = []
            if request.context.get('factoryName'):
                context_items.append(f"å·¥å‚: {request.context['factoryName']}")
            if request.context.get('industry'):
                context_items.append(f"è¡Œä¸š: {request.context['industry']}")
            if context_items:
                context_text = "\n\nèƒŒæ™¯ä¿¡æ¯:\n" + "\n".join(context_items)

        messages = [
            {"role": "system", "content": system_prompt + context_text},
            {"role": "user", "content": request.user_input}
        ]

        # è°ƒç”¨AI (ä¸å¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œæé«˜é€Ÿåº¦)
        result = query_qwen(messages, enable_thinking=False)
        response_text = result["content"].strip()

        # è§£æJSONå“åº”
        try:
            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()

            parsed = json.loads(response_text)

            # æå–å­—æ®µåˆ—è¡¨
            raw_fields = parsed.get("fields", [])
            fields = []

            for f in raw_fields:
                # å°† x_component ç­‰ä¸‹åˆ’çº¿å‘½åè½¬æ¢
                field = SchemaFieldDefinition(
                    name=f.get("name", ""),
                    title=f.get("title", ""),
                    type=f.get("type", "string"),
                    description=f.get("description"),
                    x_component=f.get("x_component", "Input"),
                    x_component_props=f.get("x_component_props"),
                    x_decorator=f.get("x_decorator", "FormItem"),
                    x_decorator_props=f.get("x_decorator_props"),
                    x_validator=f.get("x_validator"),
                    x_reactions=f.get("x_reactions"),
                    enum=f.get("enum"),
                    default=f.get("default")
                )
                fields.append(field)

            return SchemaGenerateResponse(
                success=True,
                fields=fields,
                validation_rules=parsed.get("validation_rules"),
                suggestions=parsed.get("suggestions"),
                message=f"æˆåŠŸç”Ÿæˆ {len(fields)} ä¸ªå­—æ®µå®šä¹‰"
            )

        except json.JSONDecodeError as e:
            return SchemaGenerateResponse(
                success=False,
                fields=[],
                message=f"AIè¿”å›æ ¼å¼é”™è¯¯: {str(e)}. åŸå§‹å“åº”: {response_text[:200]}"
            )

    except HTTPException as e:
        raise e
    except Exception as e:
        return SchemaGenerateResponse(
            success=False,
            fields=[],
            message=f"Schemaç”Ÿæˆå¤±è´¥: {str(e)}"
        )


# ==================== AI è§„åˆ™è§£ææœåŠ¡ ====================

class RuleParseRequest(BaseModel):
    """è§„åˆ™è§£æè¯·æ±‚"""
    user_input: str  # ç”¨æˆ·è‡ªç„¶è¯­è¨€æè¿° (ä¾‹å¦‚: "åº“å­˜ä½äº500kgæ—¶é€šçŸ¥é‡‡è´­")
    rule_group: Optional[str] = None  # è§„åˆ™ç»„ (validation, workflow, costing, quality)
    entity_type: Optional[str] = None  # å®ä½“ç±»å‹ (MaterialBatch, QualityCheck, etc.)
    factory_id: Optional[str] = None
    context: Optional[Dict] = None  # å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯


class RuleParseResponse(BaseModel):
    """è§„åˆ™è§£æå“åº”"""
    success: bool
    rule_name: Optional[str] = None  # ç”Ÿæˆçš„è§„åˆ™åç§°
    rule_description: Optional[str] = None  # è§„åˆ™æè¿°
    drl_content: Optional[str] = None  # ç”Ÿæˆçš„ DRL è§„åˆ™å†…å®¹
    rule_group: Optional[str] = None  # æ¨èçš„è§„åˆ™ç»„
    priority: Optional[int] = None  # æ¨èçš„ä¼˜å…ˆçº§
    entity_types: Optional[List[str]] = None  # æ¶‰åŠçš„å®ä½“ç±»å‹
    ai_explanation: Optional[str] = None  # AI è§£é‡Š
    suggestions: Optional[List[str]] = None  # å»ºè®®
    message: Optional[str] = None


def build_rule_parse_prompt() -> str:
    """
    æ„å»ºè§„åˆ™è§£æçš„ç³»ç»Ÿæç¤ºè¯
    """
    return """ä½ æ˜¯ç™½å©çºªé£Ÿå“æº¯æºç³»ç»Ÿçš„è§„åˆ™å¼•æ“é…ç½®åŠ©æ‰‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æè¿°è½¬æ¢ä¸º Drools DRL è§„åˆ™ã€‚

æ”¯æŒçš„å®ä½“ç±»å‹ (Fact Types):
1. MaterialBatch - åŸææ–™æ‰¹æ¬¡
   - å±æ€§: batchNumber, materialTypeId, quantity, currentQuantity, status, temperature, supplierId, expiryDate
2. ProcessingBatch - ç”Ÿäº§æ‰¹æ¬¡
   - å±æ€§: batchNumber, productTypeId, plannedQuantity, actualQuantity, status, yieldRate
3. QualityInspection - è´¨æ£€è®°å½•
   - å±æ€§: inspectionNumber, batchId, result, temperature, bacteriaCount, appearance
4. Equipment - è®¾å¤‡
   - å±æ€§: equipmentCode, equipmentName, status, lastMaintenanceDate, operatingHours
5. Shipment - å‡ºè´§è®°å½•
   - å±æ€§: shipmentNumber, customerId, quantity, status, shipmentDate

è§„åˆ™ç»„ç±»å‹:
- validation: æ•°æ®éªŒè¯è§„åˆ™
- workflow: å·¥ä½œæµè§„åˆ™ (çŠ¶æ€è½¬æ¢è§¦å‘)
- costing: æˆæœ¬è®¡ç®—è§„åˆ™
- quality: è´¨é‡æ§åˆ¶è§„åˆ™
- alert: å‘Šè­¦é€šçŸ¥è§„åˆ™

å¯ç”¨çš„å†…ç½®æœåŠ¡:
- alertService.send(level, title, message) - å‘é€å‘Šè­¦ (level: INFO/WARNING/CRITICAL)
- notifyService.notify(department, message) - é€šçŸ¥éƒ¨é—¨
- logService.log(entityId, action, details) - è®°å½•æ—¥å¿—

DRL è§„åˆ™æ ¼å¼ç¤ºä¾‹:
```
package com.cretas.aims.rules.{rule_group}

import com.cretas.aims.entity.*;
import com.cretas.aims.service.AlertService;
import com.cretas.aims.service.NotifyService;

global AlertService alertService;
global NotifyService notifyService;

rule "è§„åˆ™ä¸­æ–‡å"
    salience 10  // ä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå¤§è¶Šå…ˆæ‰§è¡Œ
    when
        $batch : MaterialBatch(currentQuantity < 500)
    then
        alertService.send("WARNING", "åº“å­˜é¢„è­¦",
            "åŸææ–™ " + $batch.getMaterialTypeId() + " åº“å­˜ä¸è¶³500kgï¼Œå½“å‰: " + $batch.getCurrentQuantity() + "kg");
        notifyService.notify("é‡‡è´­éƒ¨", "è¯·åŠæ—¶è¡¥å……åº“å­˜");
end
```

è¾“å‡ºæ ¼å¼ (ä¸¥æ ¼JSON):
{
  "rule_name": "ä½åº“å­˜é¢„è­¦",
  "rule_description": "å½“åŸææ–™åº“å­˜ä½äº500kgæ—¶å‘é€é¢„è­¦é€šçŸ¥",
  "drl_content": "å®Œæ•´çš„DRLè§„åˆ™å†…å®¹",
  "rule_group": "alert",
  "priority": 10,
  "entity_types": ["MaterialBatch"],
  "ai_explanation": "è¿™ä¸ªè§„åˆ™ä¼šç›‘æ§æ‰€æœ‰åŸææ–™æ‰¹æ¬¡çš„å½“å‰åº“å­˜é‡...",
  "suggestions": ["å»ºè®®åŒæ—¶æ·»åŠ ä¸´ç•Œå€¼å¯é…ç½®åŠŸèƒ½", "å¯ä»¥ä¸ºä¸åŒåŸæ–™è®¾ç½®ä¸åŒé˜ˆå€¼"]
}

æ³¨æ„:
- åªè¾“å‡ºJSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—
- DRLå†…å®¹ä¸­çš„å¼•å·éœ€è¦æ­£ç¡®è½¬ä¹‰
- è§„åˆ™åä½¿ç”¨ä¸­æ–‡
- ä¼˜å…ˆçº§(salience)èŒƒå›´: 0-100ï¼Œæ•°å­—è¶Šå¤§è¶Šå…ˆæ‰§è¡Œ
- æ ¹æ®è§„åˆ™è¯­ä¹‰æ¨æ–­åˆé€‚çš„rule_group"""


@app.post("/api/ai/rule/parse", response_model=RuleParseResponse)
async def parse_rule(request: RuleParseRequest):
    """
    AIè§„åˆ™è§£æ - å°†è‡ªç„¶è¯­è¨€æè¿°è½¬æ¢ä¸º Drools DRL è§„åˆ™

    ç”¨é€”:
    - å¿«é€Ÿåˆ›å»ºä¸šåŠ¡è§„åˆ™
    - éæŠ€æœ¯äººå‘˜é…ç½®è§„åˆ™
    - è§„åˆ™æ¨¡æ¿ç”Ÿæˆ

    ç¤ºä¾‹è¾“å…¥:
    "åº“å­˜ä½äº500kgæ—¶é€šçŸ¥é‡‡è´­"
    "è´¨æ£€æ¸©åº¦è¶…è¿‡-15Â°Cæ—¶æ ‡è®°ä¸åˆæ ¼"
    "è®¾å¤‡è¿è¡Œè¶…è¿‡1000å°æ—¶æ—¶æé†’ç»´æŠ¤"

    ç¤ºä¾‹è¾“å‡º:
    å®Œæ•´çš„ Drools DRL è§„åˆ™ä»£ç 
    """
    try:
        if not request.user_input or not request.user_input.strip():
            return RuleParseResponse(
                success=False,
                message="ç”¨æˆ·è¾“å…¥ä¸èƒ½ä¸ºç©º"
            )

        # æ„å»ºæç¤ºè¯
        system_prompt = build_rule_parse_prompt()

        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_text = ""
        if request.rule_group:
            context_text += f"\nç”¨æˆ·æŒ‡å®šè§„åˆ™ç»„: {request.rule_group}"
        if request.entity_type:
            context_text += f"\nç”¨æˆ·æŒ‡å®šå®ä½“ç±»å‹: {request.entity_type}"
        if request.context:
            if request.context.get('factoryName'):
                context_text += f"\nå·¥å‚: {request.context['factoryName']}"
            if request.context.get('industry'):
                context_text += f"\nè¡Œä¸š: {request.context['industry']}"

        messages = [
            {"role": "system", "content": system_prompt + context_text},
            {"role": "user", "content": request.user_input}
        ]

        # è°ƒç”¨AI
        result = query_qwen(messages, enable_thinking=False)
        response_text = result["content"].strip()

        # è§£æJSONå“åº”
        try:
            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()

            parsed = json.loads(response_text)

            return RuleParseResponse(
                success=True,
                rule_name=parsed.get("rule_name"),
                rule_description=parsed.get("rule_description"),
                drl_content=parsed.get("drl_content"),
                rule_group=parsed.get("rule_group", request.rule_group or "validation"),
                priority=parsed.get("priority", 10),
                entity_types=parsed.get("entity_types", []),
                ai_explanation=parsed.get("ai_explanation"),
                suggestions=parsed.get("suggestions"),
                message="è§„åˆ™è§£ææˆåŠŸ"
            )

        except json.JSONDecodeError as e:
            return RuleParseResponse(
                success=False,
                message=f"AIè¿”å›æ ¼å¼é”™è¯¯: {str(e)}. åŸå§‹å“åº”: {response_text[:300]}"
            )

    except HTTPException as e:
        raise e
    except Exception as e:
        return RuleParseResponse(
            success=False,
            message=f"è§„åˆ™è§£æå¤±è´¥: {str(e)}"
        )


class StateMachineParseRequest(BaseModel):
    """çŠ¶æ€æœºè§£æè¯·æ±‚"""
    user_input: str  # ç”¨æˆ·è‡ªç„¶è¯­è¨€æè¿° (ä¾‹å¦‚: "è´¨æ£€å•æœ‰å¾…æ£€ã€åˆæ ¼ã€ä¸åˆæ ¼ã€å¤æ£€å››ä¸ªçŠ¶æ€")
    entity_type: str  # å®ä½“ç±»å‹ (QualityInspection, ProcessingBatch, etc.)
    factory_id: Optional[str] = None
    context: Optional[Dict] = None


class StateDefinition(BaseModel):
    """çŠ¶æ€å®šä¹‰"""
    code: str
    name: str
    description: Optional[str] = None
    color: Optional[str] = None
    is_final: bool = False


class TransitionDefinition(BaseModel):
    """è½¬æ¢å®šä¹‰"""
    from_state: str
    to_state: str
    event: str
    guard: Optional[str] = None
    action: Optional[str] = None
    description: Optional[str] = None


class StateMachineParseResponse(BaseModel):
    """çŠ¶æ€æœºè§£æå“åº”"""
    success: bool
    machine_name: Optional[str] = None
    machine_description: Optional[str] = None
    initial_state: Optional[str] = None
    states: Optional[List[StateDefinition]] = None
    transitions: Optional[List[TransitionDefinition]] = None
    ai_explanation: Optional[str] = None
    suggestions: Optional[List[str]] = None
    message: Optional[str] = None


def build_state_machine_parse_prompt() -> str:
    """
    æ„å»ºçŠ¶æ€æœºè§£æçš„ç³»ç»Ÿæç¤ºè¯
    """
    return """ä½ æ˜¯ç™½å©çºªé£Ÿå“æº¯æºç³»ç»Ÿçš„çŠ¶æ€æœºé…ç½®åŠ©æ‰‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œç”Ÿæˆå®Œæ•´çš„çŠ¶æ€æœºé…ç½®ã€‚

çŠ¶æ€æœºç”¨äºç®¡ç†å®ä½“çš„ç”Ÿå‘½å‘¨æœŸï¼Œä¾‹å¦‚ï¼š
- è´¨æ£€å•: å¾…æ£€ â†’ æ£€éªŒä¸­ â†’ åˆæ ¼/ä¸åˆæ ¼ â†’ (ä¸åˆæ ¼å¯å¤æ£€) â†’ æœ€ç»ˆç»“æœ
- ç”Ÿäº§æ‰¹æ¬¡: è®¡åˆ’ä¸­ â†’ ç”Ÿäº§ä¸­ â†’ å®Œæˆ/æš‚åœ/å–æ¶ˆ
- å‡ºè´§å•: å¾…å‘è´§ â†’ å‘è´§ä¸­ â†’ å·²ç­¾æ”¶/å¼‚å¸¸

çŠ¶æ€è®¾è®¡è§„åˆ™:
1. æ¯ä¸ªçŠ¶æ€æœ‰å”¯ä¸€çš„ code (è‹±æ–‡å°å†™ä¸‹åˆ’çº¿) å’Œ name (ä¸­æ–‡å)
2. å¿…é¡»æœ‰ä¸€ä¸ªåˆå§‹çŠ¶æ€ (initial_state)
3. å¯ä»¥æœ‰å¤šä¸ªæœ€ç»ˆçŠ¶æ€ (is_final=true)
4. è½¬æ¢éœ€è¦å®šä¹‰è§¦å‘äº‹ä»¶ (event)

çŠ¶æ€é¢œè‰²å»ºè®®:
- å¾…å¤„ç†çŠ¶æ€: #F5A623 (æ©™è‰²)
- è¿›è¡Œä¸­çŠ¶æ€: #4A90E2 (è“è‰²)
- æˆåŠŸçŠ¶æ€: #7ED321 (ç»¿è‰²)
- å¤±è´¥çŠ¶æ€: #D0021B (çº¢è‰²)
- æš‚åœçŠ¶æ€: #9B9B9B (ç°è‰²)

è¾“å‡ºæ ¼å¼ (ä¸¥æ ¼JSON):
{
  "machine_name": "è´¨æ£€çŠ¶æ€æœº",
  "machine_description": "ç®¡ç†è´¨æ£€å•çš„çŠ¶æ€æµè½¬",
  "initial_state": "pending",
  "states": [
    {"code": "pending", "name": "å¾…æ£€", "color": "#F5A623", "is_final": false},
    {"code": "inspecting", "name": "æ£€éªŒä¸­", "color": "#4A90E2", "is_final": false},
    {"code": "passed", "name": "åˆæ ¼", "color": "#7ED321", "is_final": true},
    {"code": "failed", "name": "ä¸åˆæ ¼", "color": "#D0021B", "is_final": false},
    {"code": "reinspection", "name": "å¤æ£€ä¸­", "color": "#4A90E2", "is_final": false}
  ],
  "transitions": [
    {"from_state": "pending", "to_state": "inspecting", "event": "START_INSPECTION", "description": "å¼€å§‹æ£€éªŒ"},
    {"from_state": "inspecting", "to_state": "passed", "event": "MARK_PASSED", "guard": "result == 'pass'", "description": "æ ‡è®°åˆæ ¼"},
    {"from_state": "inspecting", "to_state": "failed", "event": "MARK_FAILED", "guard": "result == 'fail'", "description": "æ ‡è®°ä¸åˆæ ¼"},
    {"from_state": "failed", "to_state": "reinspection", "event": "REQUEST_REINSPECTION", "description": "ç”³è¯·å¤æ£€"}
  ],
  "ai_explanation": "æ ¹æ®æè¿°ç”Ÿæˆäº†5ä¸ªçŠ¶æ€å’Œ4ä¸ªè½¬æ¢...",
  "suggestions": ["å»ºè®®æ·»åŠ ä¸åˆæ ¼å¤„ç½®çŠ¶æ€", "å¯ä»¥æ·»åŠ å®¡æ‰¹æµç¨‹"]
}

æ³¨æ„:
- åªè¾“å‡ºJSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—
- çŠ¶æ€codeä½¿ç”¨è‹±æ–‡å°å†™ä¸‹åˆ’çº¿å‘½å
- äº‹ä»¶eventä½¿ç”¨è‹±æ–‡å¤§å†™ä¸‹åˆ’çº¿å‘½å
- guardæ˜¯å¯é€‰çš„å®ˆå«æ¡ä»¶è¡¨è¾¾å¼
- actionæ˜¯å¯é€‰çš„åŠ¨ä½œåç§°"""


@app.post("/api/ai/state-machine/parse", response_model=StateMachineParseResponse)
async def parse_state_machine(request: StateMachineParseRequest):
    """
    AIçŠ¶æ€æœºè§£æ - å°†è‡ªç„¶è¯­è¨€æè¿°è½¬æ¢ä¸ºçŠ¶æ€æœºé…ç½®

    ç”¨é€”:
    - å¿«é€Ÿåˆ›å»ºå®ä½“çŠ¶æ€æœº
    - å¯è§†åŒ–çŠ¶æ€æµç¨‹è®¾è®¡
    - ä¸šåŠ¡æµç¨‹é…ç½®

    ç¤ºä¾‹è¾“å…¥:
    "è´¨æ£€å•æœ‰å¾…æ£€ã€åˆæ ¼ã€ä¸åˆæ ¼ä¸‰ä¸ªçŠ¶æ€ï¼Œä¸åˆæ ¼å¯ä»¥ç”³è¯·å¤æ£€"

    ç¤ºä¾‹è¾“å‡º:
    å®Œæ•´çš„çŠ¶æ€æœºé…ç½® (çŠ¶æ€åˆ—è¡¨ + è½¬æ¢è§„åˆ™)
    """
    try:
        if not request.user_input or not request.user_input.strip():
            return StateMachineParseResponse(
                success=False,
                message="ç”¨æˆ·è¾“å…¥ä¸èƒ½ä¸ºç©º"
            )

        # æ„å»ºæç¤ºè¯
        system_prompt = build_state_machine_parse_prompt()

        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_text = f"\næ­£åœ¨é…ç½®çš„å®ä½“ç±»å‹: {request.entity_type}"
        if request.context:
            if request.context.get('factoryName'):
                context_text += f"\nå·¥å‚: {request.context['factoryName']}"
            if request.context.get('industry'):
                context_text += f"\nè¡Œä¸š: {request.context['industry']}"

        messages = [
            {"role": "system", "content": system_prompt + context_text},
            {"role": "user", "content": request.user_input}
        ]

        # è°ƒç”¨AI
        result = query_qwen(messages, enable_thinking=False)
        response_text = result["content"].strip()

        # è§£æJSONå“åº”
        try:
            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()

            parsed = json.loads(response_text)

            # è§£æçŠ¶æ€åˆ—è¡¨
            states = []
            for s in parsed.get("states", []):
                states.append(StateDefinition(
                    code=s.get("code", ""),
                    name=s.get("name", ""),
                    description=s.get("description"),
                    color=s.get("color"),
                    is_final=s.get("is_final", False)
                ))

            # è§£æè½¬æ¢åˆ—è¡¨
            transitions = []
            for t in parsed.get("transitions", []):
                transitions.append(TransitionDefinition(
                    from_state=t.get("from_state", ""),
                    to_state=t.get("to_state", ""),
                    event=t.get("event", ""),
                    guard=t.get("guard"),
                    action=t.get("action"),
                    description=t.get("description")
                ))

            return StateMachineParseResponse(
                success=True,
                machine_name=parsed.get("machine_name"),
                machine_description=parsed.get("machine_description"),
                initial_state=parsed.get("initial_state"),
                states=states,
                transitions=transitions,
                ai_explanation=parsed.get("ai_explanation"),
                suggestions=parsed.get("suggestions"),
                message=f"æˆåŠŸç”Ÿæˆ {len(states)} ä¸ªçŠ¶æ€å’Œ {len(transitions)} ä¸ªè½¬æ¢"
            )

        except json.JSONDecodeError as e:
            return StateMachineParseResponse(
                success=False,
                message=f"AIè¿”å›æ ¼å¼é”™è¯¯: {str(e)}"
            )

    except HTTPException as e:
        raise e
    except Exception as e:
        return StateMachineParseResponse(
            success=False,
            message=f"çŠ¶æ€æœºè§£æå¤±è´¥: {str(e)}"
        )


@app.get("/api/ai/rule/health")
async def rule_service_health():
    """
    è§„åˆ™è§£ææœåŠ¡å¥åº·æ£€æŸ¥
    """
    return {
        "service": "rule_parser",
        "status": "running",
        "llm_available": bool(client),
        "capabilities": [
            "drl_generation",       # DRL è§„åˆ™ç”Ÿæˆ
            "state_machine_design", # çŠ¶æ€æœºè®¾è®¡
            "rule_validation"       # è§„åˆ™éªŒè¯ (TODO)
        ],
        "supported_rule_groups": [
            "validation",
            "workflow",
            "costing",
            "quality",
            "alert"
        ],
        "supported_entity_types": [
            "MaterialBatch",
            "ProcessingBatch",
            "QualityInspection",
            "Equipment",
            "Shipment"
        ]
    }


# ==================== è°ƒåº¦æœåŠ¡ç«¯ç‚¹ ====================

# å¯¼å…¥è°ƒåº¦æœåŠ¡æ¨¡å—
try:
    from scheduling_service import (
        CompletionProbabilityRequest,
        CompletionProbabilityResponse,
        OptimizeWorkersRequest,
        OptimizeWorkersResponse,
        GenerateScheduleRequest,
        RescheduleRequest,
        calculate_completion_probability,
        optimize_workers,
        generate_schedule,
        reschedule,
        insight_generator
    )
    SCHEDULING_SERVICE_AVAILABLE = True
except ImportError as e:
    print(f"[WARN] Scheduling service not available: {e}")
    SCHEDULING_SERVICE_AVAILABLE = False


@app.post("/scheduling/completion-probability")
async def scheduling_completion_probability(request: dict):
    """
    Monte Carlo æ¨¡æ‹Ÿ - è®¡ç®—ç”Ÿäº§å®Œæˆæ¦‚ç‡

    è¾“å…¥:
    - factory_id: å·¥å‚ID
    - schedule_id: æ’ç¨‹ID
    - remaining_quantity: å‰©ä½™æ•°é‡
    - deadline: æˆªæ­¢æ—¶é—´ (ISOæ ¼å¼)
    - assigned_workers: åˆ†é…å·¥äººæ•°
    - efficiency_mean: æ•ˆç‡å‡å€¼ (å¯é€‰)
    - efficiency_std: æ•ˆç‡æ ‡å‡†å·® (å¯é€‰)

    è¾“å‡º:
    - probability: æŒ‰æ—¶å®Œæˆæ¦‚ç‡
    - mean_hours: é¢„è®¡å®Œæˆæ—¶é—´å‡å€¼
    - confidence_lower/upper: ç½®ä¿¡åŒºé—´
    - insight: AI æ´å¯Ÿæ–‡æœ¬
    """
    if not SCHEDULING_SERVICE_AVAILABLE:
        raise HTTPException(status_code=500, detail="è°ƒåº¦æœåŠ¡ä¸å¯ç”¨")

    try:
        from scheduling_service import CompletionProbabilityRequest as CPRequest
        req = CPRequest(**request)
        result = calculate_completion_probability(req)
        return result.dict()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è®¡ç®—å®Œæˆæ¦‚ç‡å¤±è´¥: {str(e)}")


@app.post("/scheduling/optimize-workers")
async def scheduling_optimize_workers(request: dict):
    """
    OR-Tools ä¼˜åŒ– - å·¥äººåˆ†é…ä¼˜åŒ–

    è¾“å…¥:
    - factory_id: å·¥å‚ID
    - plan_id: è®¡åˆ’ID
    - workers: å·¥äººåˆ—è¡¨ [{'id', 'skill', 'cost_per_hour', 'is_temporary'}]
    - schedules: æ’ç¨‹åˆ—è¡¨ [{'id', 'required_skill', 'min_workers', 'max_workers'}]
    - objective: ä¼˜åŒ–ç›®æ ‡ (minimize_cost/maximize_efficiency/balanced)
    - max_temporary_ratio: æœ€å¤§ä¸´æ—¶å·¥æ¯”ä¾‹

    è¾“å‡º:
    - assignments: åˆ†é…ç»“æœ [{'worker_id', 'schedule_id', 'assignment_type'}]
    - total_cost: æ€»æˆæœ¬
    - efficiency_score: æ•ˆç‡è¯„åˆ†
    """
    if not SCHEDULING_SERVICE_AVAILABLE:
        raise HTTPException(status_code=500, detail="è°ƒåº¦æœåŠ¡ä¸å¯ç”¨")

    try:
        from scheduling_service import OptimizeWorkersRequest as OWRequest
        req = OWRequest(**request)
        result = optimize_workers(req)
        return result.dict()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"äººå‘˜ä¼˜åŒ–å¤±è´¥: {str(e)}")


@app.post("/scheduling/generate")
async def scheduling_generate(request: dict):
    """
    AI ç”Ÿæˆè°ƒåº¦å»ºè®®

    è¾“å…¥:
    - factory_id: å·¥å‚ID
    - plan_date: è®¡åˆ’æ—¥æœŸ (YYYY-MM-DD)
    - batch_ids: æ‰¹æ¬¡IDåˆ—è¡¨
    - production_line_ids: äº§çº¿IDåˆ—è¡¨ (å¯é€‰)
    - available_worker_ids: å¯ç”¨å·¥äººIDåˆ—è¡¨ (å¯é€‰)
    - target_completion_probability: ç›®æ ‡å®Œæˆæ¦‚ç‡

    è¾“å‡º:
    - schedules: ç”Ÿæˆçš„æ’ç¨‹åˆ—è¡¨
    - confidence: AI ç½®ä¿¡åº¦
    """
    if not SCHEDULING_SERVICE_AVAILABLE:
        raise HTTPException(status_code=500, detail="è°ƒåº¦æœåŠ¡ä¸å¯ç”¨")

    try:
        from scheduling_service import GenerateScheduleRequest as GSRequest
        req = GSRequest(**request)
        result = generate_schedule(req)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆè°ƒåº¦å¤±è´¥: {str(e)}")


@app.post("/scheduling/reschedule")
async def scheduling_reschedule(request: dict):
    """
    é‡æ–°è°ƒåº¦

    è¾“å…¥:
    - factory_id: å·¥å‚ID
    - plan_id: è®¡åˆ’ID
    - reason: é‡æ–°è°ƒåº¦åŸå› 
    - keep_completed: æ˜¯å¦ä¿ç•™å·²å®Œæˆçš„æ’ç¨‹
    - schedule_ids_to_reschedule: éœ€è¦é‡æ–°è°ƒåº¦çš„æ’ç¨‹ID
    - unavailable_worker_ids: ä¸å¯ç”¨å·¥äººID

    è¾“å‡º:
    - updated_schedules: æ›´æ–°åçš„æ’ç¨‹åˆ—è¡¨
    """
    if not SCHEDULING_SERVICE_AVAILABLE:
        raise HTTPException(status_code=500, detail="è°ƒåº¦æœåŠ¡ä¸å¯ç”¨")

    try:
        from scheduling_service import RescheduleRequest as RSRequest
        req = RSRequest(**request)
        result = reschedule(req)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é‡æ–°è°ƒåº¦å¤±è´¥: {str(e)}")


@app.post("/scheduling/explain-alert")
async def scheduling_explain_alert(request: dict):
    """
    LLM è§£é‡Šå‘Šè­¦åŸå› 

    è¾“å…¥:
    - alert_type: å‘Šè­¦ç±»å‹
    - schedule_data: æ’ç¨‹æ•°æ®
    - probability: å®Œæˆæ¦‚ç‡

    è¾“å‡º:
    - explanation: å‘Šè­¦è§£é‡Šæ–‡æœ¬
    - recommendations: å»ºè®®æªæ–½
    """
    if not SCHEDULING_SERVICE_AVAILABLE:
        raise HTTPException(status_code=500, detail="è°ƒåº¦æœåŠ¡ä¸å¯ç”¨")

    try:
        alert_type = request.get('alert_type', 'low_probability')
        schedule_data = request.get('schedule_data', {})
        probability = request.get('probability', 0.5)

        explanation = insight_generator.explain_alert(alert_type, schedule_data, probability)

        return {
            'success': True,
            'explanation': explanation,
            'alert_type': alert_type,
            'probability': probability
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è§£é‡Šå‘Šè­¦å¤±è´¥: {str(e)}")


@app.get("/scheduling/health")
async def scheduling_health():
    """
    è°ƒåº¦æœåŠ¡å¥åº·æ£€æŸ¥
    """
    return {
        'service': 'scheduling',
        'status': 'running' if SCHEDULING_SERVICE_AVAILABLE else 'unavailable',
        'monte_carlo': True,
        'ortools': SCHEDULING_SERVICE_AVAILABLE,
        'llm_available': bool(client)
    }


# ==================== MLè®­ç»ƒå’Œæ··åˆé¢„æµ‹ ====================

# å¯¼å…¥MLæ¨¡å—
ML_SERVICE_AVAILABLE = False
try:
    from ml_trainer import train_models, model_loader
    from hybrid_predictor import (
        hybrid_predictor, predict_with_hybrid,
        predict_completion, get_model_status
    )
    ML_SERVICE_AVAILABLE = True
except ImportError as e:
    print(f"[WARN] MLæœåŠ¡æœªåŠ è½½: {e}")


@app.post("/ml/train")
async def ml_train_models(request: dict):
    """
    è§¦å‘æ¨¡å‹è®­ç»ƒ

    è¾“å…¥:
    - factory_id: å·¥å‚ID
    - model_types: æ¨¡å‹ç±»å‹åˆ—è¡¨ ["efficiency", "duration", "quality"]

    è¾“å‡º:
    - success: æ˜¯å¦æˆåŠŸ
    - results: å„æ¨¡å‹è®­ç»ƒç»“æœ
    """
    if not ML_SERVICE_AVAILABLE:
        raise HTTPException(status_code=500, detail="MLæœåŠ¡ä¸å¯ç”¨")

    try:
        factory_id = request.get('factory_id')
        model_types = request.get('model_types', ['efficiency', 'duration', 'quality'])

        if not factory_id:
            raise HTTPException(status_code=400, detail="factory_id ä¸èƒ½ä¸ºç©º")

        result = train_models(factory_id, model_types)
        return result

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è®­ç»ƒå¤±è´¥: {str(e)}")


@app.post("/ml/predict")
async def ml_predict(request: dict):
    """
    ä½¿ç”¨MLæ¨¡å‹è¿›è¡Œé¢„æµ‹

    è¾“å…¥:
    - factory_id: å·¥å‚ID
    - prediction_type: é¢„æµ‹ç±»å‹ (efficiency/duration/quality)
    - features: ç‰¹å¾æ•°æ®

    è¾“å‡º:
    - prediction: é¢„æµ‹å€¼
    - confidence: ç½®ä¿¡åº¦
    - model_version: æ¨¡å‹ç‰ˆæœ¬
    """
    if not ML_SERVICE_AVAILABLE:
        raise HTTPException(status_code=500, detail="MLæœåŠ¡ä¸å¯ç”¨")

    try:
        factory_id = request.get('factory_id')
        prediction_type = request.get('prediction_type', 'efficiency')
        features = request.get('features', {})

        result = predict_with_hybrid(factory_id, features, prediction_type)
        return {
            'success': True,
            **result
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é¢„æµ‹å¤±è´¥: {str(e)}")


@app.post("/scheduling/hybrid-predict")
async def scheduling_hybrid_predict(request: dict):
    """
    æ··åˆé¢„æµ‹å®Œæˆæ¦‚ç‡ (ML + Monte Carlo + LLM)

    è¾“å…¥:
    - factory_id: å·¥å‚ID
    - remaining_quantity: å‰©ä½™æ•°é‡
    - deadline_hours: æˆªæ­¢æ—¶é—´(å°æ—¶)
    - available_workers: å¯ç”¨å·¥äººæ•°
    - å…¶ä»–ç‰¹å¾...

    è¾“å‡º:
    - probability: å®Œæˆæ¦‚ç‡
    - mean_hours: é¢„è®¡å¹³å‡æ—¶é•¿
    - mode: é¢„æµ‹æ¨¡å¼ (hybrid/llm_only)
    - explanation: è§£é‡Š
    """
    if not ML_SERVICE_AVAILABLE:
        # å›é€€åˆ°åŸºç¡€Monte Carlo
        raise HTTPException(status_code=500, detail="MLæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·ä½¿ç”¨ /scheduling/completion-probability")

    try:
        factory_id = request.get('factory_id')
        if not factory_id:
            raise HTTPException(status_code=400, detail="factory_id ä¸èƒ½ä¸ºç©º")

        result = predict_completion(factory_id, request)
        return {
            'success': True,
            **result
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ··åˆé¢„æµ‹å¤±è´¥: {str(e)}")


@app.get("/ml/status/{factory_id}")
async def ml_model_status(factory_id: str):
    """
    è·å–å·¥å‚çš„MLæ¨¡å‹çŠ¶æ€

    è¾“å‡º:
    - models: å„ç±»å‹æ¨¡å‹çš„å¯ç”¨çŠ¶æ€
    """
    if not ML_SERVICE_AVAILABLE:
        return {
            'factory_id': factory_id,
            'ml_service_available': False,
            'models': {}
        }

    try:
        status = get_model_status(factory_id)
        status['ml_service_available'] = True
        return status

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}")


@app.get("/ml/health")
async def ml_health():
    """
    MLæœåŠ¡å¥åº·æ£€æŸ¥
    """
    return {
        'service': 'ml',
        'status': 'running' if ML_SERVICE_AVAILABLE else 'unavailable',
        'lightgbm_available': ML_SERVICE_AVAILABLE,
        'hybrid_predictor_available': ML_SERVICE_AVAILABLE
    }


# ==================== æ„å›¾è¯†åˆ« Fallback æœåŠ¡ ====================

class IntentClassifyRequest(BaseModel):
    """æ„å›¾åˆ†ç±»è¯·æ±‚"""
    user_input: str                          # ç”¨æˆ·åŸå§‹è¾“å…¥
    factory_id: str                          # å·¥å‚ID
    available_intents: List[Dict[str, Any]]  # å¯ç”¨æ„å›¾åˆ—è¡¨ [{code, name, description, category}]
    context: Optional[Dict] = None           # ä¸Šä¸‹æ–‡ä¿¡æ¯
    user_id: Optional[int] = None            # ç”¨æˆ·ID
    session_id: Optional[str] = None         # ä¼šè¯ID

class IntentCandidate(BaseModel):
    """å€™é€‰æ„å›¾"""
    intent_code: str
    intent_name: str
    confidence: float  # 0.0-1.0
    reasoning: Optional[str] = None

class IntentClassifyResponse(BaseModel):
    """æ„å›¾åˆ†ç±»å“åº”"""
    success: bool
    matched_intent_code: Optional[str] = None
    matched_intent_name: Optional[str] = None
    confidence: float
    candidates: List[IntentCandidate]
    is_ambiguous: bool               # æ˜¯å¦æœ‰æ­§ä¹‰
    needs_clarification: bool        # æ˜¯å¦éœ€è¦æ¾„æ¸…
    clarification_question: Optional[str] = None
    reasoning: Optional[str] = None  # AIæ¨ç†è¿‡ç¨‹
    message: Optional[str] = None

class IntentClarifyRequest(BaseModel):
    """æ¾„æ¸…é—®é¢˜ç”Ÿæˆè¯·æ±‚"""
    user_input: str
    candidates: List[Dict[str, Any]]  # å€™é€‰æ„å›¾åˆ—è¡¨
    factory_id: str
    context: Optional[Dict] = None

class IntentClarifyResponse(BaseModel):
    """æ¾„æ¸…é—®é¢˜å“åº”"""
    success: bool
    clarification_question: str      # ç”Ÿæˆçš„æ¾„æ¸…é—®é¢˜
    options: List[Dict[str, str]]    # é€‰é¡¹åˆ—è¡¨ [{value, label}]
    message: Optional[str] = None


def build_intent_classify_prompt(user_input: str, available_intents: List[Dict]) -> str:
    """æ„å»ºæ„å›¾åˆ†ç±»æç¤ºè¯"""
    intent_list = "\n".join([
        f"- {intent.get('intent_code', intent.get('code', ''))}: {intent.get('intent_name', intent.get('name', ''))} ({intent.get('description', '')})"
        for intent in available_intents
    ])

    return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ„å›¾è¯†åˆ«åŠ©æ‰‹ï¼Œè´Ÿè´£åˆ†æç”¨æˆ·è¾“å…¥å¹¶åŒ¹é…æœ€åˆé€‚çš„æ„å›¾ã€‚

## å¯ç”¨æ„å›¾åˆ—è¡¨:
{intent_list}

## ç”¨æˆ·è¾“å…¥:
"{user_input}"

## ä»»åŠ¡è¦æ±‚:
1. åˆ†æç”¨æˆ·è¾“å…¥çš„å«ä¹‰
2. ä»å¯ç”¨æ„å›¾åˆ—è¡¨ä¸­é€‰æ‹©æœ€åŒ¹é…çš„æ„å›¾
3. ç»™å‡ºç½®ä¿¡åº¦è¯„åˆ† (0.0-1.0)
4. å¦‚æœæœ‰å¤šä¸ªå¯èƒ½çš„æ„å›¾ï¼Œåˆ—å‡ºTop-3å€™é€‰
5. å¦‚æœç½®ä¿¡åº¦ä½äº0.7ï¼Œæ ‡è®°ä¸ºéœ€è¦æ¾„æ¸…

## è¾“å‡ºæ ¼å¼ (JSON):
{{
    "matched_intent_code": "æ„å›¾ä»£ç æˆ–null",
    "matched_intent_name": "æ„å›¾åç§°",
    "confidence": 0.85,
    "candidates": [
        {{"intent_code": "ä»£ç ", "intent_name": "åç§°", "confidence": 0.85}},
        {{"intent_code": "ä»£ç 2", "intent_name": "åç§°2", "confidence": 0.65}}
    ],
    "is_ambiguous": false,
    "needs_clarification": false,
    "reasoning": "åˆ†æè¿‡ç¨‹è¯´æ˜"
}}

è¯·åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"""


def build_clarify_question_prompt(user_input: str, candidates: List[Dict]) -> str:
    """æ„å»ºæ¾„æ¸…é—®é¢˜ç”Ÿæˆæç¤ºè¯"""
    candidate_list = "\n".join([
        f"- {c.get('intent_code', c.get('code', ''))}: {c.get('intent_name', c.get('name', ''))}"
        for c in candidates
    ])

    return f"""ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„å¯¹è¯åŠ©æ‰‹ï¼Œéœ€è¦ç”Ÿæˆä¸€ä¸ªæ¾„æ¸…é—®é¢˜æ¥å¸®åŠ©ç”¨æˆ·æ˜ç¡®ä»–ä»¬çš„æ„å›¾ã€‚

## ç”¨æˆ·åŸå§‹è¾“å…¥:
"{user_input}"

## å¯èƒ½çš„æ„å›¾:
{candidate_list}

## ä»»åŠ¡è¦æ±‚:
1. ç”Ÿæˆä¸€ä¸ªç®€çŸ­ã€å‹å¥½çš„æ¾„æ¸…é—®é¢˜
2. é—®é¢˜åº”è¯¥å¸®åŠ©åŒºåˆ†è¿™äº›å€™é€‰æ„å›¾
3. æä¾›æ¸…æ™°çš„é€‰é¡¹è®©ç”¨æˆ·é€‰æ‹©
4. ä½¿ç”¨è‡ªç„¶çš„ä¸­æ–‡è¡¨è¾¾

## è¾“å‡ºæ ¼å¼ (JSON):
{{
    "clarification_question": "è¯·é—®æ‚¨æ˜¯æƒ³è¦...è¿˜æ˜¯...?",
    "options": [
        {{"value": "intent_code1", "label": "é€‰é¡¹1æè¿°"}},
        {{"value": "intent_code2", "label": "é€‰é¡¹2æè¿°"}}
    ]
}}

è¯·åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"""


@app.post("/api/ai/intent/classify", response_model=IntentClassifyResponse)
async def classify_intent(request: IntentClassifyRequest):
    """
    æ„å›¾åˆ†ç±» - LLM Fallback

    å½“è§„åˆ™åŒ¹é…å¤±è´¥æˆ–ç½®ä¿¡åº¦ä½æ—¶ï¼Œä½¿ç”¨LLMè¿›è¡Œæ„å›¾åˆ†ç±»

    è¾“å…¥:
    - user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
    - available_intents: å¯ç”¨æ„å›¾åˆ—è¡¨

    è¾“å‡º:
    - matched_intent_code: åŒ¹é…çš„æ„å›¾ä»£ç 
    - confidence: ç½®ä¿¡åº¦ (0-1)
    - candidates: å€™é€‰æ„å›¾åˆ—è¡¨
    - needs_clarification: æ˜¯å¦éœ€è¦æ¾„æ¸…
    """
    if not client:
        raise HTTPException(status_code=500, detail="DASHSCOPE_API_KEYæœªé…ç½®")

    try:
        # æ„å»ºæç¤ºè¯
        prompt = build_intent_classify_prompt(
            request.user_input,
            request.available_intents
        )

        messages = [
            {"role": "system", "content": "ä½ æ˜¯é£Ÿå“åŠ å·¥æº¯æºç³»ç»Ÿçš„æ™ºèƒ½æ„å›¾è¯†åˆ«åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ]

        # è°ƒç”¨Qwen
        result = query_qwen(messages, enable_thinking=False)
        content = result.get("content", "")

        # è§£æJSONå“åº”
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_start = content.find("{")
            json_end = content.rfind("}") + 1
            if json_start >= 0 and json_end > json_start:
                json_str = content[json_start:json_end]
                parsed = json.loads(json_str)
            else:
                raise ValueError("No JSON found in response")

            # æ„å»ºå€™é€‰åˆ—è¡¨
            candidates = []
            for c in parsed.get("candidates", []):
                candidates.append(IntentCandidate(
                    intent_code=c.get("intent_code", ""),
                    intent_name=c.get("intent_name", ""),
                    confidence=float(c.get("confidence", 0)),
                    reasoning=c.get("reasoning")
                ))

            confidence = float(parsed.get("confidence", 0))
            is_ambiguous = parsed.get("is_ambiguous", False)
            needs_clarification = parsed.get("needs_clarification", confidence < 0.7)

            # å¦‚æœéœ€è¦æ¾„æ¸…ä¸”æœ‰å¤šå€™é€‰ï¼Œç”Ÿæˆæ¾„æ¸…é—®é¢˜
            clarification_question = None
            if needs_clarification and len(candidates) > 1:
                clarify_prompt = build_clarify_question_prompt(
                    request.user_input,
                    [{"intent_code": c.intent_code, "intent_name": c.intent_name} for c in candidates]
                )
                clarify_messages = [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„å¯¹è¯åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": clarify_prompt}
                ]
                clarify_result = query_qwen(clarify_messages, enable_thinking=False)
                try:
                    clarify_json = json.loads(clarify_result.get("content", "{}"))
                    clarification_question = clarify_json.get("clarification_question")
                except:
                    clarification_question = f"è¯·é—®æ‚¨æ˜¯æƒ³è¦ {candidates[0].intent_name} è¿˜æ˜¯ {candidates[1].intent_name}ï¼Ÿ"

            return IntentClassifyResponse(
                success=True,
                matched_intent_code=parsed.get("matched_intent_code"),
                matched_intent_name=parsed.get("matched_intent_name"),
                confidence=confidence,
                candidates=candidates,
                is_ambiguous=is_ambiguous,
                needs_clarification=needs_clarification,
                clarification_question=clarification_question,
                reasoning=parsed.get("reasoning")
            )

        except json.JSONDecodeError as je:
            return IntentClassifyResponse(
                success=False,
                matched_intent_code=None,
                matched_intent_name=None,
                confidence=0,
                candidates=[],
                is_ambiguous=True,
                needs_clarification=True,
                message=f"LLMå“åº”è§£æå¤±è´¥: {str(je)}"
            )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ„å›¾åˆ†ç±»å¤±è´¥: {str(e)}")


@app.post("/api/ai/intent/clarify", response_model=IntentClarifyResponse)
async def generate_clarification(request: IntentClarifyRequest):
    """
    ç”Ÿæˆæ¾„æ¸…é—®é¢˜

    å½“æ„å›¾è¯†åˆ«æœ‰æ­§ä¹‰æ—¶ï¼Œç”Ÿæˆå‹å¥½çš„æ¾„æ¸…é—®é¢˜å¸®åŠ©ç”¨æˆ·é€‰æ‹©

    è¾“å…¥:
    - user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
    - candidates: å€™é€‰æ„å›¾åˆ—è¡¨

    è¾“å‡º:
    - clarification_question: æ¾„æ¸…é—®é¢˜
    - options: é€‰é¡¹åˆ—è¡¨
    """
    if not client:
        raise HTTPException(status_code=500, detail="DASHSCOPE_API_KEYæœªé…ç½®")

    try:
        prompt = build_clarify_question_prompt(request.user_input, request.candidates)

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„å¯¹è¯åŠ©æ‰‹ï¼Œæ“…é•¿ç”Ÿæˆè‡ªç„¶çš„æ¾„æ¸…é—®é¢˜ã€‚"},
            {"role": "user", "content": prompt}
        ]

        result = query_qwen(messages, enable_thinking=False)
        content = result.get("content", "")

        try:
            json_start = content.find("{")
            json_end = content.rfind("}") + 1
            if json_start >= 0 and json_end > json_start:
                parsed = json.loads(content[json_start:json_end])
            else:
                raise ValueError("No JSON found")

            return IntentClarifyResponse(
                success=True,
                clarification_question=parsed.get("clarification_question", "è¯·é—®æ‚¨å…·ä½“æƒ³è¦åšä»€ä¹ˆï¼Ÿ"),
                options=parsed.get("options", [])
            )

        except:
            # é™çº§ï¼šç”Ÿæˆç®€å•çš„æ¾„æ¸…é—®é¢˜
            options = [
                {"value": c.get("intent_code", c.get("code", "")),
                 "label": c.get("intent_name", c.get("name", ""))}
                for c in request.candidates[:3]
            ]
            return IntentClarifyResponse(
                success=True,
                clarification_question="è¯·é—®æ‚¨æ˜¯æƒ³è¦è¿›è¡Œä»¥ä¸‹å“ªé¡¹æ“ä½œï¼Ÿ",
                options=options
            )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆæ¾„æ¸…é—®é¢˜å¤±è´¥: {str(e)}")


# ==================== æ•°æ®æ“ä½œè§£ææœåŠ¡ (BUG-003 ä¿®å¤) ====================

class DataOperationParseRequest(BaseModel):
    """æ•°æ®æ“ä½œè§£æè¯·æ±‚"""
    user_input: str                          # ç”¨æˆ·åŸå§‹è¾“å…¥
    factory_id: str                          # å·¥å‚ID
    supported_entities: List[str]            # æ”¯æŒçš„å®ä½“ç±»å‹åˆ—è¡¨
    context: Optional[Dict] = None           # ä¸Šä¸‹æ–‡ä¿¡æ¯

class DataOperationParseResponse(BaseModel):
    """æ•°æ®æ“ä½œè§£æå“åº”"""
    success: bool
    entity_type: Optional[str] = None        # å®ä½“ç±»å‹
    entity_identifier: Optional[str] = None  # å®ä½“æ ‡è¯†ç¬¦
    updates: Optional[Dict[str, Any]] = None # æ›´æ–°å­—æ®µ
    operation: Optional[str] = None          # æ“ä½œç±»å‹: UPDATE, DELETE, CREATE
    message: Optional[str] = None            # æ¶ˆæ¯

@app.post("/api/ai/intent/parse-data-operation", response_model=DataOperationParseResponse)
async def parse_data_operation(request: DataOperationParseRequest):
    """
    è§£ææ•°æ®æ“ä½œæ„å›¾ï¼Œæå–å®ä½“ç±»å‹ã€æ ‡è¯†ç¬¦å’Œæ›´æ–°å­—æ®µ

    BUG-003 ä¿®å¤: æ·»åŠ æ­¤ç«¯ç‚¹æ”¯æŒ DataOperationIntentHandler çš„ AI è§£æ
    """
    try:
        user_input = request.user_input.strip()

        if not user_input:
            return DataOperationParseResponse(
                success=False,
                message="ç”¨æˆ·è¾“å…¥ä¸ºç©º"
            )

        if not client:
            return DataOperationParseResponse(
                success=False,
                message="AIæœåŠ¡æœªé…ç½®"
            )

        # æ„å»ºè§£ææç¤ºè¯
        entities_desc = ", ".join(request.supported_entities)
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®æ“ä½œè§£æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·è¾“å…¥ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯:
1. å®ä½“ç±»å‹ (entity_type): ç”¨æˆ·æƒ³æ“ä½œå“ªç§å®ä½“ï¼Ÿé€‰æ‹©: {entities_desc}
2. å®ä½“æ ‡è¯† (entity_identifier): å®ä½“çš„IDæˆ–åç§°ï¼ˆå¦‚æœç”¨æˆ·æåˆ°ï¼‰
3. æ“ä½œç±»å‹ (operation): UPDATEï¼ˆä¿®æ”¹ï¼‰ã€CREATEï¼ˆåˆ›å»ºï¼‰ã€DELETEï¼ˆåˆ é™¤ï¼‰
4. æ›´æ–°å­—æ®µ (updates): å¦‚æœæ˜¯UPDATEæ“ä½œï¼Œæå–è¦æ›´æ–°çš„å­—æ®µå’Œæ–°å€¼

ç”¨æˆ·è¾“å…¥: "{user_input}"

è¯·ä»¥JSONæ ¼å¼è¿”å›:
{{
    "entity_type": "å®ä½“ç±»å‹ï¼ˆè‹±æ–‡ï¼‰",
    "entity_identifier": "å®ä½“æ ‡è¯†ï¼ˆå¯ä»¥ä¸ºnullï¼‰",
    "operation": "UPDATE|CREATE|DELETE",
    "updates": {{"å­—æ®µå": "æ–°å€¼"}}
}}

æ³¨æ„:
- entity_type å¿…é¡»æ˜¯: {entities_desc} ä¹‹ä¸€
- å¸¸è§å­—æ®µæ˜ å°„: å•ä»·/ä»·æ ¼â†’unitPrice, åç§°â†’name, æ•°é‡â†’quantity, çŠ¶æ€â†’status
- å¦‚æœç”¨æˆ·æåˆ°"äº§å“"ä½†æ²¡æœ‰æŒ‡å®šIDï¼Œentity_identifierå¯ä»¥ä¸ºnull"""

        # è°ƒç”¨ LLM
        response = client.chat.completions.create(
            model=DASHSCOPE_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„æ•°æ®æ“ä½œè§£æå™¨ï¼Œåªè¿”å›JSONæ ¼å¼çš„ç»“æœã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
            response_format={"type": "json_object"}
        )

        result_text = response.choices[0].message.content.strip()

        try:
            result = json.loads(result_text)

            entity_type = result.get("entity_type")
            entity_identifier = result.get("entity_identifier")
            operation = result.get("operation", "UPDATE")
            updates = result.get("updates", {})

            # éªŒè¯å®ä½“ç±»å‹
            if entity_type and entity_type not in request.supported_entities:
                # å°è¯•æ˜ å°„
                entity_map = {
                    "äº§å“": "ProductType",
                    "äº§å“ç±»å‹": "ProductType",
                    "ç”Ÿäº§è®¡åˆ’": "ProductionPlan",
                    "ç”Ÿäº§æ‰¹æ¬¡": "ProcessingBatch",
                    "åŸæ–™æ‰¹æ¬¡": "MaterialBatch",
                    "åŸæ–™": "MaterialBatch"
                }
                entity_type = entity_map.get(entity_type, entity_type)

            return DataOperationParseResponse(
                success=True,
                entity_type=entity_type,
                entity_identifier=entity_identifier,
                operation=operation,
                updates=updates
            )

        except json.JSONDecodeError:
            return DataOperationParseResponse(
                success=False,
                message=f"AIå“åº”è§£æå¤±è´¥: {result_text[:100]}"
            )

    except Exception as e:
        return DataOperationParseResponse(
            success=False,
            message=f"è§£æå¤±è´¥: {str(e)}"
        )


@app.get("/api/ai/intent/health")
async def intent_health():
    """æ„å›¾è¯†åˆ«æœåŠ¡å¥åº·æ£€æŸ¥"""
    return {
        'service': 'intent-classifier',
        'status': 'running' if client else 'unavailable',
        'model': DASHSCOPE_MODEL,
        'api_configured': bool(DASHSCOPE_API_KEY)
    }


# ==================== å¯åŠ¨ ====================
if __name__ == "__main__":
    import uvicorn
    import sys

    # ä¿®å¤Windowsç»ˆç«¯ç¼–ç é—®é¢˜
    if sys.platform == 'win32':
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

    print("\n" + "="*50)
    print("[START] AI Cost Analysis Service")
    print("="*50)
    print(f"Model: Alibaba Qwen ({DASHSCOPE_MODEL})")
    print(f"Port: 8085")

    if not DASHSCOPE_API_KEY:
        print("[WARN] DASHSCOPE_API_KEY not configured")
        print("Please set in .env: DASHSCOPE_API_KEY=sk-xxx")
    else:
        print("[OK] API Key configured")

    # é¢„çƒ­ Embedding æ¨¡å‹ (å¯é€‰)
    if EMBEDDING_ENABLED:
        print("[INFO] Embedding service enabled (Sentence-BERT)")
        # å¯åŠ¨æ—¶ä¸è‡ªåŠ¨é¢„çƒ­ï¼Œé¦–æ¬¡è¯·æ±‚æ—¶æ‡’åŠ è½½
        # warmup_embedding()
    else:
        print("[WARN] Embedding service disabled")

    print("="*50 + "\n")

    uvicorn.run("main:app", host="0.0.0.0", port=8085, reload=True)
