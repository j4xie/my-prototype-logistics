"""
ç™½å©çºªé£Ÿå“æº¯æºç³»ç»Ÿ - AIé£Ÿå“åŠ å·¥æ•°æ®åˆ†ææœåŠ¡
åŸºäºé˜¿é‡Œäº‘é€šä¹‰åƒé—® (DashScope) çš„æ™ºèƒ½åˆ†æAPI
æ”¯æŒæ€è€ƒæ¨¡å¼ (Thinking Mode) - æ·±åº¦æ¨ç†åˆ†æ
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, Dict, List
import os
import json
from dotenv import load_dotenv

# å¯¼å…¥ OpenAI SDK (é˜¿é‡Œäº‘ DashScope å…¼å®¹ OpenAI æ ¼å¼)
from openai import OpenAI

load_dotenv()

# ==================== é…ç½® ====================
# é˜¿é‡Œäº‘ DashScope é…ç½®
DASHSCOPE_API_KEY = os.environ.get('DASHSCOPE_API_KEY', '')
DASHSCOPE_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
# å¯é€‰æ¨¡å‹: qwen-turbo (æœ€å¿«æœ€ä¾¿å®œ), qwen-plus (å¹³è¡¡), qwen-max (æœ€å¼º)
DASHSCOPE_MODEL = os.environ.get('DASHSCOPE_MODEL', 'qwen-plus')

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ (å…¼å®¹ DashScope)
client = None
if DASHSCOPE_API_KEY:
    client = OpenAI(
        api_key=DASHSCOPE_API_KEY,
        base_url=DASHSCOPE_BASE_URL,
    )

# ==================== FastAPI åº”ç”¨ ====================
app = FastAPI(title="é£Ÿå“åŠ å·¥æ•°æ®åˆ†æ API", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== æ•°æ®æ¨¡å‹ ====================
class FoodProcessingRequest(BaseModel):
    section_data: Dict[str, str]  # æ‰€æœ‰å‚æ•°ï¼ˆå®é™…å€¼å’Œå¹³å‡å€¼ï¼‰

class FoodProcessingResponse(BaseModel):
    success: bool
    analysis: str
    message: Optional[str] = None

# æˆæœ¬åˆ†æä¸“ç”¨è¯·æ±‚æ¨¡å‹
class CostAnalysisRequest(BaseModel):
    message: str  # æˆæœ¬æ•°æ®çš„æ–‡æœ¬æè¿°
    user_id: str  # å·¥å‚ID_batch_æ‰¹æ¬¡ID
    session_id: Optional[str] = None
    enable_thinking: Optional[bool] = True  # é»˜è®¤å¼€å¯æ€è€ƒæ¨¡å¼
    thinking_budget: Optional[int] = 50  # æ€è€ƒé¢„ç®— (10-100)

# ==================== æ ¸å¿ƒåŠŸèƒ½ ====================
def query_qwen(messages: list, enable_thinking: bool = False, thinking_budget: int = 50) -> dict:
    """
    è°ƒç”¨é˜¿é‡Œäº‘é€šä¹‰åƒé—®æ¨¡å‹

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
        thinking_budget: æ€è€ƒé¢„ç®— (10-100)

    Returns:
        dict: {
            "content": str,  # æœ€ç»ˆå›ç­”
            "reasoning_content": str,  # æ€è€ƒè¿‡ç¨‹ (ä»…æ€è€ƒæ¨¡å¼)
            "thinking_enabled": bool
        }
    """
    if not client:
        raise HTTPException(status_code=500, detail="DASHSCOPE_API_KEYæœªé…ç½®")

    try:
        if enable_thinking:
            # æ€è€ƒæ¨¡å¼ï¼šä½¿ç”¨æµå¼å“åº”æ”¶é›†æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ
            return query_qwen_with_thinking(messages, thinking_budget)
        else:
            # æ™®é€šæ¨¡å¼
            completion = client.chat.completions.create(
                model=DASHSCOPE_MODEL,
                messages=messages,
                max_tokens=1500,
                temperature=0.7,
            )
            return {
                "content": completion.choices[0].message.content,
                "reasoning_content": "",
                "thinking_enabled": False
            }
    except Exception as e:
        # å‚è€ƒæ–‡æ¡£: https://help.aliyun.com/zh/model-studio/developer-reference/error-code
        raise HTTPException(status_code=500, detail=f"é€šä¹‰åƒé—®è°ƒç”¨å¤±è´¥: {str(e)}")


def query_qwen_with_thinking(messages: list, thinking_budget: int = 50) -> dict:
    """
    æ€è€ƒæ¨¡å¼è°ƒç”¨ - ä½¿ç”¨æµå¼å“åº”æ”¶é›†æ€è€ƒè¿‡ç¨‹

    æ€è€ƒæ¨¡å¼ä¼šè¿”å›ä¸¤éƒ¨åˆ†å†…å®¹:
    1. reasoning_content: AIçš„æ€è€ƒè¿‡ç¨‹
    2. content: æœ€ç»ˆå›ç­”
    """
    reasoning_content = ""
    answer_content = ""

    try:
        completion = client.chat.completions.create(
            model=DASHSCOPE_MODEL,
            messages=messages,
            extra_body={
                "enable_thinking": True,
                "thinking_budget": thinking_budget
            },
            stream=True,
            stream_options={
                "include_usage": True
            },
        )

        for chunk in completion:
            if chunk.choices and len(chunk.choices) > 0:
                choice = chunk.choices[0]
                delta = choice.delta

                # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹
                if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                    reasoning_content += delta.reasoning_content
                elif hasattr(delta, 'content') and delta.content:
                    answer_content += delta.content

        return {
            "content": answer_content,
            "reasoning_content": reasoning_content,
            "thinking_enabled": True
        }

    except Exception as e:
        # å¦‚æœæ€è€ƒæ¨¡å¼å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼
        print(f"[WARN] æ€è€ƒæ¨¡å¼å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼: {e}")
        completion = client.chat.completions.create(
            model=DASHSCOPE_MODEL,
            messages=messages,
            max_tokens=1500,
            temperature=0.7,
        )
        return {
            "content": completion.choices[0].message.content,
            "reasoning_content": "",
            "thinking_enabled": False
        }

def build_prompt(section_data: Dict[str, str]) -> str:
    """
    æ„å»ºåˆ†ææç¤ºè¯ - å°†ç”¨æˆ·è¾“å…¥çš„å‚æ•°è½¬æ¢ä¸ºPromptæ–‡æœ¬

    ç¤ºä¾‹ï¼š
    è¾“å…¥: {"thawing_time": "4.5", "avg_thawing_time": "4.0", ...}
    è¾“å‡º:
    '''
    è¯·åˆ†æä»¥ä¸‹é£Ÿå“åŠ å·¥æ•°æ®ï¼ˆå®é™…æ•°æ® vs å¹³å‡æ•°æ®ï¼‰ï¼š

    ã€æ¥æ”¶&åŠè§£å†»ã€‘
      è§£å†»æ—¶é—´: å®é™…=4.5 | å¹³å‡=4.0
      ...
    '''
    """
    sections = {
        'æ¥æ”¶&åŠè§£å†»': ['thawing_time', 'drip_loss', 'temperature'],
        'å»å°¾': ['tail_rate', 'trim_rate', 'rework_rate'],
        'æœºæ¢°åˆ‡ç‰‡': ['thickness_sd', 'jam_rate', 'oee'],
        'æ¸…æ´—(å€æ¸©)': ['water_usage', 'outlet_temp', 'micro_pass_rate'],
        'æ²¥å¹²': ['surface_loss', 'dwell_time'],
        'æ·±è¾Šä¸Šæµ†(åŠæˆå“)': ['marinade_absorption', 'ph_salinity', 'marinade_variance'],
        'åŒ…è£…&IQFé€Ÿå†»': ['sec', 'pack_pass_rate', 'cooling_time'],
        'å“æ§&é£Ÿå“å®‰å…¨': ['ccp_pass_rate', 'audit_issues'],
        'æ¸…æ´—&æ¢çº¿': ['clean_duration', 'atp_pass_rate'],
    }

    param_labels = {
        'thawing_time': 'è§£å†»æ—¶é—´', 'drip_loss': 'æ»´æ°´æŸå¤±ç‡(%)', 'temperature': 'æ¸©åº¦(Â°C)',
        'tail_rate': 'å°¾æ®µç‡(%)', 'trim_rate': 'ä¿®æ•´ç‡(%)', 'rework_rate': 'è¿”å·¥ç‡(%)',
        'thickness_sd': 'åšåº¦åå·®SD(mm)', 'jam_rate': 'å¡æœºç‡(%)', 'oee': 'OEE(%)',
        'water_usage': 'å•ä½ç”¨æ°´(L/kg)', 'outlet_temp': 'å‡ºå£æ¸©åº¦(Â°C)',
        'micro_pass_rate': 'å¾®ç”Ÿç‰©æ£€æµ‹åˆæ ¼ç‡(%)',
        'surface_loss': 'è¡¨é¢å¤±æ°´ç‡(%)', 'dwell_time': 'åœç•™æ—¶é—´(min)',
        'marinade_absorption': 'è…Œæ–™å¸æ”¶ç‡(%)', 'ph_salinity': 'pH/ç›åº¦',
        'marinade_variance': 'è…Œæ–™æ¶ˆè€—å·®å¼‚(%)',
        'sec': 'sEC(kWh/kg)', 'pack_pass_rate': 'åŒ…è£…åˆæ ¼ç‡(%)',
        'cooling_time': 'æ ¸å¿ƒé™æ¸©æ—¶é—´(min)',
        'ccp_pass_rate': 'CCPåˆæ ¼ç‡(%)', 'audit_issues': 'å®¡è®¡é—®é¢˜æ•°(ä¸ª)',
        'clean_duration': 'æ¸…æ´æ—¶é•¿(min)', 'atp_pass_rate': 'ATPæ£€æµ‹åˆæ ¼ç‡(%)',
    }

    prompt_parts = ["è¯·åˆ†æä»¥ä¸‹é£Ÿå“åŠ å·¥æ•°æ®ï¼ˆå®é™…æ•°æ® vs å¹³å‡æ•°æ®ï¼‰ï¼š\n"]

    # éå†æ¯ä¸ªç¯èŠ‚
    for section_name, param_keys in sections.items():
        section_text = f"\nã€{section_name}ã€‘\n"
        section_has_data = False

        # éå†æ¯ä¸ªå‚æ•°
        for param_key in param_keys:
            actual_val = section_data.get(param_key, "").strip()
            avg_val = section_data.get(f"avg_{param_key}", "").strip()

            if actual_val or avg_val:
                section_has_data = True
                label = param_labels.get(param_key, param_key)
                # ç»„è£…æˆ: "è§£å†»æ—¶é—´: å®é™…=4.5 | å¹³å‡=4.0"
                section_text += f"  {label}: å®é™…={actual_val or 'æœªå¡«'} | å¹³å‡={avg_val or 'æœªå¡«'}\n"

        if section_has_data:
            prompt_parts.append(section_text)

    if len(prompt_parts) == 1:
        prompt_parts.append("\nâš ï¸ æœªæä¾›ä»»ä½•æ•°æ®")
    else:
        prompt_parts.append("\nè¯·åŸºäºä»¥ä¸Šæ•°æ®è¿›è¡Œæ·±åº¦åˆ†æï¼Œç»™å‡ºä¸“ä¸šå»ºè®®ã€‚")

    return "".join(prompt_parts)

# ==================== APIç«¯ç‚¹ ====================
@app.get("/")
async def root():
    return {
        "service": "é£Ÿå“åŠ å·¥æ•°æ®åˆ†æ API",
        "status": "running",
        "model": f"é˜¿é‡Œäº‘é€šä¹‰åƒé—® ({DASHSCOPE_MODEL})",
        "api_configured": bool(DASHSCOPE_API_KEY)
    }

@app.post("/api/ai/food-processing-analysis", response_model=FoodProcessingResponse)
async def analyze(request: FoodProcessingRequest):
    """
    é£Ÿå“åŠ å·¥æ•°æ®åˆ†æ - æ ¸å¿ƒåŠŸèƒ½

    æµç¨‹ï¼š
    1. æ¥æ”¶section_data (æ‰€æœ‰å‚æ•°)
    2. æ„å»ºPromptæ–‡æœ¬
    3. å‘é€ç»™Llama 3.1
    4. è¿”å›AIåˆ†æç»“æœ
    """
    try:
        # æ­¥éª¤1: æ„å»ºPrompt
        prompt = build_prompt(request.section_data)

        # æ­¥éª¤2: è°ƒç”¨AIæ¨¡å‹
        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯é£Ÿå“åŠ å·¥ä¸“å®¶ï¼Œä¸“é—¨åˆ†æåŠ å·¥æ•°æ®ã€‚

ä»»åŠ¡ï¼š
1. å¯¹æ¯”å®é™…æ•°æ®ä¸å¹³å‡æ•°æ®ï¼Œè¯†åˆ«å·®å¼‚
2. è¯Šæ–­é—®é¢˜å’Œé£é™©ç‚¹
3. æä¾›å…·ä½“ä¼˜åŒ–å»ºè®®
4. åˆ†ææˆæœ¬ä¼˜åŒ–ç©ºé—´

è¦æ±‚ï¼š
- ç®€æ´ä¸“ä¸šçš„è¯­è¨€
- å…·ä½“æ•°å­—å’Œç™¾åˆ†æ¯”å¯¹æ¯”
- å¯é‡åŒ–çš„æ”¹è¿›ç›®æ ‡
- ä¸­æ–‡å›å¤

è¾“å‡ºæ ¼å¼ï¼š
ğŸ“Š **æ€»ä½“è¯„ä¼°**
[æ•´ä½“è¯„ä»·]

ğŸ” **ç¯èŠ‚åˆ†æ**
[é€ç¯èŠ‚åˆ†æå®é™…vså¹³å‡å·®å¼‚]

âš ï¸ **ä¸»è¦é—®é¢˜**
1. [é—®é¢˜åŠå½±å“]

ğŸ’¡ **ä¼˜åŒ–å»ºè®®**
1. [å…·ä½“å»ºè®®]

ğŸ“ˆ **é¢„æœŸæ”¶ç›Š**
[é¢„æœŸæ”¹å–„]"""
            },
            {
                "role": "user",
                "content": prompt  # è¿™é‡Œæ˜¯ç”¨æˆ·æ•°æ®è½¬æ¢æˆçš„Promptæ–‡æœ¬
            }
        ]

        # æ­¥éª¤3: è·å–AIåˆ†æ (æ™®é€šæ¨¡å¼ï¼Œä¸ä½¿ç”¨æ€è€ƒ)
        result = query_qwen(messages, enable_thinking=False)

        # æ­¥éª¤4: è¿”å›ç»“æœ
        return FoodProcessingResponse(
            success=True,
            analysis=result["content"],
            message="åˆ†æå®Œæˆ"
        )

    except Exception as e:
        return FoodProcessingResponse(
            success=False,
            analysis="",
            message=f"åˆ†æå¤±è´¥: {str(e)}"
        )

@app.post("/api/ai/chat")
async def cost_analysis(request: CostAnalysisRequest):
    """
    æˆæœ¬åˆ†æä¸“ç”¨æ¥å£ - ä¸Javaåç«¯é›†æˆ

    æ¥æ”¶æ ¼å¼åŒ–çš„æˆæœ¬æ•°æ®æ–‡æœ¬ï¼Œè¿”å›AIåˆ†æå»ºè®®
    """
    try:
        import uuid
        import time

        # æ„å»ºä¸“é—¨çš„æˆæœ¬åˆ†ææ¶ˆæ¯
        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯é£Ÿå“åŠ å·¥ä¼ä¸šçš„æˆæœ¬åˆ†æä¸“å®¶ã€‚

ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”Ÿäº§æ‰¹æ¬¡çš„æˆæœ¬æ•°æ®ï¼Œæä¾›ä¸“ä¸šçš„æˆæœ¬ä¼˜åŒ–å»ºè®®ã€‚

åˆ†æè¦ç‚¹ï¼š
1. æˆæœ¬ç»“æ„åˆç†æ€§ï¼šè¯„ä¼°åŸææ–™ã€äººå·¥ã€è®¾å¤‡æˆæœ¬çš„å æ¯”æ˜¯å¦åˆç†
2. å¼‚å¸¸è¯†åˆ«ï¼šæ‰¾å‡ºæˆæœ¬æ•°æ®ä¸­çš„å¼‚å¸¸ç‚¹å’Œé£é™©
3. å¯¹æ¯”åˆ†æï¼šå°†å½“å‰æˆæœ¬ä¸è¡Œä¸šæ ‡å‡†æˆ–å†å²æ•°æ®å¯¹æ¯”
4. ä¼˜åŒ–å»ºè®®ï¼šæä¾›å…·ä½“å¯è¡Œçš„æˆæœ¬é™ä½æªæ–½
5. æ•ˆç‡è¯„ä¼°ï¼šåˆ†æç”Ÿäº§æ•ˆç‡ã€è‰¯å“ç‡ã€äººå‡äº§èƒ½ç­‰æŒ‡æ ‡

è¾“å‡ºè¦æ±‚ï¼š
- ä½¿ç”¨ä¸­æ–‡
- ç®€æ´ä¸“ä¸šï¼Œæ¡ç†æ¸…æ™°
- æä¾›å…·ä½“æ•°å­—å’Œç™¾åˆ†æ¯”
- ç»™å‡ºå¯é‡åŒ–çš„æ”¹è¿›ç›®æ ‡
- åˆ†æè¦æ·±å…¥ï¼Œå»ºè®®è¦å…·ä½“

è¾“å‡ºæ ¼å¼ï¼š
ğŸ“Š **æˆæœ¬ç»“æ„åˆ†æ**
[åˆ†æå„é¡¹æˆæœ¬å æ¯”çš„åˆç†æ€§]

âš ï¸ **å‘ç°çš„é—®é¢˜**
1. [é—®é¢˜ç‚¹åŠå½±å“]

ğŸ’¡ **ä¼˜åŒ–å»ºè®®**
1. [å…·ä½“çš„æ”¹è¿›æªæ–½]

ğŸ“ˆ **é¢„æœŸæ•ˆæœ**
[å®æ–½å»ºè®®åçš„é¢„æœŸæˆæœ¬èŠ‚çœ]"""
            },
            {
                "role": "user",
                "content": request.message
            }
        ]

        # è·å–æ€è€ƒæ¨¡å¼é…ç½® (é»˜è®¤å¼€å¯)
        enable_thinking = request.enable_thinking if request.enable_thinking is not None else True
        thinking_budget = request.thinking_budget if request.thinking_budget else 50

        # å°è¯•è°ƒç”¨AIæ¨¡å‹ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›æ¨¡æ‹Ÿåˆ†æï¼ˆç”¨äºæ¼”ç¤ºï¼‰
        try:
            result = query_qwen(messages, enable_thinking=enable_thinking, thinking_budget=thinking_budget)
            ai_analysis = result["content"]
            reasoning_content = result["reasoning_content"]
            thinking_enabled = result["thinking_enabled"]
        except Exception as ai_error:
            # å¦‚æœAIè°ƒç”¨å¤±è´¥ï¼Œè¿”å›åŸºäºè§„åˆ™çš„æ¨¡æ‹Ÿåˆ†æï¼ˆä»…ç”¨äºæ¼”ç¤ºå’Œæµ‹è¯•ï¼‰
            print(f"[WARN] AIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿåˆ†æ: {ai_error}")
            ai_analysis = generate_mock_analysis(request.message)
            reasoning_content = ""
            thinking_enabled = False

        # ç”Ÿæˆä¼šè¯IDï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
        session_id = request.session_id if request.session_id else f"session_{uuid.uuid4().hex[:16]}"

        # è¿”å›ç»“æœï¼ˆåŒ¹é…JavaæœŸæœ›çš„æ ¼å¼ï¼Œå¢åŠ æ€è€ƒå†…å®¹ï¼‰
        return {
            "success": True,
            "aiAnalysis": ai_analysis,
            "reasoningContent": reasoning_content,  # æ€è€ƒè¿‡ç¨‹
            "thinkingEnabled": thinking_enabled,    # æ˜¯å¦ä½¿ç”¨äº†æ€è€ƒæ¨¡å¼
            "sessionId": session_id,
            "messageCount": 1,
            "timestamp": int(time.time() * 1000)
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"AIåˆ†æå¤±è´¥: {str(e)}")

def generate_mock_analysis(cost_data: str) -> str:
    """
    ç”Ÿæˆæ¨¡æ‹Ÿçš„æˆæœ¬åˆ†æï¼ˆç”¨äºæ¼”ç¤ºï¼Œå½“AI APIä¸å¯ç”¨æ—¶ï¼‰
    """
    # ä»æˆæœ¬æ•°æ®ä¸­æå–å…³é”®ä¿¡æ¯
    lines = cost_data.split('\n')
    batch_number = ""
    product_name = ""
    total_cost = 0
    material_cost_ratio = 0
    labor_cost_ratio = 0
    equipment_cost_ratio = 0
    yield_rate = 0

    for line in lines:
        if "æ‰¹æ¬¡ç¼–å·:" in line:
            batch_number = line.split(':')[1].strip()
        elif "äº§å“åç§°:" in line:
            product_name = line.split(':')[1].strip()
        elif "æ€»æˆæœ¬:" in line:
            # æå–æ•°å­—
            import re
            match = re.search(r'Â¥([\d,]+)', line)
            if match:
                total_cost = int(match.group(1).replace(',', ''))
        elif "åŸææ–™æˆæœ¬:" in line and "å æ¯”" in line:
            match = re.search(r'å æ¯”([\d.]+)%', line)
            if match:
                material_cost_ratio = float(match.group(1))
        elif "äººå·¥æˆæœ¬:" in line and "å æ¯”" in line:
            match = re.search(r'å æ¯”([\d.]+)%', line)
            if match:
                labor_cost_ratio = float(match.group(1))
        elif "è®¾å¤‡æˆæœ¬:" in line and "å æ¯”" in line:
            match = re.search(r'å æ¯”([\d.]+)%', line)
            if match:
                equipment_cost_ratio = float(match.group(1))
        elif "è‰¯å“ç‡:" in line:
            match = re.search(r'([\d.]+)%', line)
            if match:
                yield_rate = float(match.group(1))

    # åŸºäºæ•°æ®ç”Ÿæˆåˆ†æ
    analysis = f"""ğŸ“Š **æˆæœ¬ç»“æ„åˆ†æ**

æ ¹æ®æ‰¹æ¬¡ {batch_number} ({product_name}) çš„æˆæœ¬æ•°æ®ï¼Œæ€»æˆæœ¬ä¸º Â¥{total_cost:,}ï¼Œæˆæœ¬ç»“æ„å¦‚ä¸‹ï¼š

- åŸææ–™æˆæœ¬å æ¯” {material_cost_ratio:.1f}%
- äººå·¥æˆæœ¬å æ¯” {labor_cost_ratio:.1f}%
- è®¾å¤‡æˆæœ¬å æ¯” {equipment_cost_ratio:.1f}%

**ç»“æ„è¯„ä¼°ï¼š**
"""

    # åŸææ–™æˆæœ¬åˆ†æ
    if material_cost_ratio > 60:
        analysis += f"â€¢ åŸææ–™æˆæœ¬å æ¯” {material_cost_ratio:.1f}% åé«˜ï¼Œå»ºè®®ä¼˜åŒ–é‡‡è´­ç­–ç•¥\n"
    elif material_cost_ratio < 45:
        analysis += f"â€¢ åŸææ–™æˆæœ¬å æ¯” {material_cost_ratio:.1f}% åˆç†ï¼Œé‡‡è´­æ§åˆ¶è‰¯å¥½\n"
    else:
        analysis += f"â€¢ åŸææ–™æˆæœ¬å æ¯” {material_cost_ratio:.1f}% å¤„äºæ­£å¸¸èŒƒå›´\n"

    # äººå·¥æˆæœ¬åˆ†æ
    if labor_cost_ratio > 35:
        analysis += f"â€¢ äººå·¥æˆæœ¬å æ¯” {labor_cost_ratio:.1f}% è¾ƒé«˜ï¼Œå­˜åœ¨äººå‘˜æ•ˆç‡ä¼˜åŒ–ç©ºé—´\n"
    else:
        analysis += f"â€¢ äººå·¥æˆæœ¬å æ¯” {labor_cost_ratio:.1f}% åˆç†\n"

    # è®¾å¤‡æˆæœ¬åˆ†æ
    if equipment_cost_ratio < 15:
        analysis += f"â€¢ è®¾å¤‡æˆæœ¬å æ¯” {equipment_cost_ratio:.1f}% åˆç†ï¼Œè®¾å¤‡åˆ©ç”¨ç‡è‰¯å¥½\n"

    analysis += f"\nâš ï¸ **å‘ç°çš„é—®é¢˜**\n\n"

    problems = []
    if yield_rate < 98:
        problems.append(f"1. è‰¯å“ç‡ {yield_rate:.1f}% ä½äºè¡Œä¸šæ ‡å‡†98%ï¼Œé€ æˆåŸææ–™æµªè´¹å’Œæˆæœ¬å¢åŠ ")
    if labor_cost_ratio > 35:
        problems.append(f"2. äººå·¥æˆæœ¬å æ¯” {labor_cost_ratio:.1f}% åé«˜ï¼Œå¯èƒ½å­˜åœ¨äººå‘˜å†—ä½™æˆ–æ•ˆç‡ä¸è¶³")
    if material_cost_ratio > 60:
        problems.append(f"3. åŸææ–™æˆæœ¬å æ¯” {material_cost_ratio:.1f}% è¿‡é«˜ï¼Œéœ€è¦å®¡æŸ¥ä¾›åº”å•†æŠ¥ä»·å’Œé‡‡è´­æµç¨‹")

    if problems:
        analysis += "\n".join(problems)
    else:
        analysis += "1. æœªå‘ç°é‡å¤§æˆæœ¬å¼‚å¸¸ï¼Œæ•´ä½“æ§åˆ¶è‰¯å¥½\n"

    analysis += f"\n\nğŸ’¡ **ä¼˜åŒ–å»ºè®®**\n\n"

    suggestions = []
    if yield_rate < 98:
        target_saving = total_cost * (98 - yield_rate) / 100 * 0.5
        suggestions.append(f"1. **æå‡è‰¯å“ç‡**ï¼šåŠ å¼ºè´¨é‡æ§åˆ¶ï¼Œç›®æ ‡æå‡è‡³98%ä»¥ä¸Šï¼Œé¢„è®¡èŠ‚çœæˆæœ¬çº¦Â¥{target_saving:,.0f}")

    if labor_cost_ratio > 35:
        target_ratio = 30
        target_saving = total_cost * (labor_cost_ratio - target_ratio) / 100
        suggestions.append(f"2. **ä¼˜åŒ–äººå‘˜é…ç½®**ï¼šé€šè¿‡æµç¨‹ä¼˜åŒ–å’ŒåŸ¹è®­æå‡äººå‡äº§èƒ½ï¼Œç›®æ ‡é™ä½äººå·¥æˆæœ¬è‡³30%ï¼Œé¢„è®¡èŠ‚çœÂ¥{target_saving:,.0f}")

    if material_cost_ratio > 60:
        target_saving = total_cost * 0.05
        suggestions.append(f"3. **é‡‡è´­ä¼˜åŒ–**ï¼šå¯¹æ¯”å¤šå®¶ä¾›åº”å•†æŠ¥ä»·ï¼Œæ‰¹é‡é‡‡è´­è°ˆåˆ¤ï¼Œé¢„è®¡å¯é™ä½é‡‡è´­æˆæœ¬3-5%ï¼Œçº¦Â¥{target_saving:,.0f}")

    suggestions.append("4. **è®¾å¤‡åˆ©ç”¨ç‡**ï¼šä¿æŒç°æœ‰è®¾å¤‡åˆ©ç”¨æ°´å¹³ï¼Œå®šæœŸç»´æŠ¤ä¿å…»å»¶é•¿ä½¿ç”¨å¯¿å‘½")

    analysis += "\n".join(suggestions)

    # é¢„æœŸæ”¶ç›Š
    total_potential_saving = sum([
        total_cost * (98 - yield_rate) / 100 * 0.5 if yield_rate < 98 else 0,
        total_cost * (labor_cost_ratio - 30) / 100 if labor_cost_ratio > 35 else 0,
        total_cost * 0.05 if material_cost_ratio > 60 else 0
    ])

    if total_potential_saving > 0:
        new_unit_cost_estimate = (total_cost - total_potential_saving) / (total_cost / 7.20)  # å‡è®¾å•ä½æˆæœ¬7.20
        analysis += f"\n\nğŸ“ˆ **é¢„æœŸæ•ˆæœ**\n\n"
        analysis += f"å®æ–½ä»¥ä¸Šä¼˜åŒ–æªæ–½åï¼š\n"
        analysis += f"â€¢ é¢„è®¡æ€»æˆæœ¬å¯ä» Â¥{total_cost:,} é™ä½è‡³ Â¥{total_cost - total_potential_saving:,.0f}\n"
        analysis += f"â€¢ æˆæœ¬èŠ‚çœçº¦ Â¥{total_potential_saving:,.0f} ({total_potential_saving/total_cost*100:.1f}%)\n"
        analysis += f"â€¢ å•ä½æˆæœ¬é¢„è®¡ä» Â¥7.20/kg é™è‡³ Â¥{new_unit_cost_estimate:.2f}/kg\n"
        analysis += f"â€¢ æ•´ä½“åˆ©æ¶¦ç‡å¯æå‡ {total_potential_saving/total_cost*100:.1f} ä¸ªç™¾åˆ†ç‚¹"
    else:
        analysis += f"\n\nğŸ“ˆ **é¢„æœŸæ•ˆæœ**\n\n"
        analysis += f"å½“å‰æˆæœ¬æ§åˆ¶å·²ç»è¾ƒä¸ºä¼˜ç§€ï¼Œå»ºè®®ï¼š\n"
        analysis += f"â€¢ ä¿æŒç°æœ‰æˆæœ¬ç®¡ç†æ°´å¹³\n"
        analysis += f"â€¢ æŒç»­ç›‘æ§å„é¡¹æˆæœ¬æŒ‡æ ‡\n"
        analysis += f"â€¢ æ¢ç´¢è‡ªåŠ¨åŒ–å’ŒæŠ€æœ¯å‡çº§æœºä¼š"

    analysis += "\n\n---\nğŸ’¡ *æœ¬åˆ†æåŸºäºæä¾›çš„æˆæœ¬æ•°æ®ç”Ÿæˆï¼Œå…·ä½“å®æ–½è¯·ç»“åˆå·¥å‚å®é™…æƒ…å†µè°ƒæ•´*"

    return analysis


# ==================== è°ƒåº¦æœåŠ¡ç«¯ç‚¹ ====================

# å¯¼å…¥è°ƒåº¦æœåŠ¡æ¨¡å—
try:
    from scheduling_service import (
        CompletionProbabilityRequest,
        CompletionProbabilityResponse,
        OptimizeWorkersRequest,
        OptimizeWorkersResponse,
        GenerateScheduleRequest,
        RescheduleRequest,
        calculate_completion_probability,
        optimize_workers,
        generate_schedule,
        reschedule,
        insight_generator
    )
    SCHEDULING_SERVICE_AVAILABLE = True
except ImportError as e:
    print(f"[WARN] Scheduling service not available: {e}")
    SCHEDULING_SERVICE_AVAILABLE = False


@app.post("/scheduling/completion-probability")
async def scheduling_completion_probability(request: dict):
    """
    Monte Carlo æ¨¡æ‹Ÿ - è®¡ç®—ç”Ÿäº§å®Œæˆæ¦‚ç‡

    è¾“å…¥:
    - factory_id: å·¥å‚ID
    - schedule_id: æ’ç¨‹ID
    - remaining_quantity: å‰©ä½™æ•°é‡
    - deadline: æˆªæ­¢æ—¶é—´ (ISOæ ¼å¼)
    - assigned_workers: åˆ†é…å·¥äººæ•°
    - efficiency_mean: æ•ˆç‡å‡å€¼ (å¯é€‰)
    - efficiency_std: æ•ˆç‡æ ‡å‡†å·® (å¯é€‰)

    è¾“å‡º:
    - probability: æŒ‰æ—¶å®Œæˆæ¦‚ç‡
    - mean_hours: é¢„è®¡å®Œæˆæ—¶é—´å‡å€¼
    - confidence_lower/upper: ç½®ä¿¡åŒºé—´
    - insight: AI æ´å¯Ÿæ–‡æœ¬
    """
    if not SCHEDULING_SERVICE_AVAILABLE:
        raise HTTPException(status_code=500, detail="è°ƒåº¦æœåŠ¡ä¸å¯ç”¨")

    try:
        from scheduling_service import CompletionProbabilityRequest as CPRequest
        req = CPRequest(**request)
        result = calculate_completion_probability(req)
        return result.dict()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è®¡ç®—å®Œæˆæ¦‚ç‡å¤±è´¥: {str(e)}")


@app.post("/scheduling/optimize-workers")
async def scheduling_optimize_workers(request: dict):
    """
    OR-Tools ä¼˜åŒ– - å·¥äººåˆ†é…ä¼˜åŒ–

    è¾“å…¥:
    - factory_id: å·¥å‚ID
    - plan_id: è®¡åˆ’ID
    - workers: å·¥äººåˆ—è¡¨ [{'id', 'skill', 'cost_per_hour', 'is_temporary'}]
    - schedules: æ’ç¨‹åˆ—è¡¨ [{'id', 'required_skill', 'min_workers', 'max_workers'}]
    - objective: ä¼˜åŒ–ç›®æ ‡ (minimize_cost/maximize_efficiency/balanced)
    - max_temporary_ratio: æœ€å¤§ä¸´æ—¶å·¥æ¯”ä¾‹

    è¾“å‡º:
    - assignments: åˆ†é…ç»“æœ [{'worker_id', 'schedule_id', 'assignment_type'}]
    - total_cost: æ€»æˆæœ¬
    - efficiency_score: æ•ˆç‡è¯„åˆ†
    """
    if not SCHEDULING_SERVICE_AVAILABLE:
        raise HTTPException(status_code=500, detail="è°ƒåº¦æœåŠ¡ä¸å¯ç”¨")

    try:
        from scheduling_service import OptimizeWorkersRequest as OWRequest
        req = OWRequest(**request)
        result = optimize_workers(req)
        return result.dict()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"äººå‘˜ä¼˜åŒ–å¤±è´¥: {str(e)}")


@app.post("/scheduling/generate")
async def scheduling_generate(request: dict):
    """
    AI ç”Ÿæˆè°ƒåº¦å»ºè®®

    è¾“å…¥:
    - factory_id: å·¥å‚ID
    - plan_date: è®¡åˆ’æ—¥æœŸ (YYYY-MM-DD)
    - batch_ids: æ‰¹æ¬¡IDåˆ—è¡¨
    - production_line_ids: äº§çº¿IDåˆ—è¡¨ (å¯é€‰)
    - available_worker_ids: å¯ç”¨å·¥äººIDåˆ—è¡¨ (å¯é€‰)
    - target_completion_probability: ç›®æ ‡å®Œæˆæ¦‚ç‡

    è¾“å‡º:
    - schedules: ç”Ÿæˆçš„æ’ç¨‹åˆ—è¡¨
    - confidence: AI ç½®ä¿¡åº¦
    """
    if not SCHEDULING_SERVICE_AVAILABLE:
        raise HTTPException(status_code=500, detail="è°ƒåº¦æœåŠ¡ä¸å¯ç”¨")

    try:
        from scheduling_service import GenerateScheduleRequest as GSRequest
        req = GSRequest(**request)
        result = generate_schedule(req)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆè°ƒåº¦å¤±è´¥: {str(e)}")


@app.post("/scheduling/reschedule")
async def scheduling_reschedule(request: dict):
    """
    é‡æ–°è°ƒåº¦

    è¾“å…¥:
    - factory_id: å·¥å‚ID
    - plan_id: è®¡åˆ’ID
    - reason: é‡æ–°è°ƒåº¦åŸå› 
    - keep_completed: æ˜¯å¦ä¿ç•™å·²å®Œæˆçš„æ’ç¨‹
    - schedule_ids_to_reschedule: éœ€è¦é‡æ–°è°ƒåº¦çš„æ’ç¨‹ID
    - unavailable_worker_ids: ä¸å¯ç”¨å·¥äººID

    è¾“å‡º:
    - updated_schedules: æ›´æ–°åçš„æ’ç¨‹åˆ—è¡¨
    """
    if not SCHEDULING_SERVICE_AVAILABLE:
        raise HTTPException(status_code=500, detail="è°ƒåº¦æœåŠ¡ä¸å¯ç”¨")

    try:
        from scheduling_service import RescheduleRequest as RSRequest
        req = RSRequest(**request)
        result = reschedule(req)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é‡æ–°è°ƒåº¦å¤±è´¥: {str(e)}")


@app.post("/scheduling/explain-alert")
async def scheduling_explain_alert(request: dict):
    """
    LLM è§£é‡Šå‘Šè­¦åŸå› 

    è¾“å…¥:
    - alert_type: å‘Šè­¦ç±»å‹
    - schedule_data: æ’ç¨‹æ•°æ®
    - probability: å®Œæˆæ¦‚ç‡

    è¾“å‡º:
    - explanation: å‘Šè­¦è§£é‡Šæ–‡æœ¬
    - recommendations: å»ºè®®æªæ–½
    """
    if not SCHEDULING_SERVICE_AVAILABLE:
        raise HTTPException(status_code=500, detail="è°ƒåº¦æœåŠ¡ä¸å¯ç”¨")

    try:
        alert_type = request.get('alert_type', 'low_probability')
        schedule_data = request.get('schedule_data', {})
        probability = request.get('probability', 0.5)

        explanation = insight_generator.explain_alert(alert_type, schedule_data, probability)

        return {
            'success': True,
            'explanation': explanation,
            'alert_type': alert_type,
            'probability': probability
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è§£é‡Šå‘Šè­¦å¤±è´¥: {str(e)}")


@app.get("/scheduling/health")
async def scheduling_health():
    """
    è°ƒåº¦æœåŠ¡å¥åº·æ£€æŸ¥
    """
    return {
        'service': 'scheduling',
        'status': 'running' if SCHEDULING_SERVICE_AVAILABLE else 'unavailable',
        'monte_carlo': True,
        'ortools': SCHEDULING_SERVICE_AVAILABLE,
        'llm_available': bool(client)
    }


# ==================== MLè®­ç»ƒå’Œæ··åˆé¢„æµ‹ ====================

# å¯¼å…¥MLæ¨¡å—
ML_SERVICE_AVAILABLE = False
try:
    from ml_trainer import train_models, model_loader
    from hybrid_predictor import (
        hybrid_predictor, predict_with_hybrid,
        predict_completion, get_model_status
    )
    ML_SERVICE_AVAILABLE = True
except ImportError as e:
    print(f"[WARN] MLæœåŠ¡æœªåŠ è½½: {e}")


@app.post("/ml/train")
async def ml_train_models(request: dict):
    """
    è§¦å‘æ¨¡å‹è®­ç»ƒ

    è¾“å…¥:
    - factory_id: å·¥å‚ID
    - model_types: æ¨¡å‹ç±»å‹åˆ—è¡¨ ["efficiency", "duration", "quality"]

    è¾“å‡º:
    - success: æ˜¯å¦æˆåŠŸ
    - results: å„æ¨¡å‹è®­ç»ƒç»“æœ
    """
    if not ML_SERVICE_AVAILABLE:
        raise HTTPException(status_code=500, detail="MLæœåŠ¡ä¸å¯ç”¨")

    try:
        factory_id = request.get('factory_id')
        model_types = request.get('model_types', ['efficiency', 'duration', 'quality'])

        if not factory_id:
            raise HTTPException(status_code=400, detail="factory_id ä¸èƒ½ä¸ºç©º")

        result = train_models(factory_id, model_types)
        return result

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è®­ç»ƒå¤±è´¥: {str(e)}")


@app.post("/ml/predict")
async def ml_predict(request: dict):
    """
    ä½¿ç”¨MLæ¨¡å‹è¿›è¡Œé¢„æµ‹

    è¾“å…¥:
    - factory_id: å·¥å‚ID
    - prediction_type: é¢„æµ‹ç±»å‹ (efficiency/duration/quality)
    - features: ç‰¹å¾æ•°æ®

    è¾“å‡º:
    - prediction: é¢„æµ‹å€¼
    - confidence: ç½®ä¿¡åº¦
    - model_version: æ¨¡å‹ç‰ˆæœ¬
    """
    if not ML_SERVICE_AVAILABLE:
        raise HTTPException(status_code=500, detail="MLæœåŠ¡ä¸å¯ç”¨")

    try:
        factory_id = request.get('factory_id')
        prediction_type = request.get('prediction_type', 'efficiency')
        features = request.get('features', {})

        result = predict_with_hybrid(factory_id, features, prediction_type)
        return {
            'success': True,
            **result
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é¢„æµ‹å¤±è´¥: {str(e)}")


@app.post("/scheduling/hybrid-predict")
async def scheduling_hybrid_predict(request: dict):
    """
    æ··åˆé¢„æµ‹å®Œæˆæ¦‚ç‡ (ML + Monte Carlo + LLM)

    è¾“å…¥:
    - factory_id: å·¥å‚ID
    - remaining_quantity: å‰©ä½™æ•°é‡
    - deadline_hours: æˆªæ­¢æ—¶é—´(å°æ—¶)
    - available_workers: å¯ç”¨å·¥äººæ•°
    - å…¶ä»–ç‰¹å¾...

    è¾“å‡º:
    - probability: å®Œæˆæ¦‚ç‡
    - mean_hours: é¢„è®¡å¹³å‡æ—¶é•¿
    - mode: é¢„æµ‹æ¨¡å¼ (hybrid/llm_only)
    - explanation: è§£é‡Š
    """
    if not ML_SERVICE_AVAILABLE:
        # å›é€€åˆ°åŸºç¡€Monte Carlo
        raise HTTPException(status_code=500, detail="MLæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·ä½¿ç”¨ /scheduling/completion-probability")

    try:
        factory_id = request.get('factory_id')
        if not factory_id:
            raise HTTPException(status_code=400, detail="factory_id ä¸èƒ½ä¸ºç©º")

        result = predict_completion(factory_id, request)
        return {
            'success': True,
            **result
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ··åˆé¢„æµ‹å¤±è´¥: {str(e)}")


@app.get("/ml/status/{factory_id}")
async def ml_model_status(factory_id: str):
    """
    è·å–å·¥å‚çš„MLæ¨¡å‹çŠ¶æ€

    è¾“å‡º:
    - models: å„ç±»å‹æ¨¡å‹çš„å¯ç”¨çŠ¶æ€
    """
    if not ML_SERVICE_AVAILABLE:
        return {
            'factory_id': factory_id,
            'ml_service_available': False,
            'models': {}
        }

    try:
        status = get_model_status(factory_id)
        status['ml_service_available'] = True
        return status

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}")


@app.get("/ml/health")
async def ml_health():
    """
    MLæœåŠ¡å¥åº·æ£€æŸ¥
    """
    return {
        'service': 'ml',
        'status': 'running' if ML_SERVICE_AVAILABLE else 'unavailable',
        'lightgbm_available': ML_SERVICE_AVAILABLE,
        'hybrid_predictor_available': ML_SERVICE_AVAILABLE
    }


# ==================== å¯åŠ¨ ====================
if __name__ == "__main__":
    import uvicorn
    import sys

    # ä¿®å¤Windowsç»ˆç«¯ç¼–ç é—®é¢˜
    if sys.platform == 'win32':
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

    print("\n" + "="*50)
    print("[START] AI Cost Analysis Service")
    print("="*50)
    print(f"Model: Alibaba Qwen ({DASHSCOPE_MODEL})")
    print(f"Port: 8085")

    if not DASHSCOPE_API_KEY:
        print("[WARN] DASHSCOPE_API_KEY not configured")
        print("Please set in .env: DASHSCOPE_API_KEY=sk-xxx")
    else:
        print("[OK] API Key configured")

    print("="*50 + "\n")

    uvicorn.run("main:app", host="0.0.0.0", port=8085, reload=True)
