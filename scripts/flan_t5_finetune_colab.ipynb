{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flan-T5 意图判别器微调\n",
    "\n",
    "**目标**: 微调 Flan-T5-base 用于意图判别 (JudgeRLVR)\n",
    "\n",
    "**环境**: Google Colab (免费 GPU)\n",
    "\n",
    "**输出**: ONNX 模型，可在 CPU 服务器部署"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes\n",
    "!pip install -q optimum[onnxruntime] onnx onnxruntime\n",
    "!pip install -q scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer, \n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import os\n",
    "\n",
    "# 检查 GPU\n",
    "print(f\"GPU 可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 型号: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"显存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 上传训练数据\n",
    "\n",
    "请上传以下文件到 Colab:\n",
    "- `intent_judge_train.csv`\n",
    "- `intent_judge_valid.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# 上传训练数据\n",
    "print(\"请上传 intent_judge_train.csv 和 intent_judge_valid.csv\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "train_df = pd.read_csv('intent_judge_train.csv', comment='#')\n",
    "valid_df = pd.read_csv('intent_judge_valid.csv', comment='#')\n",
    "\n",
    "print(f\"训练集: {len(train_df)} 条\")\n",
    "print(f\"验证集: {len(valid_df)} 条\")\n",
    "print(f\"\\n数据样例:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    \"\"\"将数据转换为 T5 格式\"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # 输入格式: \"判断用户输入是否匹配意图。用户输入: {input} 意图: {intent} - {description}\"\n",
    "        input_text = f\"判断用户输入是否匹配意图。用户输入: {row['user_input']} 意图: {row['intent_code']} - {row['intent_description']}\"\n",
    "        # 输出格式: \"是\" 或 \"否\"\n",
    "        target_text = \"是\" if row['label'] == 1 else \"否\"\n",
    "        \n",
    "        inputs.append(input_text)\n",
    "        targets.append(target_text)\n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "        'input_text': inputs,\n",
    "        'target_text': targets\n",
    "    })\n",
    "\n",
    "train_dataset = prepare_data(train_df)\n",
    "valid_dataset = prepare_data(valid_df)\n",
    "\n",
    "print(f\"训练样例: {train_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 加载模型和 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 flan-t5-base (中等大小，适合微调)\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,  # 使用半精度节省显存\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"模型参数量: {model.num_parameters() / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 配置 LoRA (低秩适配)\n",
    "\n",
    "LoRA 可以大幅减少需要训练的参数，节省显存和时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 配置\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=16,                    # LoRA 秩\n",
    "    lora_alpha=32,           # LoRA alpha\n",
    "    lora_dropout=0.1,        # Dropout\n",
    "    target_modules=[\"q\", \"v\"],  # 只训练注意力的 Q 和 V\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 数据 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 8\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples['input_text'],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples['target_text'],\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_tokenized = valid_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./flan-t5-intent-judge\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # 训练参数\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # 评估\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    # 日志\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # 优化\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    # 生成\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=valid_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"开始训练...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, df):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    for i, example in enumerate(dataset):\n",
    "        input_ids = torch.tensor([example['input_ids']]).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_length=MAX_TARGET_LENGTH,\n",
    "                num_beams=1,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        pred_label = 1 if \"是\" in pred_text else 0\n",
    "        predictions.append(pred_label)\n",
    "    \n",
    "    # 计算指标\n",
    "    true_labels = df['label'].tolist()\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    print(f\"准确率: {accuracy:.4f}\")\n",
    "    print(\"\\n分类报告:\")\n",
    "    print(classification_report(true_labels, predictions, target_names=['否', '是']))\n",
    "    \n",
    "    return accuracy, predictions\n",
    "\n",
    "print(\"评估验证集...\")\n",
    "accuracy, preds = evaluate_model(model, tokenizer, valid_tokenized, valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 合并 LoRA 权重并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并 LoRA 权重到基础模型\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# 保存完整模型\n",
    "MERGED_DIR = \"./flan-t5-intent-judge-merged\"\n",
    "merged_model.save_pretrained(MERGED_DIR)\n",
    "tokenizer.save_pretrained(MERGED_DIR)\n",
    "\n",
    "print(f\"模型已保存到: {MERGED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 导出 ONNX 模型 (用于 CPU 部署)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "\n",
    "ONNX_DIR = \"./flan-t5-intent-judge-onnx\"\n",
    "\n",
    "# 导出 ONNX\n",
    "print(\"导出 ONNX 模型...\")\n",
    "ort_model = ORTModelForSeq2SeqLM.from_pretrained(\n",
    "    MERGED_DIR,\n",
    "    export=True\n",
    ")\n",
    "ort_model.save_pretrained(ONNX_DIR)\n",
    "tokenizer.save_pretrained(ONNX_DIR)\n",
    "\n",
    "print(f\"ONNX 模型已保存到: {ONNX_DIR}\")\n",
    "\n",
    "# 显示文件大小\n",
    "import os\n",
    "for f in os.listdir(ONNX_DIR):\n",
    "    path = os.path.join(ONNX_DIR, f)\n",
    "    size = os.path.getsize(path) / 1024 / 1024\n",
    "    print(f\"  {f}: {size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 测试 ONNX 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 ONNX 模型测试\n",
    "onnx_model = ORTModelForSeq2SeqLM.from_pretrained(ONNX_DIR)\n",
    "onnx_tokenizer = T5Tokenizer.from_pretrained(ONNX_DIR)\n",
    "\n",
    "def test_inference(text, intent_code, intent_desc):\n",
    "    prompt = f\"判断用户输入是否匹配意图。用户输入: {text} 意图: {intent_code} - {intent_desc}\"\n",
    "    inputs = onnx_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    import time\n",
    "    start = time.time()\n",
    "    outputs = onnx_model.generate(**inputs, max_length=8)\n",
    "    latency = (time.time() - start) * 1000\n",
    "    \n",
    "    result = onnx_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"输入: {text}\")\n",
    "    print(f\"意图: {intent_code}\")\n",
    "    print(f\"判断: {result}\")\n",
    "    print(f\"延迟: {latency:.1f}ms\")\n",
    "    print()\n",
    "    return result\n",
    "\n",
    "# 测试用例\n",
    "test_inference(\"这个月销售怎么样\", \"sales_overview\", \"销售情况概览查询\")  # 应该输出: 是\n",
    "test_inference(\"删除这条记录\", \"sales_overview\", \"销售情况概览查询\")     # 应该输出: 否\n",
    "test_inference(\"查一下库存\", \"material_query\", \"原料库存查询\")           # 应该输出: 是"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打包 ONNX 模型\n",
    "!zip -r flan-t5-intent-judge-onnx.zip flan-t5-intent-judge-onnx/\n",
    "\n",
    "# 下载\n",
    "from google.colab import files\n",
    "files.download('flan-t5-intent-judge-onnx.zip')\n",
    "\n",
    "print(\"\\n请下载 flan-t5-intent-judge-onnx.zip 并上传到服务器:\")\n",
    "print(\"scp flan-t5-intent-judge-onnx.zip root@139.196.165.140:/www/wwwroot/cretas/models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 部署说明\n",
    "\n",
    "### 1. 上传到服务器\n",
    "```bash\n",
    "scp flan-t5-intent-judge-onnx.zip root@139.196.165.140:/www/wwwroot/cretas/models/\n",
    "ssh root@139.196.165.140\n",
    "cd /www/wwwroot/cretas/models/\n",
    "unzip flan-t5-intent-judge-onnx.zip\n",
    "mv flan-t5-intent-judge-onnx flan-t5-base\n",
    "```\n",
    "\n",
    "### 2. 修改 Java 配置\n",
    "```properties\n",
    "cretas.ai.flan-t5.enabled=true\n",
    "cretas.ai.flan-t5.engine=ONNX\n",
    "cretas.ai.flan-t5.model-path=/www/wwwroot/cretas/models/flan-t5-base\n",
    "```\n",
    "\n",
    "### 3. 重启服务\n",
    "```bash\n",
    "cd /www/wwwroot/cretas && bash restart.sh\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
