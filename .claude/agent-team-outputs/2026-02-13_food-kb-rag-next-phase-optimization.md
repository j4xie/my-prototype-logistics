# 食品知识库RAG系统下一阶段优化方案

**生成日期**: 2026-02-13
**研究模式**: Full (3 Researchers + Analyst + Critic + Integrator)

---

## Executive Summary

食品知识库RAG系统检索层MRR=1.0(60查询)可能反映评测集与知识库过度对齐，而非真实检索能力。建议采用"先验证再扩展"策略：第0步用10-20条真实用户风格查询验证MRR基线，然后渐进式扩展评测集、建立LLM-as-Judge自动评测、设计混合反馈闭环。核心风险：食品安全领域LLM评判人机一致率仅60-68%，数值幻觉漏检率高；月均60次查询下自然反馈闭环物理上不可行，需改为人工精标+隐式反馈混合策略。

---

## 一、核心结论

### 1.1 三大方向优先级排序

| 优先级 | 方向 | 置信度 | 理由 |
|--------|------|--------|------|
| **P0** | 评测集扩展 + 基线验证 | ★★★★☆ | 当前60条评测集统计意义不足，是所有后续优化的基石 |
| **P1** | LLM回答质量自动评测 | ★★★☆☆ | 理论成熟但食品安全领域需额外规则层防护数值幻觉 |
| **P2** | 用户反馈闭环 | ★★☆☆☆ | 月60查询量下自然反馈不可行，需改为混合策略 |

### 1.2 关键发现

1. **MRR=1.0是假象风险**: 60条查询可能与知识库高度对齐，真实用户查询MRR可能降至0.5-0.7
2. **LLM-as-Judge不可单独依赖**: 食品安全领域数值敏感（剂量单位、标准号），LLM评判无法可靠区分"0.075g/kg"和"0.75g/kg"
3. **反馈飞轮需要改造**: 6000条反馈→6-10%收益的理论在月60查询场景下不适用，需人工精标替代

---

## 二、实施方案

### Phase 0: 基线验证（第1周，3-5小时）

**目标**: 验证MRR=1.0是否可泛化

| 步骤 | 具体操作 | 输出 |
|------|----------|------|
| 1 | 邀请3-5名实际用户（工厂管理员/质检员/采购）提供10-20条自然语言查询 | 真实查询集 |
| 2 | 用现有eval_rag.py测试这些查询的MRR/NDCG | 基线验证报告 |
| 3 | 若MRR>0.95 → 继续Phase 1；若MRR<0.85 → 先优化检索 | 决策点 |

### Phase 1: 评测集扩展（第1-2周，16-20小时）

**目标**: 60条 → 180-200条，三层设计

| 层级 | 数量 | 占比 | 描述 | 示例 |
|------|------|------|------|------|
| Happy Path | 90条 | ~50% | 直接匹配知识库的标准查询 | "山梨酸钾在肉制品中的限量" |
| Edge Cases | 72条 | ~40% | 多跳推理、跨分类、模糊表述 | "冷链断裂后有机蔬菜还能用吗" |
| Adversarial | 18条 | ~10% | 错别字、非食品问题、缺失信息 | "食品安全法第几条规定了什么"（故意模糊） |

**分类覆盖要求**: 42个分类 × 每分类至少4条 = 168条最低；目标180-200条

**标注格式**:
```json
{
  "query": "查询文本",
  "expected_doc_keywords": ["相关文档关键词"],
  "difficulty": "easy|medium|hard",
  "category": "additive|process|...",
  "query_type": "fact_single|multi_hop|adversarial"
}
```

### Phase 2: LLM回答质量自动评测（第3-4周，12-16小时）

**目标**: 建立5维自动评分 + 规则幻觉检测

**5维Rubric (1-5分)**:

| 维度 | 权重 | 1分 | 3分 | 5分 |
|------|------|-----|-----|-----|
| 准确性 | 40% | 包含错误标准号/数值 | 基本正确但有细节偏差 | 完全准确，与文档一致 |
| 幻觉程度 | 20% | 编造标准号/条款内容 | 有推测但标注了不确定 | 零幻觉，全部有据 |
| 完整性 | 20% | 遗漏关键信息 | 覆盖主要要点 | 全面且有补充价值 |
| 引用质量 | 10% | 无引用或引用错误 | 引用正确但不精确 | 精确引用到文档段落 |
| 可操作性 | 10% | 纯理论无指导 | 有一般性建议 | 具体可执行的操作步骤 |

**分层幻觉检测**:
1. **规则层**: GB标准号正则匹配实体字典、数值单位合理性检查
2. **LLM层**: 温度0.3，5次重采样取中位数
3. **人工层**: 高风险回答（幻觉分>3.5）进入复核队列

**评测脚本**: 纯Python <500行，输出JSON + Langfuse兼容格式

### Phase 3: 反馈闭环（第5-8周，20-24小时）

**目标**: 建立数据收集基础设施，混合精标+弱标

| 反馈类型 | 实现方式 | 预期月采集量 |
|---------|---------|------------|
| 显式-点赞/踩 | Java IntentExecuteResponse + 前端按钮 | 2-5条 |
| 显式-多维分类 | 可选："不准确/不完整/过时/无关" | 1-3条 |
| 隐式-重查询 | 同session重新提问同topic检测 | 5-10条 |
| 隐式-点击引用 | 用户是否点击RAG引用链接 | 10-20条 |
| 人工精标 | 领域专家标注边界cases | 50条(一次性) |

**数据库设计**:
```sql
CREATE TABLE food_kb_feedback (
  id SERIAL PRIMARY KEY,
  query TEXT NOT NULL,
  answer TEXT,
  retrieved_doc_ids INTEGER[],
  rating SMALLINT, -- 1-5
  feedback_type VARCHAR(20), -- explicit/implicit/expert
  feedback_detail JSONB, -- {category: "inaccurate", comment: "..."}
  session_id VARCHAR(64),
  user_id BIGINT,
  created_at TIMESTAMPTZ DEFAULT NOW()
);
```

---

## 三、团队共识与分歧

### 共识点
- 评测集60条不足，需要扩展（所有agent一致）
- 纯Python脚本优于RAGAS/TruLens重框架（分析员+集成者）
- 自然反馈闭环在低流量下不可行（评论员+集成者）

### 分歧点

| 议题 | 分析员观点 | 评论员观点 | 最终决定 |
|------|-----------|-----------|---------|
| 评测集规模 | 300条足够 | 需500-600条 | 先180-200条启动，后续扩展 |
| LLM-as-Judge可靠性 | 可靠（配rubric） | 食品领域仅60-68%一致率 | 可用但需规则层+人工兜底 |
| 月度成本 | ~200元 | ~560元（含人工） | ~600元（含100条/月人工复核） |
| MRR=1.0含义 | 检索层已达高水平 | 几乎必然是过度对齐 | 需第0步验证 |

---

## 四、成本估算

| 项目 | 一次性成本 | 月度成本 |
|------|-----------|---------|
| 评测集构建(180条) | 16-20小时人力 | - |
| 人工精标(50条) | 8-10小时专家 | - |
| LLM评测调用 | - | ~200元 |
| 人工复核(100条/月) | - | ~400元/8小时 |
| Langfuse自托管 | 2-3小时部署 | 0元(开源) |
| **合计** | ~30小时 + 2-3小时 | ~600元/月 |

---

## 五、风险与触发条件

| 触发条件 | 应对措施 |
|---------|---------|
| Phase 0: MRR跌至<0.6 | 暂停评测扩展，优先优化检索（知识库覆盖+查询改写） |
| Phase 2: Cohen's Kappa<0.5 | 重新设计Rubric或升级为多LLM投票 |
| Phase 3: 月反馈率<1% | 增加显式引导（弹窗提示）或改为纯人工标注 |
| 规则幻觉漏检>10% | 加入轻量ONNX NER验证实体合理性 |

---

## 六、开放问题

1. 知识库781文档在42分类上的分布是否均匀？稀疏分类需要先补内容还是先评测？
2. 食品安全"准确性"权重是否应从40%提升到70-80%？（健康风险考量）
3. 嵌入微调(GISTEmbed)还是LoRA微调？150条数据量是否足够？
4. 多跳推理(MultiHop-RAG)是否应在Phase 3后独立评估？
5. DashScope qwen-max的logprobs支持程度如何？影响G-Eval可用性

---

### Process Note
- Mode: Full
- Researchers deployed: 3
- Total sources found: 38+
- Key disagreements: 4 resolved (评测集规模、LLM可靠性、成本、MRR含义), 2 unresolved (嵌入微调方法、多跳推理优先级)
- Phases completed: Research → Analysis → Critique → Integration
